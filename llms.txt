# `xplainfi`

The goal of `xplainfi` is to collect common feature importance methods
under a unified and extensible interface.

It is built around [mlr3](https://mlr-org.com/) as available
abstractions for learners, tasks, measures, etc. greatly simplify the
implementation of importance measures.

## Installation

You can install the development version of `xplainfi` like using `pak`:

``` r
# install.packages(pak)
pak::pak("mlr-org/xplainfi")
```

## Example: PFI

Here is a basic example on how to calculate PFI for a given learner and
task, using repeated cross-validation as resampling strategy and
computing PFI within each resampling 10 times on the `friedman1` task
(see
[`?mlbench::mlbench.friedman1`](https://rdrr.io/pkg/mlbench/man/mlbench.friedman1.html)).

The `friedman1` task has the following structure:

\\y = 10 \sin(\pi x_1 x_2) + 20(x_3 - 0.5)^2 + 10x_4 + 5x_5 +
\varepsilon\\

Where \\x\_{1,2,3,4,5}\\ are named `important1` through `important5` in
the `Task`, with additional numbered `unimportant` features without
effect on \\y\\.

``` r
library(xplainfi)
library(mlr3learners)
#> Loading required package: mlr3

task = tgen("friedman1")$generate(1000)
learner = lrn("regr.ranger", num.trees = 100)
measure = msr("regr.mse")

pfi = PFI$new(
    task = task,
    learner = learner,
    measure = measure,
    resampling = rsmp("cv", folds = 3),
    n_repeats = 10
)
```

Compute and print PFI scores:

``` r
pfi$compute()
pfi$importance()
#> Key: <feature>
#>          feature  importance
#>           <char>       <num>
#>  1:   important1  8.26830487
#>  2:   important2  7.56661552
#>  3:   important3  1.54377613
#>  4:   important4 12.60915426
#>  5:   important5  2.71382429
#>  6: unimportant1  0.02113506
#>  7: unimportant2 -0.01250122
#>  8: unimportant3 -0.04414584
#>  9: unimportant4 -0.05157362
#> 10: unimportant5  0.04819212
```

If it aides interpretation, importances can also be calculates as the
*ratio* rather then the *difference* between the baseline and
post-permutation losses:

``` r
pfi$importance(relation = "ratio")
#> Key: <feature>
#>          feature importance
#>           <char>      <num>
#>  1:   important1  2.7167932
#>  2:   important2  2.5798917
#>  3:   important3  1.3234729
#>  4:   important4  3.6334554
#>  5:   important5  1.5680305
#>  6: unimportant1  1.0047822
#>  7: unimportant2  0.9972307
#>  8: unimportant3  0.9907595
#>  9: unimportant4  0.9890062
#> 10: unimportant5  1.0100964
```

When PFI is computed based on resampling with multiple iterations, and /
or multiple permutation iterations, the individual scores can be
retrieved as a `data.table`:

``` r
str(pfi$scores())
#> Classes 'data.table' and 'data.frame':   300 obs. of  6 variables:
#>  $ feature          : chr  "important1" "important1" "important1" "important1" ...
#>  $ iter_rsmp        : int  1 1 1 1 1 1 1 1 1 1 ...
#>  $ iter_repeat      : int  1 2 3 4 5 6 7 8 9 10 ...
#>  $ regr.mse_baseline: num  4.56 4.56 4.56 4.56 4.56 ...
#>  $ regr.mse_post    : num  11.4 11.5 12.7 12.9 10.7 ...
#>  $ importance       : num  6.83 6.96 8.15 8.35 6.09 ...
#>  - attr(*, ".internal.selfref")=<externalptr>
```

Where `iter_rsmp` corresponds to the resampling iteration, i.e., 3 for
3-fold cross-validation, and `iter_repeat` corresponds to the
permutation iteration within each resampling iteration, 5 in this case.
While `pfi$importance()` contains the means across all iterations,
`pfi$scores()` allows you to manually visualize or aggregate them in any
way you see fit.

For example:

``` r
library(ggplot2)

ggplot(
    pfi$scores(),
    aes(x = importance, y = reorder(feature, importance))
) +
    geom_boxplot(color = "#f44560", fill = alpha("#f44560", 0.4)) +
    labs(
        title = "Permutation Feature Importance on Friedman1",
        subtitle = "Computed over 3-fold CV with 5 permutations per iteration using Random Forest",
        x = "Importance",
        y = "Feature"
    ) +
    theme_minimal(base_size = 16) +
    theme(
        plot.title.position = "plot",
        panel.grid.major.y = element_blank()
    )
```

![](reference/figures/README-pfi-plot-1.png)

If the measure in question needs to be maximized rather than minimized
(like \\R^2\\), the internal importance calculation takes that into
account via the `$minimize` property of the measure and calculates
importances such that the intuition “performance improvement” -\>
“higher importance score” still holds:

``` r
pfi = PFI$new(
    task = task,
    learner = learner,
    measure = msr("regr.rsq")
)
#> ℹ No <Resampling> provided
#> Using `resampling = rsmp("holdout")` with default `ratio = 0.67`.

pfi$compute()
pfi$importance()
#> Key: <feature>
#>          feature    importance
#>           <char>         <num>
#>  1:   important1  0.3538670264
#>  2:   important2  0.3078201501
#>  3:   important3  0.0684752046
#>  4:   important4  0.5509192405
#>  5:   important5  0.1324330936
#>  6: unimportant1 -0.0033190271
#>  7: unimportant2  0.0002369042
#>  8: unimportant3 -0.0020602383
#>  9: unimportant4 -0.0016842988
#> 10: unimportant5 -0.0007178819
```

See
[`vignette("xplainfi")`](https://jemus42.github.io/xplainfi/articles/xplainfi.md)
for more examples.

# Package index

## Importance Measures

Base class

- [`FeatureImportanceMethod`](https://jemus42.github.io/xplainfi/reference/FeatureImportanceMethod.md)
  : Feature Importance Method Class

### Perturbation-Based Importance Measures

Methods which perturb features of interest either marginally (PFI) or
conditionally on all (CFI) or a subset of remaining features (RFI)

- [`PerturbationImportance`](https://jemus42.github.io/xplainfi/reference/PerturbationImportance.md)
  : Perturbation Feature Importance Base Class
- [`PFI`](https://jemus42.github.io/xplainfi/reference/PFI.md) :
  Permutation Feature Importance
- [`CFI`](https://jemus42.github.io/xplainfi/reference/CFI.md) :
  Conditional Feature Importance
- [`RFI`](https://jemus42.github.io/xplainfi/reference/RFI.md) :
  Relative Feature Importance

### Model Refitting Measures

Methods which refit models with one (or more) features omitted (LOCO) or
included (LOCI).

- [`WVIM`](https://jemus42.github.io/xplainfi/reference/WVIM.md) :
  Williamson's Variable Importance Measure (WVIM)
- [`LOCO`](https://jemus42.github.io/xplainfi/reference/LOCO.md) :
  Leave-One-Covariate-Out (LOCO)

### Shapley-Based Approaches

Shapley Additive Global Importance (SAGE) in marginal and conditional
variants

- [`SAGE`](https://jemus42.github.io/xplainfi/reference/SAGE.md) :
  Shapley Additive Global Importance (SAGE) Base Class
- [`MarginalSAGE`](https://jemus42.github.io/xplainfi/reference/MarginalSAGE.md)
  : Marginal SAGE
- [`ConditionalSAGE`](https://jemus42.github.io/xplainfi/reference/ConditionalSAGE.md)
  : Conditional SAGE

## Sampling Infrastructure

Base class and three families of samplers (Marginal, Conditional,
Knockoff)

### Base classes

Abstract base classes for the three sampler families. FeatureSampler is
the top-level base class, MarginalSampler is for marginal sampling
methods, and ConditionalSampler is for conditional sampling methods.

- [`FeatureSampler`](https://jemus42.github.io/xplainfi/reference/FeatureSampler.md)
  : Feature Sampler Class
- [`MarginalSampler`](https://jemus42.github.io/xplainfi/reference/MarginalSampler.md)
  : Marginal Sampler Base Class
- [`ConditionalSampler`](https://jemus42.github.io/xplainfi/reference/ConditionalSampler.md)
  : Conditional Feature Sampler

### Marginal sampling (no conditioning)

Samplers that draw from the marginal distribution P(X_S) without
conditioning on other features.

- [`MarginalPermutationSampler`](https://jemus42.github.io/xplainfi/reference/MarginalPermutationSampler.md)
  : Marginal Permutation Sampler
- [`MarginalReferenceSampler`](https://jemus42.github.io/xplainfi/reference/MarginalReferenceSampler.md)
  : Marginal Reference Sampler

### Conditional sampling (with conditioning)

Samplers that draw from the conditional distribution P(X_S \| X_C) where
X_C is an arbitrary conditioning set. The conditioning set can be
specified via the `conditioning_set` parameter.

- [`ConditionalARFSampler`](https://jemus42.github.io/xplainfi/reference/ConditionalARFSampler.md)
  : ARF-based Conditional Sampler
- [`ConditionalGaussianSampler`](https://jemus42.github.io/xplainfi/reference/ConditionalGaussianSampler.md)
  : Gaussian Conditional Sampler
- [`ConditionalKNNSampler`](https://jemus42.github.io/xplainfi/reference/ConditionalKNNSampler.md)
  : k-Nearest Neighbors Conditional Sampler
- [`ConditionalCtreeSampler`](https://jemus42.github.io/xplainfi/reference/ConditionalCtreeSampler.md)
  : (experimental) Conditional Inference Tree Conditional Sampler

### Knockoff sampling

Knockoffs satisfy certain theoretical properties that exceed those of
the conditional samplers. They generate one (or more) knockoff matrix on
construction, and sampling is always performed in reference to these.
Unlike other samplers, they do not allow sampling from new data with
`$sample_newdata()`, as that would require re-creating a knockoff matrix
on the fly.

- [`KnockoffSampler`](https://jemus42.github.io/xplainfi/reference/KnockoffSampler.md)
  : Knockoff-based Conditional Sampler
- [`KnockoffGaussianSampler`](https://jemus42.github.io/xplainfi/reference/KnockoffGaussianSampler.md)
  : Gaussian Knockoff Conditional Sampler
- [`KnockoffSequentialSampler`](https://jemus42.github.io/xplainfi/reference/KnockoffSequentialSampler.md)
  : Gaussian Knockoff Conditional Sampler

## Utilities

- [`wvim_design_matrix()`](https://jemus42.github.io/xplainfi/reference/wvim_design_matrix.md)
  : Create Feature Selection Design Matrix

- [`check_groups()`](https://jemus42.github.io/xplainfi/reference/check_groups.md)
  : Check group specification

- [`` `%||%` ``](https://jemus42.github.io/xplainfi/reference/op-null-default.md)
  :

  Default value for `NULL`

## Data simulation

- [`sim_dgp_ewald()`](https://jemus42.github.io/xplainfi/reference/sim_dgp_ewald.md)
  : Simulate data as in Ewald et al. (2024)
- [`sim_dgp_correlated()`](https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_mediated()`](https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_confounded()`](https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_interactions()`](https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_independent()`](https://jemus42.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  : Simulation DGPs for Feature Importance Method Comparison

# Articles

### Feature Importance Methods

- [Perturbation-based Feature Importance
  Methods](https://jemus42.github.io/xplainfi/articles/perturbation-importance.md):
- [LOCO and WVIM](https://jemus42.github.io/xplainfi/articles/loco.md):
- [Shapley Additive Global Importance
  (SAGE)](https://jemus42.github.io/xplainfi/articles/sage-methods.md):

### Specialized topics

- [Inference
  (Experimental)](https://jemus42.github.io/xplainfi/articles/inference.md):
- [Feature
  Samplers](https://jemus42.github.io/xplainfi/articles/feature-samplers.md):

### Testing and Validation

- [Simulation Settings for Feature Importance
  Methods](https://jemus42.github.io/xplainfi/articles/simulation-settings.md):
- [Comparison with fippy (Python
  Implementation)](https://jemus42.github.io/xplainfi/articles/fippy-comparison.md):
