# `xplainfi`

The goal of `xplainfi` is to collect common feature importance methods
under a unified and extensible interface.

It is built around [mlr3](https://mlr-org.com/) as available
abstractions for learners, tasks, measures, etc. greatly simplify the
implementation of importance measures.

## Installation

Install `xplainfi` from CRAN:

``` r
install.packages("xplainfi")
```

Or install from [R-universe](https://mlr-org.r-universe.dev):

``` r
install.packages("xplainfi", repos = c("https://mlr-org.r-universe.dev", "https://cloud.r-project.org"))
```

The latest development version of `xplainfi` can be installed with
`pak`:

``` r
# install.packages(pak)
pak::pak("mlr-org/xplainfi")
```

## Example: PFI

Here is a basic example on how to calculate PFI for an untrained learner
and task, using cross-validation for resampling and computing PFI within
each resampling iteration 10 times on the `friedman1` task (see
[`?mlbench::mlbench.friedman1`](https://rdrr.io/pkg/mlbench/man/mlbench.friedman1.html)).

The `friedman1` task has the following structure:

\\y = 10 \sin(\pi x_1 x_2) + 20(x_3 - 0.5)^2 + 10x_4 + 5x_5 +
\varepsilon\\

Where \\x\_{\\1,2,3,4,5\\}\\ are named `important1` through `important5`
in the `Task`, with additional numbered `unimportant` features without
effect on \\y\\.

``` r
library(xplainfi)
library(mlr3learners)
#> Loading required package: mlr3

task = tgen("friedman1")$generate(1000)
learner = lrn("regr.ranger", num.trees = 100)
measure = msr("regr.mse")

pfi = PFI$new(
    task = task,
    learner = learner,
    measure = measure,
    resampling = rsmp("cv", folds = 3),
    n_repeats = 30
)
```

Compute and print PFI scores:

``` r
pfi$compute()
pfi$importance()
#> Key: <feature>
#>          feature   importance
#>           <char>        <num>
#>  1:   important1  8.183995584
#>  2:   important2  7.481268675
#>  3:   important3  1.571760349
#>  4:   important4 12.585739572
#>  5:   important5  2.810875567
#>  6: unimportant1  0.030667439
#>  7: unimportant2 -0.002837696
#>  8: unimportant3 -0.044922079
#>  9: unimportant4 -0.060054450
#> 10: unimportant5  0.060148388
```

If it aids interpretation, importances can also be calculated as the
*ratio* rather than the *difference* between the baseline and
post-permutation losses:

``` r
pfi$importance(relation = "ratio")
#> Key: <feature>
#>          feature importance
#>           <char>      <num>
#>  1:   important1  2.6987668
#>  2:   important2  2.5598945
#>  3:   important3  1.3294180
#>  4:   important4  3.6278508
#>  5:   important5  1.5874860
#>  6: unimportant1  1.0067957
#>  7: unimportant2  0.9994507
#>  8: unimportant3  0.9905990
#>  9: unimportant4  0.9874657
#> 10: unimportant5  1.0126572
```

When PFI is computed based on resampling with multiple iterations, and /
or multiple permutation iterations, the individual scores can be
retrieved as a `data.table`:

``` r
str(pfi$scores())
#> Classes 'data.table' and 'data.frame':   900 obs. of  6 variables:
#>  $ feature          : chr  "important1" "important1" "important1" "important1" ...
#>  $ iter_rsmp        : int  1 1 1 1 1 1 1 1 1 1 ...
#>  $ iter_repeat      : int  1 2 3 4 5 6 7 8 9 10 ...
#>  $ regr.mse_baseline: num  4.56 4.56 4.56 4.56 4.56 ...
#>  $ regr.mse_post    : num  12.3 11.9 11.3 12.1 13.6 ...
#>  $ importance       : num  7.77 7.33 6.74 7.56 9.06 ...
#>  - attr(*, ".internal.selfref")=<externalptr>
```

Where `iter_rsmp` corresponds to the resampling iteration, i.e., 3 for
3-fold cross-validation, and `iter_repeat` corresponds to the
permutation iteration within each resampling iteration, 5 in this case.
While `pfi$importance()` contains the means across all iterations,
`pfi$scores()` allows you to manually visualize or aggregate them in any
way you see fit.

For example:

``` r
library(ggplot2)

ggplot(
    pfi$scores(),
    aes(x = importance, y = reorder(feature, importance))
) +
    geom_boxplot(color = "#f44560", fill = alpha("#f44560", 0.4)) +
    labs(
        title = "Permutation Feature Importance on Friedman1",
        subtitle = "Computed over 3-fold CV with 5 permutations per iteration using Random Forest",
        x = "Importance",
        y = "Feature"
    ) +
    theme_minimal(base_size = 16) +
    theme(
        plot.title.position = "plot",
        panel.grid.major.y = element_blank()
    )
```

![](reference/figures/README-pfi-plot-1.png)

If the measure in question needs to be maximized rather than minimized
(like \\R^2\\), the internal importance calculation takes that into
account via the `$minimize` property of the measure and calculates
importances such that the intuition “performance improvement” -\>
“higher importance score” still holds:

``` r
pfi = PFI$new(
    task = task,
    learner = learner,
    measure = msr("regr.rsq")
)
#> ℹ No <Resampling> provided, using `resampling = rsmp("holdout", ratio = 2/3)`
#>   (test set size: 333)

pfi$compute()
pfi$importance()
#> Key: <feature>
#>          feature   importance
#>           <char>        <num>
#>  1:   important1  0.329915393
#>  2:   important2  0.297695022
#>  3:   important3  0.063613087
#>  4:   important4  0.493673768
#>  5:   important5  0.121794662
#>  6: unimportant1  0.003972813
#>  7: unimportant2  0.002157623
#>  8: unimportant3 -0.002780577
#>  9: unimportant4  0.001914150
#> 10: unimportant5  0.001366645
```

See
[`vignette("xplainfi")`](https://mlr-org.github.io/xplainfi/articles/xplainfi.md)
for more examples.

# Package index

## Importance Measures

Base class

- [`FeatureImportanceMethod`](https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.md)
  : Feature Importance Method Class

### Perturbation-Based Importance Measures

Methods which perturb features of interest either marginally (PFI) or
conditionally on all (CFI) or a subset of remaining features (RFI)

- [`PerturbationImportance`](https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.md)
  : Perturbation Feature Importance Base Class
- [`PFI`](https://mlr-org.github.io/xplainfi/reference/PFI.md) :
  Permutation Feature Importance
- [`CFI`](https://mlr-org.github.io/xplainfi/reference/CFI.md) :
  Conditional Feature Importance
- [`RFI`](https://mlr-org.github.io/xplainfi/reference/RFI.md) :
  Relative Feature Importance

### Model Refitting Measures

Methods which refit models with one (or more) features omitted (LOCO) or
included (LOCI).

- [`WVIM`](https://mlr-org.github.io/xplainfi/reference/WVIM.md) :
  Williamson's Variable Importance Measure (WVIM)
- [`LOCO`](https://mlr-org.github.io/xplainfi/reference/LOCO.md) :
  Leave-One-Covariate-Out (LOCO)

### Shapley-Based Approaches

Shapley Additive Global Importance (SAGE) in marginal and conditional
variants

- [`SAGE`](https://mlr-org.github.io/xplainfi/reference/SAGE.md) :
  Shapley Additive Global Importance (SAGE) Base Class
- [`MarginalSAGE`](https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.md)
  : Marginal SAGE
- [`ConditionalSAGE`](https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.md)
  : Conditional SAGE

## Sampling Infrastructure

Base class and three families of samplers (Marginal, Conditional,
Knockoff)

### Base classes

Abstract base classes for the three sampler families. FeatureSampler is
the top-level base class, MarginalSampler is for marginal sampling
methods, and ConditionalSampler is for conditional sampling methods.

- [`FeatureSampler`](https://mlr-org.github.io/xplainfi/reference/FeatureSampler.md)
  : Feature Sampler Class
- [`MarginalSampler`](https://mlr-org.github.io/xplainfi/reference/MarginalSampler.md)
  : Marginal Sampler Base Class
- [`ConditionalSampler`](https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.md)
  : Conditional Feature Sampler

### Marginal sampling (no conditioning)

Samplers that draw from the marginal distribution P(X_S) without
conditioning on other features.

- [`MarginalPermutationSampler`](https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.md)
  : Marginal Permutation Sampler
- [`MarginalReferenceSampler`](https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.md)
  : Marginal Reference Sampler

### Conditional sampling (with conditioning)

Samplers that draw from the conditional distribution P(X_S \| X_C) where
X_C is an arbitrary conditioning set. The conditioning set can be
specified via the `conditioning_set` parameter.

- [`ConditionalARFSampler`](https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.md)
  : ARF-based Conditional Sampler
- [`ConditionalGaussianSampler`](https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.md)
  : Gaussian Conditional Sampler
- [`ConditionalKNNSampler`](https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.md)
  : k-Nearest Neighbors Conditional Sampler
- [`ConditionalCtreeSampler`](https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.md)
  : (experimental) Conditional Inference Tree Conditional Sampler

### Knockoff sampling

Knockoffs satisfy certain theoretical properties that exceed those of
the conditional samplers. They generate one (or more) knockoff matrix on
construction, and sampling is always performed in reference to these.
Unlike other samplers, they do not allow sampling from new data with
`$sample_newdata()`, as that would require re-creating a knockoff matrix
on the fly.

- [`KnockoffGaussianSampler`](https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.md)
  : Gaussian Knockoff Conditional Sampler
- [`KnockoffSampler`](https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.md)
  : Knockoff Sampler

## Utilities

- [`wvim_design_matrix()`](https://mlr-org.github.io/xplainfi/reference/wvim_design_matrix.md)
  : Create Feature Selection Design Matrix

- [`rsmp_all_test()`](https://mlr-org.github.io/xplainfi/reference/rsmp_all_test.md)
  : Create a resampling with all data being test data

- [`check_groups()`](https://mlr-org.github.io/xplainfi/reference/check_groups.md)
  : Check group specification

- [`` `%||%` ``](https://mlr-org.github.io/xplainfi/reference/op-null-default.md)
  :

  Default value for `NULL`

- [`xplain_opt()`](https://mlr-org.github.io/xplainfi/reference/xplain_opt.md)
  : xplainfi Package Options

## Data simulation

- [`sim_dgp_ewald()`](https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.md)
  : Simulate data as in Ewald et al. (2024)
- [`sim_dgp_correlated()`](https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_mediated()`](https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_confounded()`](https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_interactions()`](https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  [`sim_dgp_independent()`](https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.md)
  : Simulation DGPs for Feature Importance Method Comparison

# Articles

### Feature Importance Methods

- [Perturbation-based Feature Importance
  Methods](https://mlr-org.github.io/xplainfi/articles/perturbation-importance.md):
- [LOCO and WVIM](https://mlr-org.github.io/xplainfi/articles/loco.md):
- [Shapley Additive Global Importance
  (SAGE)](https://mlr-org.github.io/xplainfi/articles/sage-methods.md):

### Specialized topics

- [Inference for Feature
  Importance](https://mlr-org.github.io/xplainfi/articles/inference.md):
- [Feature
  Samplers](https://mlr-org.github.io/xplainfi/articles/feature-samplers.md):

### Testing and Validation

- [Simulation Settings for Feature Importance
  Methods](https://mlr-org.github.io/xplainfi/articles/simulation-settings.md):
