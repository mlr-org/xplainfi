[{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed. version GNU Lesser General Public License incorporates terms conditions version 3 GNU General Public License, supplemented additional permissions listed .","code":""},{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":"id_0-additional-definitions","dir":"","previous_headings":"","what":"0. Additional Definitions","title":"GNU Lesser General Public License","text":"used herein, “License” refers version 3 GNU Lesser General Public License, “GNU GPL” refers version 3 GNU General Public License. “Library” refers covered work governed License, Application Combined Work defined . “Application” work makes use interface provided Library, otherwise based Library. Defining subclass class defined Library deemed mode using interface provided Library. “Combined Work” work produced combining linking Application Library. particular version Library Combined Work made also called “Linked Version”. “Minimal Corresponding Source” Combined Work means Corresponding Source Combined Work, excluding source code portions Combined Work , considered isolation, based Application, Linked Version. “Corresponding Application Code” Combined Work means object code /source code Application, including data utility programs needed reproducing Combined Work Application, excluding System Libraries Combined Work.","code":""},{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":"id_1-exception-to-section-3-of-the-gnu-gpl","dir":"","previous_headings":"","what":"1. Exception to Section 3 of the GNU GPL","title":"GNU Lesser General Public License","text":"may convey covered work sections 3 4 License without bound section 3 GNU GPL.","code":""},{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":"id_2-conveying-modified-versions","dir":"","previous_headings":"","what":"2. Conveying Modified Versions","title":"GNU Lesser General Public License","text":"modify copy Library, , modifications, facility refers function data supplied Application uses facility (argument passed facility invoked), may convey copy modified version: ) License, provided make good faith effort ensure , event Application supply function data, facility still operates, performs whatever part purpose remains meaningful, b) GNU GPL, none additional permissions License applicable copy.","code":""},{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":"id_3-object-code-incorporating-material-from-library-header-files","dir":"","previous_headings":"","what":"3. Object Code Incorporating Material from Library Header Files","title":"GNU Lesser General Public License","text":"object code form Application may incorporate material header file part Library. may convey object code terms choice, provided , incorporated material limited numerical parameters, data structure layouts accessors, small macros, inline functions templates (ten fewer lines length), following: ) Give prominent notice copy object code Library used Library use covered License. b) Accompany object code copy GNU GPL license document.","code":""},{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":"id_4-combined-works","dir":"","previous_headings":"","what":"4. Combined Works","title":"GNU Lesser General Public License","text":"may convey Combined Work terms choice , taken together, effectively restrict modification portions Library contained Combined Work reverse engineering debugging modifications, also following: ) Give prominent notice copy Combined Work Library used Library use covered License. b) Accompany Combined Work copy GNU GPL license document. c) Combined Work displays copyright notices execution, include copyright notice Library among notices, well reference directing user copies GNU GPL license document. d) one following: 0) Convey Minimal Corresponding Source terms License, Corresponding Application Code form suitable , terms permit, user recombine relink Application modified version Linked Version produce modified Combined Work, manner specified section 6 GNU GPL conveying Corresponding Source. 1) Use suitable shared library mechanism linking Library. suitable mechanism one () uses run time copy Library already present user’s computer system, (b) operate properly modified version Library interface-compatible Linked Version. e) Provide Installation Information, otherwise required provide information section 6 GNU GPL, extent information necessary install execute modified version Combined Work produced recombining relinking Application modified version Linked Version. (use option 4d0, Installation Information must accompany Minimal Corresponding Source Corresponding Application Code. use option 4d1, must provide Installation Information manner specified section 6 GNU GPL conveying Corresponding Source.)","code":""},{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":"id_5-combined-libraries","dir":"","previous_headings":"","what":"5. Combined Libraries","title":"GNU Lesser General Public License","text":"may place library facilities work based Library side side single library together library facilities Applications covered License, convey combined library terms choice, following: ) Accompany combined library copy work based Library, uncombined library facilities, conveyed terms License. b) Give prominent notice combined library part work based Library, explaining find accompanying uncombined form work.","code":""},{"path":"https://mlr-org.github.io/xplainfi/LICENSE.html","id":"id_6-revised-versions-of-the-gnu-lesser-general-public-license","dir":"","previous_headings":"","what":"6. Revised Versions of the GNU Lesser General Public License","title":"GNU Lesser General Public License","text":"Free Software Foundation may publish revised /new versions GNU Lesser General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Library received specifies certain numbered version GNU Lesser General Public License “later version” applies , option following terms conditions either published version later version published Free Software Foundation. Library received specify version number GNU Lesser General Public License, may choose version GNU Lesser General Public License ever published Free Software Foundation. Library received specifies proxy can decide whether future versions GNU Lesser General Public License shall apply, proxy’s public statement acceptance version permanent authorization choose version Library.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Feature Samplers","text":"create two tasks: one mixed features (penguins data), one -numeric features.","code":"library(xplainfi) library(mlr3) library(mlr3learners) library(data.table)  # Create a task for demonstration task_mixed = tsk(\"penguins\") task_numeric = sim_dgp_correlated(n = 200)"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"base-class-featuresampler","dir":"Articles","previous_headings":"","what":"Base Class: FeatureSampler","title":"Feature Samplers","text":"feature samplers inherit FeatureSampler base class, provides common interface sampling features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"key-properties","dir":"Articles","previous_headings":"Base Class: FeatureSampler","what":"Key Properties","title":"Feature Samplers","text":"Feature Type Support: sampler declares feature types supports: Two Sampling Methods: $sample(feature, row_ids) - Sample stored task $sample_newdata(feature, newdata) - Sample using external data Let’s demonstrate permutation sampler: Notice : sampled feature values change (permuted) features target remain unchanged data structure preserved","code":"# Check supported feature types for different samplers task_mixed$feature_types #> Key: <id> #>                id    type #>            <char>  <char> #> 1:     bill_depth numeric #> 2:    bill_length numeric #> 3:      body_mass integer #> 4: flipper_length integer #> 5:         island  factor #> 6:            sex  factor #> 7:           year integer permutation = MarginalPermutationSampler$new(task_mixed) permutation$feature_types #> [1] \"numeric\"   \"factor\"    \"ordered\"   \"integer\"   \"logical\"   \"Date\"      #> [7] \"POSIXct\"   \"character\" # Sample from stored task (using row_ids) sampled_task = permutation$sample(     feature = \"bill_length\",     row_ids = 40:45 ) sampled_task #>    species bill_depth bill_length body_mass flipper_length island    sex  year #>     <fctr>      <num>       <num>     <int>          <int> <fctr> <fctr> <int> #> 1:  Adelie       19.1        39.8      4650            184  Dream   male  2007 #> 2:  Adelie       18.0        44.1      3150            182  Dream female  2007 #> 3:  Adelie       18.4        37.0      3900            195  Dream   male  2007 #> 4:  Adelie       18.5        36.5      3100            186  Dream female  2007 #> 5:  Adelie       19.7        40.8      4400            196  Dream   male  2007 #> 6:  Adelie       16.9        36.0      3000            185  Dream female  2007  # Sample from \"external\" data test_data = task_mixed$data(rows = 40:45) sampled_external = permutation$sample_newdata(     feature = \"bill_length\",     newdata = test_data ) sampled_external #>    species bill_depth bill_length body_mass flipper_length island    sex  year #>     <fctr>      <num>       <num>     <int>          <int> <fctr> <fctr> <int> #> 1:  Adelie       19.1        36.5      4650            184  Dream   male  2007 #> 2:  Adelie       18.0        37.0      3150            182  Dream female  2007 #> 3:  Adelie       18.4        39.8      3900            195  Dream   male  2007 #> 4:  Adelie       18.5        44.1      3100            186  Dream female  2007 #> 5:  Adelie       19.7        36.0      4400            196  Dream   male  2007 #> 6:  Adelie       16.9        40.8      3000            185  Dream female  2007"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"permutation-sampler","dir":"Articles","previous_headings":"","what":"Permutation Sampler","title":"Feature Samplers","text":"MarginalPermutationSampler performs simple random permutation features, breaking relationship target features. classic approach used Permutation Feature Importance (PFI). works: feature randomly shuffled (permuted) independently association feature values target values broken association feature values across rows broken marginal distribution feature preserved Note permutation performed within requested row_ids. Use PFI: permutation sampler used default Permutation Feature Importance","code":"# Create permutation sampler permutation = MarginalPermutationSampler$new(task_mixed)  # Sample a continuous feature original = task_mixed$data(rows = 1:10) sampled = permutation$sample(\"bill_length\", row_ids = 1:10)  # Compare original and sampled values data.table(     original_bill = original$bill_length,     sampled_bill = sampled$bill_length,     sex = original$sex # Unchanged ) #>     original_bill sampled_bill    sex #>             <num>        <num> <fctr> #>  1:          39.1         42.0   male #>  2:          39.5         39.1 female #>  3:          40.3           NA female #>  4:            NA         34.1   <NA> #>  5:          36.7         39.5 female #>  6:          39.3         36.7   male #>  7:          38.9         38.9 female #>  8:          39.2         39.2   male #>  9:          34.1         40.3   <NA> #> 10:          42.0         39.3   <NA>"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"marginal-reference-sampler","dir":"Articles","previous_headings":"","what":"Marginal Reference Sampler","title":"Feature Samplers","text":"MarginalReferenceSampler another type marginal sampler samples complete rows reference data. Unlike MarginalPermutationSampler shuffles feature values independently, sampler preserves within-row dependencies sampling intact observations. Key differences MarginalPermutationSampler: MarginalPermutationSampler: Shuffles feature independently – breaks dependencies sampled features MarginalReferenceSampler: Samples complete reference rows – preserves dependencies sampled features approach used SAGE (Shapley Additive Global importancE) marginal feature importance. SAGE implementation, MarginalImputer, uses approach. “imputer” name comes “imputation” model predictions using --coalition features sampling marginal distribution. xplainfi, separate “feature sampling” “model prediction” steps (now), keep sampling infrastructure independent. works: Parameters: NULL: uses task data specified: subsamples many rows task Preserving within-row correlations: demonstrate difference, consider features correlated task_numeric. , x1 x2 correlated, sample jointly, MarginalReferenceSampler retains original correlation (approximately). reference sampler better preserves correlation structure samples complete rows, permutation completely breaks dependency.","code":"# Create marginal reference sampler with n_samples reference pool marginal_ref = MarginalReferenceSampler$new(task_mixed, n_samples = 30L)  # Sample a feature - each row gets values from a randomly sampled reference row original = task_mixed$data(rows = 1:5) sampled = marginal_ref$sample(\"bill_length\", row_ids = 1:5)  # Compare data.table(     original_bill = original$bill_length,     sampled_bill = sampled$bill_length,     sex = original$sex # Unchanged ) #>    original_bill sampled_bill    sex #>            <num>        <num> <fctr> #> 1:          39.1         51.1   male #> 2:          39.5         41.1 female #> 3:          40.3         38.3 female #> 4:            NA         46.4   <NA> #> 5:          36.7         41.1 female # Sample with MarginalPermutationSampler (breaks correlations) perm = MarginalPermutationSampler$new(task_numeric) sampled_perm = perm$sample(c(\"x1\", \"x2\"), row_ids = 1:10)  # Sample with MarginalReferenceSampler (preserves within-row correlations) ref = MarginalReferenceSampler$new(task_numeric, n_samples = 50L) sampled_ref = ref$sample(c(\"x1\", \"x2\"), row_ids = 1:10)  # Check correlations cor_original = cor(task_numeric$data()$x1, task_numeric$data()$x2) cor_perm = cor(sampled_perm$x1, sampled_perm$x2) cor_ref = cor(sampled_ref$x1, sampled_ref$x2)  data.table(     method = c(\"Original\", \"Permutation\", \"Reference\"),     correlation = c(cor_original, cor_perm, cor_ref) ) #>         method correlation #>         <char>       <num> #> 1:    Original   0.8794067 #> 2: Permutation   0.2353592 #> 3:   Reference   0.8579748"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"conditional-samplers","dir":"Articles","previous_headings":"","what":"Conditional Samplers","title":"Feature Samplers","text":"Conditional samplers account dependencies features sampling \\(P(X_j | X_{-j})\\) rather marginal \\(P(X_j)\\). relevant features correlated. conditional samplers inherit ConditionalSampler support: Specifying features condition via conditioning_set $sample() $sample_newdata() methods Mixed feature types (depending specific sampler)","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"gaussian-conditional-sampler","dir":"Articles","previous_headings":"Conditional Samplers","what":"Gaussian Conditional Sampler","title":"Feature Samplers","text":"ConditionalGaussianSampler assumes features follow multivariate Gaussian distribution uses closed-form conditional distributions. Advantages: fast (model fitting sampling) Deterministic given seed hyperparameters Limitations: Assumes multivariate normality supports continuous features May produce --range values Notice sampled values respect conditional distribution - ’re different original plausible given conditioning features.","code":"# Create Gaussian conditional sampler gaussian = ConditionalGaussianSampler$new(task_numeric)  # Sample x1 conditioned on other features sampled = gaussian$sample(     feature = \"x1\",     row_ids = 1:10,     conditioning_set = c(\"x2\", \"x3\", \"x4\") )  # Compare original and conditionally sampled values original = task_numeric$data(rows = 1:10) data.table(     original = original$x1,     sampled = sampled$x1,     x2 = original$x2 # Conditioning feature (unchanged) ) #>        original       sampled         x2 #>           <num>         <num>      <num> #>  1: -1.05068376  0.1621207720 -0.5272128 #>  2: -2.06809208 -1.0256534013 -1.2991235 #>  3:  1.13656002  0.3037750530  1.3031674 #>  4: -1.67500747 -1.3601136679 -1.1771093 #>  5: -0.35705594 -0.4412497414 -0.3692326 #>  6: -0.13511336 -0.0008602544  0.2388833 #>  7:  0.88352949 -0.3288406905 -0.2852529 #>  8: -0.55523636 -1.6344225147 -1.3064160 #>  9: -0.47024600 -1.1859281874 -0.3218053 #> 10: -0.02536502 -0.5130306461 -0.4861314"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"arf-sampler","dir":"Articles","previous_headings":"Conditional Samplers","what":"ARF Sampler","title":"Feature Samplers","text":"ConditionalARFSampler uses Adversarial Random Forests model complex conditional distributions. ’s flexible conditional sampler. Advantages: Handles mixed feature types (continuous, categorical, ordered) distributional assumptions Captures non-linear relationships Limitations: Requires fitting ARF model (slower initialization sampling large data) hyperparameters tune Stochastic sampling Use CFI: ConditionalARFSampler default Conditional Feature Importance since can used task, unlike samplers.","code":"# Create ARF sampler (works with full task including categorical features) arf = ConditionalARFSampler$new(task_mixed, num_trees = 20, verbose = FALSE)  # Sample island conditioned on body measurements sampled = arf$sample(     feature = \"island\",     row_ids = 1:10,     conditioning_set = c(\"bill_length\", \"body_mass\") )  # Compare original and sampled island original = task_mixed$data(rows = 1:10) data.table(     original_island = original$island,     sampled_island = sampled$island,     bill_length = original$bill_length, # Conditioning feature     body_mass = original$body_mass # Conditioning feature ) #>     original_island sampled_island bill_length body_mass #>              <fctr>         <fctr>       <num>     <int> #>  1:       Torgersen         Biscoe        39.1      3750 #>  2:       Torgersen         Biscoe        39.5      3800 #>  3:       Torgersen      Torgersen        40.3      3250 #>  4:       Torgersen      Torgersen          NA        NA #>  5:       Torgersen          Dream        36.7      3450 #>  6:       Torgersen      Torgersen        39.3      3650 #>  7:       Torgersen      Torgersen        38.9      3625 #>  8:       Torgersen         Biscoe        39.2      4675 #>  9:       Torgersen      Torgersen        34.1      3475 #> 10:       Torgersen      Torgersen        42.0      4250"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"ctree-conditional-sampler","dir":"Articles","previous_headings":"Conditional Samplers","what":"Ctree Conditional Sampler","title":"Feature Samplers","text":"ConditionalCtreeSampler uses conditional inference trees partition feature space sample local neighborhoods. Advantages: Handles mixed feature types Interpretable tree structure Automatic feature selection (splits informative features) Limitations: Requires tree building (slower kNN) May produce duplicates terminal nodes small ctree sampler partitions observations based conditioning features samples within partition (terminal node).","code":"# Create ctree sampler ctree = ConditionalCtreeSampler$new(task_mixed)  # Sample with default parameters sampled = ctree$sample(     feature = \"bill_length\",     row_ids = 1:10,     conditioning_set = \"island\" )  original = task_mixed$data(rows = 1:10) data.table(     island = original$island, # Conditioning feature     original = original$bill_length,     sampled = sampled$bill_length ) #>        island original sampled #>        <fctr>    <num>   <num> #>  1: Torgersen     39.1    42.5 #>  2: Torgersen     39.5    46.0 #>  3: Torgersen     40.3    39.5 #>  4: Torgersen       NA    35.2 #>  5: Torgersen     36.7    38.5 #>  6: Torgersen     39.3    41.4 #>  7: Torgersen     38.9    42.0 #>  8: Torgersen     39.2    34.6 #>  9: Torgersen     34.1    42.1 #> 10: Torgersen     42.0    42.5"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"knn-conditional-sampler","dir":"Articles","previous_headings":"Conditional Samplers","what":"kNN Conditional Sampler","title":"Feature Samplers","text":"ConditionalKNNSampler finds k nearest neighbors based conditioning features samples . Advantages: simple intuitive Fast (model fitting) Single hyperparameter (k) Automatic distance metric selection Supports mixed feature types (numeric, categorical, ordered, logical) Limitations: Sensitive choice k May produce duplicates k small Distance metric: sampler automatically selects appropriate distance metric based conditioning features: Euclidean distance conditioning features numeric/integer (standardized) Gower distance conditioning features categorical (factor, ordered, logical)","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"example-1-all-numeric-conditioning-euclidean-distance","dir":"Articles","previous_headings":"Conditional Samplers > kNN Conditional Sampler","what":"Example 1: All-numeric conditioning (Euclidean distance)","title":"Feature Samplers","text":"","code":"# Create kNN sampler with k=5 neighbors knn_numeric = ConditionalKNNSampler$new(task_numeric, k = 5)  # Sample x1 based on nearest neighbors in (x2, x3) space sampled_numeric = knn_numeric$sample(     feature = \"x1\",     row_ids = 1:5,     conditioning_set = c(\"x2\", \"x3\") )  original_numeric = task_numeric$data(rows = 1:5) data.table(     x2 = original_numeric$x2,     x3 = original_numeric$x3,     original_x1 = original_numeric$x1,     sampled_x1 = sampled_numeric$x1 ) #>            x2         x3 original_x1  sampled_x1 #>         <num>      <num>       <num>       <num> #> 1: -0.5272128  1.7361110  -1.0506838 -0.04454979 #> 2: -1.2991235 -0.8452478  -2.0680921 -1.85485761 #> 3:  1.3031674 -0.9615715   1.1365600  1.54017968 #> 4: -1.1771093  1.0174911  -1.6750075 -1.67500747 #> 5: -0.3692326 -1.4960537  -0.3570559 -0.35705594"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"example-2-mixed-type-conditioning-gower-distance","dir":"Articles","previous_headings":"Conditional Samplers > kNN Conditional Sampler","what":"Example 2: Mixed-type conditioning (Gower distance)","title":"Feature Samplers","text":"kNN sampler finds k similar observations (based conditioning features) samples feature values. distance metric chosen automatically based feature types.","code":"# Use task with categorical features knn_mixed = ConditionalKNNSampler$new(task_mixed, k = 5)  # Sample bill_length conditioning on island (categorical) and body_mass (numeric) sampled_mixed = knn_mixed$sample(     feature = \"bill_length\",     row_ids = 1:5,     conditioning_set = c(\"island\", \"body_mass\") )  original_mixed = task_mixed$data(rows = 1:5) data.table(     island = original_mixed$island,     body_mass = original_mixed$body_mass,     original_bill = original_mixed$bill_length,     sampled_bill = sampled_mixed$bill_length ) #>       island body_mass original_bill sampled_bill #>       <fctr>     <int>         <num>        <num> #> 1: Torgersen      3750          39.1           NA #> 2: Torgersen      3800          39.5         36.7 #> 3: Torgersen      3250          40.3         37.8 #> 4: Torgersen        NA            NA         37.8 #> 5: Torgersen      3450          36.7         40.2"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"knockoff-samplers","dir":"Articles","previous_headings":"","what":"Knockoff Samplers","title":"Feature Samplers","text":"Now ’ve seen conditional samplers, can understand important limitation knockoff samplers: unlike conditional samplers , knockoffs don’t support arbitrary conditioning sets. Knockoff samplers create synthetic features (knockoffs) satisfy specific statistical properties. must fulfill knockoff swap property: swapping feature knockoff change joint distribution. Knockoffs separate category : require special construction satisfy theoretical guarantees, implementations limited don’t support conditional sampling arbitrary conditioning sets ’re used primarily feature selection FDR control, general importance","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"gaussian-knockoffs","dir":"Articles","previous_headings":"Knockoff Samplers","what":"Gaussian Knockoffs","title":"Feature Samplers","text":"multivariate Gaussian data, can construct exact knockoffs: Key properties knockoffs: Different values similar statistical properties Pairwise exchangeability originals Preserve correlation structure specify features condition (determined knockoff construction) Conditional Independence Testing: Knockoffs particularly relevant conditional independence testing implemented cpi package. can combine knockoff samplers CFI perform inference: See vignette(\"inference\") details statistical inference feature importance. Key takeaways: Permutation sampling produces value marginal distribution Conditional samplers produce values consistent conditioning features Choice sampler affects feature importance estimates, especially features correlated","code":"# Create Gaussian knockoff sampler (using task_numeric from earlier) knockoff = KnockoffGaussianSampler$new(task_numeric)  # Generate knockoffs original = task_numeric$data(rows = 1:5) knockoffs = knockoff$sample(     feature = task_numeric$feature_names,     row_ids = 1:5 )  # Original vs knockoff values data.table(     x1_original = original$x1,     x1_knockoff = knockoffs$x1,     x2_original = original$x2,     x2_knockoff = knockoffs$x2 ) #>    x1_original x1_knockoff x2_original x2_knockoff #>          <num>       <num>       <num>       <num> #> 1:  -1.0506838  -0.7513172  -0.5272128   -1.433181 #> 2:  -2.0680921  -0.6129308  -1.2991235   -1.183605 #> 3:   1.1365600   1.6247546   1.3031674    1.642699 #> 4:  -1.6750075  -0.8129150  -1.1771093   -1.450024 #> 5:  -0.3570559  -1.2109496  -0.3692326   -1.074235 # CFI with knockoff sampler for conditional independence testing cfi_knockoff = CFI$new(     task = task_numeric,     learner = lrn(\"regr.ranger\"),     measure = msr(\"regr.mse\"),     sampler = knockoff )  # Compute importance with CPI-based inference cfi_knockoff$compute() cfi_knockoff$importance(ci_method = \"cpi\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/feature-samplers.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Feature Samplers","text":"General guidelines: Use permutation sampling features independent baseline PFI Use Gaussian conditional fast conditional sampling continuous features Use ARF flexible conditional sampling mixed types Use kNN simple, fast conditional sampling Use ctree want interpretable conditional sampling Use knockoffs need theoretical guarantees feature selection inference","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Inference for Feature Importance","text":"use simple linear DGP demonstration purposes \\(X_1\\) \\(X_2\\) strongly correlated (r = 0.7) \\(X_1\\) \\(X_3\\) effect Y \\(X_2\\) \\(X_4\\) don’t effect DAG correlated features DGP","code":"task = sim_dgp_correlated(n = 2000, r = 0.7) learner = lrn(\"regr.ranger\", num.trees = 500) measure = msr(\"regr.mse\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"resampling-based-variability","dir":"Articles","previous_headings":"","what":"Resampling-based variability","title":"Inference for Feature Importance","text":"compute feature importance resampling — example, subsampling multiple repeats — resampling iteration yields separate importance estimate. default, $importance() simply averages estimates without reporting measure variability: several ways quantify variability estimates, ranging purely descriptive parametric inference.","code":"pfi = PFI$new(     task = task,     learner = learner,     resampling = rsmp(\"subsampling\", repeats = 15),     measure = measure,     n_repeats = 20 # for stability within resampling iters )  pfi$compute() pfi$importance() #> Key: <feature> #>    feature   importance #>     <char>        <num> #> 1:      x1  6.476555494 #> 2:      x2  0.095842584 #> 3:      x3  1.793989195 #> 4:      x4 -0.000962437"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"empirical-quantiles","dir":"Articles","previous_headings":"Resampling-based variability","what":"Empirical quantiles","title":"Inference for Feature Importance","text":"simplest approach look empirical distribution importance scores across resampling iterations. formal inference method — merely describes observed variability without distributional assumptions. \"quantile\" method returns confidence bounds (conf_lower, conf_upper) without se, statistic, p.value, since empirical quantiles statistical test. conf_* naming kept consistency methods ease visualization.","code":"pfi_ci_quantile = pfi$importance(ci_method = \"quantile\", alternative = \"two.sided\") pfi_ci_quantile #> Key: <feature> #>    feature   importance   conf_lower   conf_upper #>     <char>        <num>        <num>        <num> #> 1:      x1  6.476555494  6.108602488 6.8752121255 #> 2:      x2  0.095842584  0.071144370 0.1213703258 #> 3:      x3  1.793989195  1.736434146 1.8848348302 #> 4:      x4 -0.000962437 -0.003342628 0.0006686405"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"raw-confidence-intervals","dir":"Articles","previous_headings":"Resampling-based variability","what":"Raw confidence intervals","title":"Inference for Feature Importance","text":"natural next step assume importance scores approximately normally distributed across resampling iterations compute t-based confidence intervals. However, unadjusted confidence intervals narrow hence invalid inference: resampling iterations share overlapping training sets, violating independence assumption underlying t-distribution. parametric CI methods (\"raw\" \"nadeau_bengio\") return se, statistic, p.value, conf_lower, conf_upper. alternative parameter controls whether one-sided test (\"greater\", default, testing H0: importance <= 0) two-sided test (\"two.sided\") performed. use alternative = \"two.sided\" visualization purposes conf_upper finite.","code":"pfi_ci_raw = pfi$importance(ci_method = \"raw\", alternative = \"two.sided\") pfi_ci_raw #> Key: <feature> #>    feature   importance           se  statistic      p.value  conf_lower #>     <char>        <num>        <num>      <num>        <num>       <num> #> 1:      x1  6.476555494 0.0707450088  91.547879 7.519530e-21  6.32482254 #> 2:      x2  0.095842584 0.0042155496  22.735490 1.879277e-12  0.08680113 #> 3:      x3  1.793989195 0.0132950768 134.936355 3.311520e-23  1.76547409 #> 4:      x4 -0.000962437 0.0002849623  -3.377418 4.511052e-03 -0.00157362 #>       conf_upper #>            <num> #> 1:  6.6282884472 #> 2:  0.1048840389 #> 3:  1.8225042988 #> 4: -0.0003512536"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"corrected-t-test-nadeau-bengio","dir":"Articles","previous_headings":"Resampling-based variability","what":"Corrected t-test (Nadeau & Bengio)","title":"Inference for Feature Importance","text":"account dependence resampling iterations, can use correction proposed Nadeau & Bengio (2003) recommended Molnar et al. (2023). inflates variance estimate account overlap training sets, yielding wider (honest) confidence intervals: Nadeau-Bengio correction provides better (still imperfect) coverage compared raw approach.","code":"pfi_ci_corrected = pfi$importance(ci_method = \"nadeau_bengio\", alternative = \"two.sided\") pfi_ci_corrected #> Key: <feature> #>    feature   importance           se statistic      p.value   conf_lower #>     <char>        <num>        <num>     <num>        <num>        <num> #> 1:      x1  6.476555494 0.2063236235 31.390276 2.231930e-14  6.034035333 #> 2:      x2  0.095842584 0.0122944003  7.795629 1.848448e-06  0.069473718 #> 3:      x3  1.793989195 0.0387743032 46.267477 1.027030e-16  1.710826586 #> 4:      x4 -0.000962437 0.0008310757 -1.158062 2.662136e-01 -0.002744917 #>      conf_upper #>           <num> #> 1: 6.9190756552 #> 2: 0.1222114505 #> 3: 1.8771518043 #> 4: 0.0008200431"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"comparison","dir":"Articles","previous_headings":"Resampling-based variability","what":"Comparison","title":"Inference for Feature Importance","text":"highlight differences three approaches, visualize side side:  results highlight just optimistic unadjusted, raw confidence intervals .","code":"pfi_cis = rbindlist(     list(         pfi_ci_raw[, type := \"raw\"],         pfi_ci_corrected[, type := \"nadeau_bengio\"],         pfi_ci_quantile[, type := \"quantile\"]     ),     fill = TRUE )  ggplot(pfi_cis, aes(y = feature, color = type)) +     geom_errorbar(         aes(xmin = conf_lower, xmax = conf_upper),         position = position_dodge(width = 0.6),         width = .5     ) +     geom_point(aes(x = importance), position = position_dodge(width = 0.6)) +     scale_color_brewer(palette = \"Set2\") +     labs(         title = \"Parametric & non-parametric CI methods\",         subtitle = \"RF with 15 subsampling iterations\",         color = NULL     ) +     theme_minimal(base_size = 14) +     theme(legend.position = \"bottom\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"multiplicity-correction","dir":"Articles","previous_headings":"Resampling-based variability","what":"Multiplicity correction","title":"Inference for Feature Importance","text":"testing many features simultaneously, p-values adjusted control overall error rate. two main frameworks multiplicity correction: Family-wise error rate (FWER): probability making least one false rejection among tests. FWER control conservative approach appropriate single false positive costly. Methods include Bonferroni (\\(\\alpha / k\\)) Holm (step-variant uniformly powerful Bonferroni still controlling FWER). False discovery rate (FDR): expected proportion false rejections among rejected hypotheses. FDR control less conservative better suited exploratory analyses false positives tolerable. common method Benjamini-Hochberg (BH). p_adjust parameter accepts method stats::p.adjust.methods defaults \"none\" (adjustment). Bonferroni correction (FWER control), p-values confidence intervals adjusted. confidence intervals use adjusted significance level \\(\\alpha / k\\) \\(k\\) number features: sequential adaptive procedures Holm (FWER) Benjamini-Hochberg (FDR), p-values adjusted. methods yield clean per-comparison \\(\\alpha\\) used CI construction, confidence intervals remain nominal level: applies ci_methods produce p-values (\"raw\", \"nadeau_bengio\", \"cpi\", \"lei\").","code":"pfi$importance(ci_method = \"nadeau_bengio\", alternative = \"two.sided\", p_adjust = \"none\") #> Key: <feature> #>    feature   importance           se statistic      p.value   conf_lower #>     <char>        <num>        <num>     <num>        <num>        <num> #> 1:      x1  6.476555494 0.2063236235 31.390276 2.231930e-14  6.034035333 #> 2:      x2  0.095842584 0.0122944003  7.795629 1.848448e-06  0.069473718 #> 3:      x3  1.793989195 0.0387743032 46.267477 1.027030e-16  1.710826586 #> 4:      x4 -0.000962437 0.0008310757 -1.158062 2.662136e-01 -0.002744917 #>      conf_upper #>           <num> #> 1: 6.9190756552 #> 2: 0.1222114505 #> 3: 1.8771518043 #> 4: 0.0008200431 pfi$importance(ci_method = \"nadeau_bengio\", alternative = \"two.sided\", p_adjust = \"bonferroni\") #> Key: <feature> #>    feature   importance           se statistic      p.value   conf_lower #>     <char>        <num>        <num>     <num>        <num>        <num> #> 1:      x1  6.476555494 0.2063236235 31.390276 8.927721e-14  5.748317045 #> 2:      x2  0.095842584 0.0122944003  7.795629 7.393792e-06  0.052448353 #> 3:      x3  1.793989195 0.0387743032 46.267477 4.108119e-16  1.657131680 #> 4:      x4 -0.000962437 0.0008310757 -1.158062 1.000000e+00 -0.003895796 #>     conf_upper #>          <num> #> 1: 7.204793944 #> 2: 0.139236816 #> 3: 1.930846710 #> 4: 0.001970922 pfi$importance(ci_method = \"nadeau_bengio\", alternative = \"two.sided\", p_adjust = \"BH\") #> Key: <feature> #>    feature   importance           se statistic      p.value   conf_lower #>     <char>        <num>        <num>     <num>        <num>        <num> #> 1:      x1  6.476555494 0.2063236235 31.390276 4.463861e-14  6.034035333 #> 2:      x2  0.095842584 0.0122944003  7.795629 2.464597e-06  0.069473718 #> 3:      x3  1.793989195 0.0387743032 46.267477 4.108119e-16  1.710826586 #> 4:      x4 -0.000962437 0.0008310757 -1.158062 2.662136e-01 -0.002744917 #>      conf_upper #>           <num> #> 1: 6.9190756552 #> 2: 0.1222114505 #> 3: 1.8771518043 #> 4: 0.0008200431"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"observation-wise-inference-on-test-data","dir":"Articles","previous_headings":"","what":"Observation-wise inference on test data","title":"Inference for Feature Importance","text":"resampling-based approaches quantify variability due choice train/test split. different approach uses observation-wise loss differences test set formal statistical inference. idea straightforward: test observation, compare loss without feature (either perturbing feature retraining model without ). resulting per-observation importance scores ..d. appropriate conditions, enabling standard statistical tests. Inference guaranteed valid single train/test split (holdout), test observations truly ..d. model fixed. resampling strategies may used come caveats (see ). CPI / cARFi conditional feature importance (perturbation-based, model fixed per resampling iteration) Lei et al. LOCO importance (retraining-based, model refitted without feature)","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"conditional-predictive-impact-cpi","dir":"Articles","previous_headings":"Observation-wise inference on test data","what":"Conditional predictive impact (CPI)","title":"Inference for Feature Importance","text":"CPI (Conditional Predictive Impact) introduced Watson & Wright (2021) statistical inference conditional feature importance using knockoffs. Two main approaches supported: CPI knockoffs: original method using model-X knockoffs conditional sampling. cARFi (Blesch et al., 2025): Uses Adversarial Random Forests conditional sampling, works without Gaussian assumptions supports mixed data. CPI originally implemented cpi package. works mlr3 output data looks like :","code":"library(cpi)  resampling = rsmp(\"cv\", folds = 5) resampling$instantiate(task) cpi_res = cpi(     task = task,     learner = learner,     resampling = resampling,     measure = measure,     test = \"t\" ) setDT(cpi_res) setnames(cpi_res, \"Variable\", \"feature\") cpi_res[, method := \"CPI\"]  cpi_res #>    feature           CPI          SE   test  statistic      estimate #>     <char>         <num>       <num> <char>      <num>         <num> #> 1:      x1  4.5092929912 0.147978747      t 30.4725717  4.5092929912 #> 2:      x2  0.0029567621 0.003362453      t  0.8793467  0.0029567621 #> 3:      x3  1.8536055915 0.060102228      t 30.8408797  1.8536055915 #> 4:      x4 -0.0008885688 0.000770307      t -1.1535255 -0.0008885688 #>          p.value        ci.lo method #>            <num>        <num> <char> #> 1: 3.859678e-168  4.265776760    CPI #> 2:  1.896595e-01 -0.002576545    CPI #> 3: 1.768282e-171  1.754700388    CPI #> 4:  8.755837e-01 -0.002156198    CPI"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"cpi-with-knockoffs","dir":"Articles","previous_headings":"Observation-wise inference on test data > Conditional predictive impact (CPI)","what":"CPI with knockoffs","title":"Inference for Feature Importance","text":"Since xplainfi also includes knockoffs via KnockoffSampler KnockoffGaussianSampler, latter implementing second order Gaussian knockoffs also used default cpi, can recreate results using CFI corresponding sampler. CFI knockoff sampler supports CPI inference directly via ci_method = \"cpi\". results similar computed cpi(), let’s compare :  notable caveat knockoff approach readily available mixed data (categorical features).","code":"knockoff_gaussian = KnockoffGaussianSampler$new(task)  cfi = CFI$new(     task = task,     learner = learner,     resampling = resampling,     measure = measure,     sampler = knockoff_gaussian,     n_repeats = 1 # generate 1 knockoff matrix, like cpi() )  cfi$compute()  cfi_cpi_res = cfi$importance(ci_method = \"cpi\") #> Warning: Observation-wise inference was validated with a single test set. #> ! Current resampling has 5 iterations. #> ℹ With cross-validation, models are fit on overlapping training data, which may #>   affect coverage. #> ℹ With bootstrap or subsampling, test observations are not i.i.d. #> ℹ See `vignette(\"inference\", package = \"xplainfi\")` for details. cfi_cpi_res #> Key: <feature> #>    feature   importance           se  statistic       p.value   conf_lower #>     <char>        <num>        <num>      <num>         <num>        <num> #> 1:      x1  4.362355571 0.1355320358 32.1868962 1.697321e-183  4.096556727 #> 2:      x2  0.001742650 0.0029223989  0.5963079  5.510371e-01 -0.003988617 #> 3:      x3  1.769344462 0.0548657780 32.2485988 4.580895e-184  1.661744364 #> 4:      x4 -0.000470146 0.0009370589 -0.5017251  6.159162e-01 -0.002307860 #>     conf_upper #>          <num> #> 1: 4.628154415 #> 2: 0.007473916 #> 3: 1.876944560 #> 4: 0.001367568  # Rename columns to match cpi package output for comparison setnames(cfi_cpi_res, c(\"importance\", \"conf_lower\"), c(\"CPI\", \"ci.lo\")) cfi_cpi_res[, method := \"CFI+Knockoffs\"] rbindlist(list(cpi_res, cfi_cpi_res), fill = TRUE) |>     ggplot(aes(y = feature, x = CPI, color = method)) +     geom_point(position = position_dodge(width = 0.3)) +     geom_errorbar(         aes(xmin = CPI, xmax = ci.lo),         position = position_dodge(width = 0.3),         width = 0.5     ) +     scale_color_brewer(palette = \"Dark2\") +     labs(         title = \"CPI and CFI with Knockoff sampler\",         subtitle = \"RF with 5-fold CV\",         color = NULL     ) +     theme_minimal(base_size = 14) +     theme(legend.position = \"top\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"carfi-cpi-with-arf","dir":"Articles","previous_headings":"Observation-wise inference on test data > Conditional predictive impact (CPI)","what":"cARFi: CPI with ARF","title":"Inference for Feature Importance","text":"alternative available using ARF conditional sampler rather knockoffs. approach, called cARFi, introduced Blesch et al. (2025) works without Gaussian assumptions: can now compare three methods:  expected, ARF-based approach differs knockoff-based approaches, roughly agreement. Note resampling strategy: CPI inference validated using holdout (single train/test split), inference provably valid setting. cross-validation, test observations still ..d., models fit overlapping training data — technically, affects inference reason Nadeau & Bengio correction exists resampling-based CIs. bootstrap subsampling, test observations may longer ..d. (due repeated observations across test sets) training sets overlap. practice, Watson & Wright (2021) found empirical results strongly depend choice risk estimator, inference guaranteed valid holdout. resampling strategies employed understanding coverage guarantees may hold.","code":"arf_sampler = ConditionalARFSampler$new(     task = task,     finite_bounds = \"local\",     min_node_size = 20,     epsilon = 1e-15 )  cfi_arf = CFI$new(     task = task,     learner = learner,     resampling = resampling,     measure = measure,     sampler = arf_sampler )  cfi_arf$compute()  # CPI inference with ARF sampler (cARFi) cfi_arf_res = cfi_arf$importance(ci_method = \"cpi\") #> Warning: Observation-wise inference was validated with a single test set. #> ! Current resampling has 5 iterations. #> ℹ With cross-validation, models are fit on overlapping training data, which may #>   affect coverage. #> ℹ With bootstrap or subsampling, test observations are not i.i.d. #> ℹ See `vignette(\"inference\", package = \"xplainfi\")` for details. cfi_arf_res #> Key: <feature> #>    feature    importance           se statistic    p.value    conf_lower #>     <char>         <num>        <num>     <num>      <num>         <num> #> 1:      x1  3.9865071718 0.0715166883 55.742335 0.00000000  3.8462521170 #> 2:      x2  0.0053611218 0.0025570164  2.096632 0.03615158  0.0003464254 #> 3:      x3  1.7527732858 0.0325190020 53.899972 0.00000000  1.6889985989 #> 4:      x4 -0.0009391978 0.0004918991 -1.909330 0.05636256 -0.0019038865 #>      conf_upper #>           <num> #> 1: 4.126762e+00 #> 2: 1.037582e-02 #> 3: 1.816548e+00 #> 4: 2.549088e-05  # Rename columns to match cpi package output for comparison setnames(cfi_arf_res, c(\"importance\", \"conf_lower\"), c(\"CPI\", \"ci.lo\")) cfi_arf_res[, method := \"CFI+ARF\"] rbindlist(list(cpi_res, cfi_cpi_res, cfi_arf_res), fill = TRUE) |>     ggplot(aes(y = feature, x = CPI, color = method)) +     geom_point(position = position_dodge(width = 0.3)) +     geom_errorbar(         aes(xmin = CPI, xmax = ci.lo),         position = position_dodge(width = 0.3),         width = 0.5     ) +     scale_color_brewer(palette = \"Dark2\") +     labs(         title = \"CPI and CFI with Knockoffs and ARF\",         subtitle = \"RF with 5-fold CV\",         color = NULL     ) +     theme_minimal(base_size = 14) +     theme(legend.position = \"top\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"statistical-tests-with-cpi","dir":"Articles","previous_headings":"Observation-wise inference on test data > Conditional predictive impact (CPI)","what":"Statistical tests with CPI","title":"Inference for Feature Importance","text":"CPI can also perform additional tests besides default t-test, specifically Wilcoxon-, Fisher-, binomial test:  choice test depends distributional assumptions: t-test assumes normality, Fisher Wilcoxon non-parametric alternatives. Following Watson & Wright (2021), can additionally apply Benjamini-Hochberg FDR control (p_adjust = \"BH\"), often desirable exploratory settings many features tested simultaneously: Note multiplcity adjustment general case limited p-values, general Bonferroni method easily applicable confidence intervals.","code":"(cpi_res_wilcoxon = cfi_arf$importance(ci_method = \"cpi\", test = \"wilcoxon\")) #> Warning: Observation-wise inference was validated with a single test set. #> ! Current resampling has 5 iterations. #> ℹ With cross-validation, models are fit on overlapping training data, which may #>   affect coverage. #> ℹ With bootstrap or subsampling, test observations are not i.i.d. #> ℹ See `vignette(\"inference\", package = \"xplainfi\")` for details. #> Key: <feature> #>    feature    importance    se statistic      p.value    conf_lower #>     <char>         <num> <num>     <num>        <num>         <num> #> 1:      x1  3.9865071718    NA   2001000 0.000000e+00  3.2122074217 #> 2:      x2  0.0053611218    NA   1194853 5.295553e-14  0.0029276504 #> 3:      x3  1.7527732858    NA   2000886 0.000000e+00  1.3958358130 #> 4:      x4 -0.0009391978    NA   1031351 2.323267e-01 -0.0001657344 #>      conf_upper #>           <num> #> 1: 3.4270970290 #> 2: 0.0049551400 #> 3: 1.4966237507 #> 4: 0.0006339549 # Fisher test with same default for B as in cpi() (cpi_res_fisher = cfi_arf$importance(ci_method = \"cpi\", test = \"fisher\", B = 1999)) #> Warning: Observation-wise inference was validated with a single test set. #> ! Current resampling has 5 iterations. #> ℹ With cross-validation, models are fit on overlapping training data, which may #>   affect coverage. #> ℹ With bootstrap or subsampling, test observations are not i.i.d. #> ℹ See `vignette(\"inference\", package = \"xplainfi\")` for details. #> Key: <feature> #>    feature    importance    se     statistic p.value    conf_lower   conf_upper #>     <char>         <num> <num>         <num>   <num>         <num>        <num> #> 1:      x1  3.9865071718    NA  3.9865071718  0.0005  3.7697941737 4.203211e+00 #> 2:      x2  0.0053611218    NA  0.0053611218  0.0365  0.0001883662 1.032052e-02 #> 3:      x3  1.7527732858    NA  1.7527732858  0.0005  1.6527820910 1.848633e+00 #> 4:      x4 -0.0009391978    NA -0.0009391978  0.0650 -0.0019275157 7.687918e-05 (cpi_res_binom = cfi_arf$importance(ci_method = \"cpi\", test = \"binomial\")) #> Warning: Observation-wise inference was validated with a single test set. #> ! Current resampling has 5 iterations. #> ℹ With cross-validation, models are fit on overlapping training data, which may #>   affect coverage. #> ℹ With bootstrap or subsampling, test observations are not i.i.d. #> ℹ See `vignette(\"inference\", package = \"xplainfi\")` for details. #> Key: <feature> #>    feature    importance    se statistic      p.value conf_lower conf_upper #>     <char>         <num> <num>     <num>        <num>      <num>      <num> #> 1:      x1  3.9865071718    NA      2000 0.000000e+00  0.9981573  1.0000000 #> 2:      x2  0.0053611218    NA      1215 5.988736e-22  0.5857045  0.6289798 #> 3:      x3  1.7527732858    NA      1995 0.000000e+00  0.9941756  0.9991878 #> 4:      x4 -0.0009391978    NA      1094 2.860106e-05  0.5248785  0.5689835  rbindlist(     list(         cfi_arf$importance(ci_method = \"cpi\")[, test := \"t\"],         cpi_res_wilcoxon[, test := \"Wilcoxon\"],         cpi_res_fisher[, test := \"Fisher\"],         cpi_res_binom[, test := \"Binomial\"]     ),     fill = TRUE ) |>     ggplot(aes(y = feature, x = importance, color = test)) +     geom_point(position = position_dodge(width = 0.3)) +     geom_errorbar(         aes(xmin = importance, xmax = conf_lower),         position = position_dodge(width = 0.3),         width = 0.5     ) +     scale_color_brewer(palette = \"Dark2\") +     labs(         title = \"CPI test with CFI/ARF\",         subtitle = \"RF with 5-fold CV\",         color = \"Test\"     ) +     theme_minimal(base_size = 14) +     theme(legend.position = \"top\") #> Warning: Observation-wise inference was validated with a single test set. #> ! Current resampling has 5 iterations. #> ℹ With cross-validation, models are fit on overlapping training data, which may #>   affect coverage. #> ℹ With bootstrap or subsampling, test observations are not i.i.d. #> ℹ See `vignette(\"inference\", package = \"xplainfi\")` for details. cfi_arf$importance(ci_method = \"cpi\", p_adjust = \"BH\") #> Warning: Observation-wise inference was validated with a single test set. #> ! Current resampling has 5 iterations. #> ℹ With cross-validation, models are fit on overlapping training data, which may #>   affect coverage. #> ℹ With bootstrap or subsampling, test observations are not i.i.d. #> ℹ See `vignette(\"inference\", package = \"xplainfi\")` for details. #> Key: <feature> #>    feature    importance           se statistic    p.value    conf_lower #>     <char>         <num>        <num>     <num>      <num>         <num> #> 1:      x1  3.9865071718 0.0715166883 55.742335 0.00000000  3.8462521170 #> 2:      x2  0.0053611218 0.0025570164  2.096632 0.04820210  0.0003464254 #> 3:      x3  1.7527732858 0.0325190020 53.899972 0.00000000  1.6889985989 #> 4:      x4 -0.0009391978 0.0004918991 -1.909330 0.05636256 -0.0019038865 #>      conf_upper #>           <num> #> 1: 4.126762e+00 #> 2: 1.037582e-02 #> 3: 1.816548e+00 #> 4: 2.549088e-05"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"distribution-free-inference-for-loco-lei-et-al--2018","dir":"Articles","previous_headings":"Observation-wise inference on test data","what":"Distribution-free inference for LOCO (Lei et al., 2018)","title":"Inference for Feature Importance","text":"Lei et al. (2018) proposed distribution-free inference LOCO importance based observation-wise loss differences. Unlike CPI/cARFi, LOCO requires retraining model without feature. inference conditional training data \\(D_1\\) single train/test split: observation-wise loss differences test set ..d. given \\(D_1\\), enabling nonparametric tests. idea test whether excess test error removing feature \\(j\\) significantly greater zero: \\[ \\theta_j = \\mathrm{med}\\left(     |Y - \\hat{f}_{n_1}^{-j}(X)| - |Y - \\hat{f}_{n_1}(X)| \\big| D_1 \\right) \\] paper proposes: L1 (absolute) loss measure Median aggregation function single train/test split (holdout), since inference conditional training data Wilcoxon signed-rank test (sign test) inference Bonferroni correction multiple comparisons available xplainfi via ci_method = \"lei\": ci_method = \"lei\" method works observation-wise loss differences internally, using median default point estimate Wilcoxon signed-rank test confidence intervals p-values.","code":"# mae has L1 loss on observation-level, the \"mean\" aggregation is ignored here measure_mae = msr(\"regr.mae\")  loco = LOCO$new(     task = sim_dgp_correlated(n = 2000, r = 0.7),     learner = lrn(\"regr.ranger\", num.trees = 500),     resampling = rsmp(\"holdout\"),     measure = measure_mae )  loco$compute() loco$importance(     ci_method = \"lei\",     alternative = \"two.sided\",     p_adjust = \"bonferroni\",     aggregator = median # default spelled out explicitly ) #> Key: <feature> #>    feature importance    se statistic      p.value conf_lower conf_upper #>     <char>      <num> <num>     <num>        <num>      <num>      <num> #> 1:      x1 0.80833037    NA    216167 9.828138e-98 0.82052360 1.00057066 #> 2:      x2 0.01819719    NA    135421 5.538819e-06 0.01210851 0.03837064 #> 3:      x3 0.52495928    NA    211296 5.603680e-89 0.51589328 0.64318191 #> 4:      x4 0.01989962    NA    146280 9.639960e-12 0.01774627 0.03932460"},{"path":"https://mlr-org.github.io/xplainfi/articles/inference.html","id":"configurable-parameters","dir":"Articles","previous_headings":"Observation-wise inference on test data > Distribution-free inference for LOCO (Lei et al., 2018)","what":"Configurable parameters","title":"Inference for Feature Importance","text":"components proposed Lei et al. defaults can customized: test: \"wilcoxon\" (default), \"t\", \"fisher\", \"binomial\" (sign test) aggregator: defaults stats::median, can changed (e.g. mean) p_adjust: p-value correction multiple comparisons, accepts method stats::p.adjust.methods (e.g. \"bonferroni\", \"holm\", \"BH\", \"none\"). Default \"none\". \"bonferroni\", confidence intervals also adjusted (alpha/k). methods like \"holm\" \"BH\", p-values adjusted sequential/adaptive procedures yield clean per-comparison alpha CI construction.","code":"loco$importance(     ci_method = \"lei\",     test = \"t\",     aggregator = mean,     alternative = \"two.sided\",     p_adjust = \"holm\" ) #> Key: <feature> #>    feature importance          se statistic       p.value conf_lower conf_upper #>     <char>      <num>       <num>     <num>         <num>      <num>      <num> #> 1:      x1 0.97220860 0.033777130 28.783043 2.152723e-118 0.90588611 1.03853109 #> 2:      x2 0.04079587 0.007403802  5.510125  5.128595e-08 0.02625827 0.05533348 #> 3:      x3 0.61203461 0.022878509 26.751508 3.754000e-107 0.56711192 0.65695730 #> 4:      x4 0.04338934 0.005255062  8.256675  1.611160e-15 0.03307085 0.05370782"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"example-data-interaction-effects","dir":"Articles","previous_headings":"","what":"Example Data: Interaction Effects","title":"LOCO and WVIM","text":"illustrate LOCO feature importance, ’ll use data generating process interaction effects: \\[y = 2 \\cdot x_1 \\cdot x_2 + x_3 + \\epsilon\\] \\(\\epsilon \\sim N(0, 0.5^2)\\) features \\(x_1, x_2, x_3, noise_1, noise_2 \\sim N(0,1)\\) independent. Key characteristics: \\(x_1, x_2\\): individual effects, interact \\(x_3\\): direct main effect \\(y\\) \\(noise_1, noise_2\\): Pure noise variables effect \\(y\\) setup demonstrates LOCO handles interaction effects.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"leave-one-covariate-out-loco","dir":"Articles","previous_headings":"","what":"Leave-One-Covariate-Out (LOCO)","title":"LOCO and WVIM","text":"LOCO measures feature importance comparing model performance without feature. feature, learner retrained without feature performance difference indicates feature’s importance. feature \\(j\\), LOCO calculated difference expected loss model fit without feature full model: \\[\\text{LOCO}_j = \\mathbb{E}(L(Y, f_{-j}(X_{-j}))) - \\mathbb{E}(L(Y, f(X)))\\] Higher values indicate important features (larger performance drop removed). $importance() method returns data.table aggregated importance scores per feature.","code":"task <- sim_dgp_interactions(n = 2000) learner <- lrn(\"regr.nnet\", trace = FALSE) measure <- msr(\"regr.mse\")  resampling <- rsmp(\"holdout\") resampling$instantiate(task)  loco <- LOCO$new(     task = task,     learner = learner,     measure = measure,     resampling = resampling )  loco$compute() loco$importance() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:  noise1 -0.7663935 #> 2:  noise2 -0.5653578 #> 3:      x1  2.8024966 #> 4:      x2  2.8173232 #> 5:      x3  0.7637862"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"understanding-the-results","dir":"Articles","previous_headings":"","what":"Understanding the Results","title":"LOCO and WVIM","text":"LOCO results interpretation: \\(x_3\\) show high importance due direct main effect \\(x_1\\) \\(x_2\\) show variable importance depending model’s ability capture interactions \\(noise_1\\) \\(noise_2\\) show low negative importance demonstrates LOCO measures feature’s contribution model performance","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"detailed-scores","dir":"Articles","previous_headings":"","what":"Detailed Scores","title":"LOCO and WVIM","text":"$scores() method provides detailed information feature, resampling iteration, refit: LOCO scores baseline post-refit score","code":"loco$scores() |>     knitr::kable(digits = 4, caption = \"LOCO scores with baseline and post-refit score\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"multiple-refits","dir":"Articles","previous_headings":"","what":"Multiple Refits","title":"LOCO and WVIM","text":"LOCO also supports n_repeats multiple refits within resampling iteration, improves stability: First 10 LOCO scores per refit resampling fold Since refit requires retrain provided learner, course increases computational load. Suitable values depend resources available, doubt usually better overshoot encounter diminishing returns rather undershooting getting unreliable results.","code":"loco_multi = LOCO$new(     task = task,     learner = learner,     measure = measure,     resampling = rsmp(\"cv\", folds = 3),     n_repeats = 10 )  loco_multi$compute() loco_multi$importance() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:  noise1  0.2379570 #> 2:  noise2  0.3506664 #> 3:      x1  4.0299444 #> 4:      x2  3.9218550 #> 5:      x3  1.3056282  # Check individual scores with multiple refits loco_multi$scores() |>     head(10) |>     knitr::kable(digits = 4, caption = \"First 10 LOCO scores per refit and resampling fold\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"using-different-measures","dir":"Articles","previous_headings":"","what":"Using Different Measures","title":"LOCO and WVIM","text":"LOCO also works mlr3 measure. Different measures can highlight different aspects feature importance:","code":"# Use same resampling for fair comparison loco_mae <- LOCO$new(     task = task,     learner = learner,     measure = msr(\"regr.mae\"),     resampling = resampling )"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"comparison-with-perturbation-methods","dir":"Articles","previous_headings":"","what":"Comparison with Perturbation Methods","title":"LOCO and WVIM","text":"LOCO differs perturbation-based methods like PFI CFI: LOCO: Retrains model without feature (computationally expensive) PFI/CFI: Perturb feature values using existing model (faster) Note: Using instantiated resampling ensures evaluate methods identical train/test splits. Even , stochastic learners like ranger produce slightly different results methods due random forest’s sampling. LOCO measures value feature available training, PFI measures value informative feature values prediction time.","code":"# Compare LOCO with PFI using same resampling pfi <- PFI$new(task, learner, measure, resampling = resampling) pfi$compute()  comparison <- merge(     loco$importance()[, .(feature, loco = importance)],     pfi$importance()[, .(feature, pfi = importance)],     by = \"feature\" )  comparison #> Key: <feature> #>    feature       loco          pfi #>     <char>      <num>        <num> #> 1:  noise1 -0.7663935 -0.002685962 #> 2:  noise2 -0.5653578  0.002991822 #> 3:      x1  2.8024966  6.496274359 #> 4:      x2  2.8173232  6.578501614 #> 5:      x3  0.7637862  1.770219866"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"wvim-the-general-framework","dir":"Articles","previous_headings":"","what":"WVIM: The General Framework","title":"LOCO and WVIM","text":"LOCO special case Ewald et al. refer “Williamson’s Variable Importance Measure” (WVIM), provides general formulation refit-based feature importance. WVIM allows one feature interest time, meaning via groups argument features can assigned groups always “left ” “left ” together. also direction argument, specifying whether features left .","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"replicating-loco-with-wvim","dir":"Articles","previous_headings":"WVIM: The General Framework","what":"Replicating LOCO with WVIM","title":"LOCO and WVIM","text":"can manually replicate LOCO using WVIM’s \"leave-\" direction. Since features specified rather groups, feature well left one time, resulting LOCO procedure. Note: get comparable results methods, must use instantiated resampling. Even , stochastic learners like ranger produce slightly different results due random forest’s bootstrapping split selection, overall patterns consistent.","code":"# Create WVIM instance (LOCO's parent class) using same resampling wvim_loco <- WVIM$new(     task = task,     learner = learner,     measure = measure,     resampling = resampling,     features = task$feature_names,     direction = \"leave-out\" ) wvim_loco$compute()  # Compare with original LOCO results comparison_wvim <- merge(     loco$importance()[, .(feature, loco = importance)],     wvim_loco$importance()[, .(feature, wvim = importance)],     by = \"feature\" )  comparison_wvim #> Key: <feature> #>    feature       loco       wvim #>     <char>      <num>      <num> #> 1:  noise1 -0.7663935 -1.1171265 #> 2:  noise2 -0.5653578 -1.4339895 #> 3:      x1  2.8024966  2.2075487 #> 4:      x2  2.8173232  2.2197634 #> 5:      x3  0.7637862 -0.1849607"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"loci-leave-one-covariate-in","dir":"Articles","previous_headings":"WVIM: The General Framework","what":"LOCI: Leave-One-Covariate-In","title":"LOCO and WVIM","text":"WVIM allows us compute LOCI (Leave-One-Covariate-) changing direction “leave-”. LOCI trains models single features compares featureless baseline. Note: LOCI questionable utility practice essentially just measure bivariate association target feature separately. However, WVIM makes trivial compute desired: LOCI interprets importance differently LOCO: LOCO: “much performance degrade feature removed full model?” LOCI: “much feature alone improve featureless baseline?” interaction data \\(y = 2 \\cdot x_1 \\cdot x_2 + x_3 + \\epsilon\\): show low importance \\(x_1\\) \\(x_2\\) individually (main effects) show high importance \\(x_3\\) (strong main effect) LOCO better captures value features participate interactions However, since used neural network learner, expected learn meaningful relations based training one feature. hopefully showcases LOCI primarily “useful” teaching exercise, rather meaningful importance measure.","code":"# LOCI: train with only one feature at a time wvim_loci <- WVIM$new(     task = task,     learner = learner,     measure = measure,     features = task$feature_names,     direction = \"leave-in\",     resampling = rsmp(\"cv\", folds = 3),     n_repeats = 10 )  wvim_loci$compute() wvim_loci$importance() #> Key: <feature> #>    feature   importance #>     <char>        <num> #> 1:  noise1 -0.005507611 #> 2:  noise2 -0.030116216 #> 3:      x1 -6.172977924 #> 4:      x2 -0.276415375 #> 5:      x3  0.986802222"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"wvim-with-feature-groups","dir":"Articles","previous_headings":"","what":"WVIM with Feature Groups","title":"LOCO and WVIM","text":"LOCO measures importance individual features, WVIM generalizes arbitrary feature groups. allows us measure collective importance sets features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"understanding-feature-groups","dir":"Articles","previous_headings":"WVIM with Feature Groups","what":"Understanding Feature Groups","title":"LOCO and WVIM","text":"Feature groups useful : Features naturally related (e.g., dummy-encoded categorical variables) want measure importance feature subsets Features known functional relationships Let’s create groups interaction data:","code":"# Define feature groups groups <- list(     interaction_pair = c(\"x1\", \"x2\"), # Features that interact     main_effect = \"x3\", # Feature with direct effect     noise_features = c(\"noise1\", \"noise2\") # Pure noise )  groups #> $interaction_pair #> [1] \"x1\" \"x2\" #>  #> $main_effect #> [1] \"x3\" #>  #> $noise_features #> [1] \"noise1\" \"noise2\""},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"wvim-with-leave-out-direction","dir":"Articles","previous_headings":"WVIM with Feature Groups","what":"WVIM with Leave-Out Direction","title":"LOCO and WVIM","text":"Using direction = \"leave-\", WVIM computes importance group measuring performance entire group removed: \\[\\text{WVIM}_{\\text{group}} = \\mathbb{E}(L(Y, f_{-\\text{group}}(X_{-\\text{group}}))) - \\mathbb{E}(L(Y, f(X)))\\] compares model without group full model. Interpretation: interaction_pair (x1, x2) shows joint contribution interacting features main_effect (x3) shows contribution direct effect noise_features show near-zero negative importance","code":"wvim_groups_out <- WVIM$new(     task = task,     learner = learner,     measure = measure,     groups = groups,     direction = \"leave-out\",     resampling = rsmp(\"cv\", folds = 3),     n_repeats = 10 )  wvim_groups_out$compute() wvim_groups_out$importance() #> Key: <feature> #>             feature importance #>              <char>      <num> #> 1: interaction_pair  3.3780282 #> 2:      main_effect  1.3852866 #> 3:   noise_features -0.3314483"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"wvim-with-leave-in-direction","dir":"Articles","previous_headings":"WVIM with Feature Groups","what":"WVIM with Leave-In Direction","title":"LOCO and WVIM","text":"Using direction = \"leave-\", WVIM trains models group compares featureless baseline: \\[\\text{WVIM}_{\\text{group}} = \\mathbb{E}(L(Y, f_{\\emptyset})) - \\mathbb{E}(L(Y, f_{\\text{group}}(X_{\\text{group}})))\\] measures much group alone improves features.","code":"wvim_groups_in <- WVIM$new(     task = task,     learner = learner,     measure = measure,     groups = groups,     direction = \"leave-in\",     resampling = rsmp(\"cv\", folds = 3) )  wvim_groups_in$compute() wvim_groups_in$importance() #> Key: <feature> #>             feature importance #>              <char>      <num> #> 1: interaction_pair  3.6348866 #> 2:      main_effect  0.9813703 #> 3:   noise_features -0.2612117"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"comparing-leave-out-vs-leave-in-for-groups","dir":"Articles","previous_headings":"WVIM with Feature Groups","what":"Comparing Leave-Out vs Leave-In for Groups","title":"LOCO and WVIM","text":"key difference two directions baseline model used comparison: Measures: “lose removing group?” Higher values → group important overall model performance Measures: “gain using group?” Higher values → group alone provides better prediction baseline interaction data \\(y = 2 \\cdot x_1 \\cdot x_2 + x_3 + \\epsilon\\): Leave-captures much group contributes full model Leave-shows main_effect group (x3) provides substantial prediction alone, interaction_pair (x1, x2) limited predictive power isolation Features interact strongly may show high importance leave-(matter full model) low importance leave-(don’t work well alone).","code":"comparison_directions <- merge(     wvim_groups_out$importance()[, .(feature, leave_out = importance)],     wvim_groups_in$importance()[, .(feature, leave_in = importance)],     by = \"feature\" )  comparison_directions #> Key: <feature> #>             feature  leave_out   leave_in #>              <char>      <num>      <num> #> 1: interaction_pair  3.3780282  3.6348866 #> 2:      main_effect  1.3852866  0.9813703 #> 3:   noise_features -0.3314483 -0.2612117"},{"path":"https://mlr-org.github.io/xplainfi/articles/loco.html","id":"practical-considerations","dir":"Articles","previous_headings":"WVIM with Feature Groups","what":"Practical Considerations","title":"LOCO and WVIM","text":"use feature groups: Analyzing categorical variables (group dummy columns together) Testing domain-specific feature sets (e.g., “demographic features”, “behavioral features”) Measuring importance feature engineering transformations collectively Computational cost: WVIM requires retraining model group resampling fold groups, efficient analyzing features individually Use n_repeats parameter control number refits per fold variance estimation","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-1-interaction-effects","dir":"Articles","previous_headings":"","what":"Scenario 1: Interaction Effects","title":"Perturbation-based Feature Importance Methods","text":"scenario demonstrates marginal methods (PFI) can miss important interaction effects conditional methods (CFI) capture: Causal Structure: x1 x2 direct effects - affect y interaction (thick red arrow). However, PFI still show important permuting either feature destroys crucial interaction term.","code":"# Generate interaction scenario task_int <- sim_dgp_interactions(n = 1000) data_int <- task_int$data()"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"analysis","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects","what":"Analysis","title":"Perturbation-based Feature Importance Methods","text":"Let’s analyze interaction scenario \\(y = 2 \\cdot x_1 \\cdot x_2 + x_3 + \\epsilon\\). Note x1 x2 main effects.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"pfi-on-interactions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"PFI on Interactions","title":"Perturbation-based Feature Importance Methods","text":"Expected: x1 x2 show high importance PFI permuting either feature destroys interaction term x1×x2, crucial prediction. demonstrates key limitation PFI interactions.","code":"pfi_int <- PFI$new(     task = task_int,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20 )  # Compute importance scores pfi_int$compute() pfi_int$importance(relation = \"difference\") #> Key: <feature> #>    feature  importance #>     <char>       <num> #> 1:  noise1 0.007709169 #> 2:  noise2 0.010890405 #> 3:      x1 2.235765433 #> 4:      x2 2.113462681 #> 5:      x3 2.089067824"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"cfi-on-interactions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"CFI on Interactions","title":"Perturbation-based Feature Importance Methods","text":"CFI preserves joint distribution, better capture interaction effect: Expected: CFI show somewhat lower importance x1 x2 compared PFI better preserves interaction structure conditional sampling, providing nuanced importance estimate.","code":"# Create ARF sampler for the interaction task sampler_int = ConditionalARFSampler$new(task = task_int, finite_bounds = \"local\")  cfi_int <- CFI$new(     task = task_int,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20,     sampler = sampler_int )  # Compute importance scores cfi_int$compute() cfi_int$importance(relation = \"difference\") #> Key: <feature> #>    feature   importance #>     <char>        <num> #> 1:  noise1  0.002804323 #> 2:  noise2 -0.001548059 #> 3:      x1  2.071614103 #> 4:      x2  1.746959285 #> 5:      x3  1.990800605"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"rfi-on-interactions-targeted-conditional-questions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"RFI on Interactions: Targeted Conditional Questions","title":"Perturbation-based Feature Importance Methods","text":"RFI’s unique strength answering specific conditional questions. Let’s explore happens condition different features: RFI Results: x1 given x2: 2.243 (important x1 condition x2) x2 given x1: 1.863 (important x2 condition x1) x3 given x2: 2.055 (important x3 condition x2) pure interaction case (y = 2·x1·x2 + x3), condition one interacting feature, becomes extremely important matter together. demonstrates RFI’s power answer targeted questions like “Given already know x2, much x1 add?”","code":"# RFI conditioning on x2: \"How important is x1 given we know x2?\" rfi_int_x2 <- RFI$new(     task = task_int,     learner = learner,     measure = measure,     resampling = resampling,     conditioning_set = \"x2\", # Condition on x2     n_repeats = 20,     sampler = sampler_int ) rfi_int_x2$compute()  # RFI conditioning on x1: \"How important is x2 given we know x1?\" rfi_int_x1 <- RFI$new(     task = task_int,     learner = learner,     measure = measure,     resampling = resampling,     conditioning_set = \"x1\", # Condition on x1     n_repeats = 20,     sampler = sampler_int ) rfi_int_x1$compute()"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"comparing-methods-on-interactions","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects > Analysis","what":"Comparing Methods on Interactions","title":"Perturbation-based Feature Importance Methods","text":"Let’s compare methods handle interaction:","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"interaction-effects","dir":"Articles","previous_headings":"Scenario 1: Interaction Effects","what":"Interaction Effects","title":"Perturbation-based Feature Importance Methods","text":"CFI vs PFI Interacting Features x1 x2 main effects, PFI still correctly identifies important permuting either feature destroys interaction term x1×x2, crucial prediction.","code":"# Combine results and calculate ratios comp_int <- rbindlist(list(     pfi_int$importance()[, .(feature, importance, method = \"PFI\")],     cfi_int$importance()[, .(feature, importance, method = \"CFI\")] ))  # Calculate the ratio of CFI to PFI importance for interacting features int_ratio <- dcast(comp_int[feature %in% c(\"x1\", \"x2\")], feature ~ method, value.var = \"importance\") int_ratio[, cfi_pfi_ratio := CFI / PFI] setnames(int_ratio, c(\"PFI\", \"CFI\"), c(\"pfi_importance\", \"cfi_importance\"))  int_ratio |>     knitr::kable(         digits = 3,         caption = \"CFI vs PFI for Interacting Features\"     )"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-2-confounding","dir":"Articles","previous_headings":"","what":"Scenario 2: Confounding","title":"Perturbation-based Feature Importance Methods","text":"scenario shows hidden confounders affect importance estimates conditioning can help: Causal Structure: red arrows show confounding paths: hidden confounder creates spurious correlations x1, proxy, y. blue arrows show true direct causal effects. Note independent truly independent (confounding) proxy provides noisy measurement confounder. observable confounder scenaro (used later), confounder H included feature dataset, allowing direct conditioning rather relying noisy proxy.  hidden confounder creates spurious correlations x1 y (red paths), making x1 appear important true direct effect. RFI conditioning proxy (measures confounder) help isolate true direct effect (blue path).","code":"# Generate confounding scenario task_conf <- sim_dgp_confounded(n = 1000) data_conf <- task_conf$data()"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"analysis-1","dir":"Articles","previous_headings":"Scenario 2: Confounding","what":"Analysis","title":"Perturbation-based Feature Importance Methods","text":"Now let’s analyze confounding scenario hidden confounder affects features outcome.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"pfi-on-confounded-data","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"PFI on Confounded Data","title":"Perturbation-based Feature Importance Methods","text":"","code":"pfi_conf <- PFI$new(     task = task_conf,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20 )  pfi_conf$compute() pfi_conf$importance() #> Key: <feature> #>        feature importance #>         <char>      <num> #> 1: independent   1.589470 #> 2:       proxy   1.083619 #> 3:          x1   3.843644"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"rfi-conditioning-on-proxy","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"RFI Conditioning on Proxy","title":"Perturbation-based Feature Importance Methods","text":"RFI can condition proxy help isolate direct effects:","code":"# Create sampler for confounding task sampler_conf = ConditionalARFSampler$new(     task = task_conf,     verbose = FALSE,     finite_bounds = \"local\" )  # RFI conditioning on the proxy rfi_conf <- RFI$new(     task = task_conf,     learner = learner,     measure = measure,     resampling = resampling,     conditioning_set = \"proxy\", # Condition on proxy to reduce confounding     n_repeats = 20,     sampler = sampler_conf )  rfi_conf$compute() rfi_conf$importance() #> Key: <feature> #>        feature importance #>         <char>      <num> #> 1: independent   1.597174 #> 2:       proxy   0.000000 #> 3:          x1   1.714698"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"also-trying-cfi-for-comparison","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"Also trying CFI for comparison","title":"Perturbation-based Feature Importance Methods","text":"","code":"cfi_conf <- CFI$new(     task = task_conf,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20,     sampler = sampler_conf ) #> Warning: ! Provided sampler has a pre-configured `conditioning_set`. #> ℹ To calculate <CFI> correctly, `conditioning_set` will be reset such that #>   sampling is performed conditionally on all remaining features.  cfi_conf$compute() cfi_conf$importance() #> Key: <feature> #>        feature importance #>         <char>      <num> #> 1: independent  1.5623424 #> 2:       proxy  0.2479059 #> 3:          x1  1.6977570"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"observable-confounder-scenario","dir":"Articles","previous_headings":"Scenario 2: Confounding > Analysis","what":"Observable Confounder Scenario","title":"Perturbation-based Feature Importance Methods","text":"many real-world situations, confounders actually observable (e.g., demographics, baseline characteristics). Let’s explore RFI performs can condition directly true confounder: x1 importance: PFI = 1.669, RFI|confounder = 0.544 independent importance: PFI = 1.433, RFI|confounder = 1.437 conditioning true confounder, RFI show reduced importance x1 (since much apparent importance due confounding) independent maintains importance (since ’s truly causally related y).","code":"# Generate scenario where confounder is observable task_conf_obs <- sim_dgp_confounded(n = 1000, hidden = FALSE)  # Now we can condition directly on the true confounder sampler_conf_obs = ConditionalARFSampler$new(     task = task_conf_obs,     verbose = FALSE,     finite_bounds = \"local\" )  # RFI conditioning on the true confounder (not just proxy) rfi_conf_obs <- RFI$new(     task = task_conf_obs,     learner = learner,     measure = measure,     resampling = resampling,     conditioning_set = \"confounder\", # Condition on true confounder     n_repeats = 20,     sampler = sampler_conf_obs )  rfi_conf_obs$compute() rfi_conf_obs$importance() #> Key: <feature> #>        feature importance #>         <char>      <num> #> 1:  confounder 0.00000000 #> 2: independent 1.43713482 #> 3:       proxy 0.01481911 #> 4:          x1 0.54425680  # Compare with PFI on the same data pfi_conf_obs <- PFI$new(     task = task_conf_obs,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20 ) pfi_conf_obs$compute() pfi_conf_obs$importance() #> Key: <feature> #>        feature importance #>         <char>      <num> #> 1:  confounder  1.6772911 #> 2: independent  1.4332083 #> 3:       proxy  0.1429327 #> 4:          x1  1.6693868"},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"confounding-effects","dir":"Articles","previous_headings":"Scenario 2: Confounding","what":"Confounding Effects","title":"Perturbation-based Feature Importance Methods","text":"Effect Conditioning Proxy Confounded Scenario confounding scenario, observed: PFI shows confounded effects: Without accounting confounders, PFI overestimates importance x1 due spurious correlation y hidden confounder. RFI conditioning proxy reduces bias: conditioning proxy (noisy measurement confounder), RFI can partially isolate direct effects, though confounding remains due measurement error. RFI conditioning true confounder removes bias: confounder observable can condition directly , RFI dramatically reduces apparent importance x1, revealing true direct effect. CFI partially accounts confounding: conditional sampling, CFI captures confounding structure target specific confounders like RFI can.","code":"# Show how conditioning affects importance estimates conf_wide <- dcast(comp_conf_long, feature ~ method, value.var = \"importance\") conf_summary <- conf_wide[, .(     feature,     pfi_importance = round(PFI, 3),     cfi_importance = round(CFI, 3),     rfi_proxy_importance = round(RFI, 3),     pfi_rfi_diff = round(PFI - RFI, 3) )]  conf_summary |>     knitr::kable(         caption = \"Effect of Conditioning on Proxy in Confounded Scenario\"     )"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-3-correlated-features","dir":"Articles","previous_headings":"","what":"Scenario 3: Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"scenario demonstrates fundamental difference marginal conditional methods features highly correlated: Causal Structure: x1 x2 highly correlated (r = 0.9 MVN) x1 causal effect y. x2 spurious predictor - correlated causal feature causal .","code":"# Generate correlated features scenario task_cor <- sim_dgp_correlated(n = 1000) data_cor <- task_cor$data()"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"analysis-2","dir":"Articles","previous_headings":"Scenario 3: Correlated Features","what":"Analysis","title":"Perturbation-based Feature Importance Methods","text":"Let’s analyze different methods handle highly correlated features:","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"pfi-on-correlated-features","dir":"Articles","previous_headings":"Scenario 3: Correlated Features > Analysis","what":"PFI on Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"Expected: PFI show high importance x1 x2, even though x1 true causal effect. happens x2 highly correlated x1, permuting x2 destroys predictive information x1.","code":"pfi_cor <- PFI$new(     task = task_cor,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20 )  pfi_cor$compute() pfi_cor$importance() #> Key: <feature> #>    feature    importance #>     <char>         <num> #> 1:      x1  4.760001e+00 #> 2:      x2  4.996568e-01 #> 3:      x3  1.691757e+00 #> 4:      x4 -1.259272e-05"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"cfi-on-correlated-features","dir":"Articles","previous_headings":"Scenario 3: Correlated Features > Analysis","what":"CFI on Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"Expected: CFI show high importance x1 (true causal feature) near-zero importance x2 (spurious correlated feature) conditional sampling preserves correlation structure can distinguish causal spurious predictors.","code":"# Create ARF sampler for correlated task sampler_cor = ConditionalARFSampler$new(task = task_cor, finite_bounds = \"local\")  cfi_cor <- CFI$new(     task = task_cor,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20,     sampler = sampler_cor )  cfi_cor$compute() cfi_cor$importance() #> Key: <feature> #>    feature    importance #>     <char>         <num> #> 1:      x1  1.672411e+00 #> 2:      x2  5.157227e-02 #> 3:      x3  1.591436e+00 #> 4:      x4 -4.946158e-05"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"rfi-to-answer-conditional-questions","dir":"Articles","previous_headings":"Scenario 3: Correlated Features > Analysis","what":"RFI to Answer Conditional Questions","title":"Perturbation-based Feature Importance Methods","text":"x2 given x1: 0.044 (much x2 add already know x1?) x1 given x2: 1.744 (much x1 add already know x2?) Expected: conditioning x1, importance x2 near zero (vice versa) ’re almost identical - knowing one tells almost everything .","code":"# RFI conditioning on x1: \"How important is x2 given we know x1?\" rfi_cor_x1 <- RFI$new(     task = task_cor,     learner = learner,     measure = measure,     resampling = resampling,     conditioning_set = \"x1\",     n_repeats = 20,     sampler = sampler_cor ) rfi_cor_x1$compute()  # RFI conditioning on x2: \"How important is x1 given we know x2?\" rfi_cor_x2 <- RFI$new(     task = task_cor,     learner = learner,     measure = measure,     resampling = resampling,     conditioning_set = \"x2\",     n_repeats = 20,     sampler = sampler_cor ) rfi_cor_x2$compute()"},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"correlated-features","dir":"Articles","previous_headings":"Scenario 3: Correlated Features","what":"Correlated Features","title":"Perturbation-based Feature Importance Methods","text":"CFI vs PFI Highly Correlated Features correlated features scenario: PFI overestimates importance spurious features: PFI assigns high importance x1 (causal) x2 (spurious) ’re highly correlated. Permuting x2 destroys information x1, making x2 appear important even though causal effect. CFI correctly identifies causal features: preserving correlation structure conditional sampling, CFI can distinguish x1 (truly causal) x2 (merely correlated), assigning high importance x1. RFI reveals redundancy: conditioning x1, additional importance x2 near zero (vice versa), correctly identifying redundancy prediction. Practical implication: PFI mislead think features important. CFI correctly shows x1 truly important, x2 just along ride due correlation.","code":"cor_ratio |>     knitr::kable(         digits = 3,         caption = \"CFI vs PFI for Highly Correlated Features\"     )"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"scenario-4-independent-features-baseline","dir":"Articles","previous_headings":"","what":"Scenario 4: Independent Features (Baseline)","title":"Perturbation-based Feature Importance Methods","text":"provide baseline comparison, let’s examine scenario feature importance methods produce similar results: Causal Structure: simplest scenario: features independent, interactions, confounding. feature direct effect y (effect case noise).","code":"# Generate independent features scenario task_ind <- sim_dgp_independent(n = 1000) data_ind <- task_ind$data()"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"running-all-methods-on-independent-features","dir":"Articles","previous_headings":"Scenario 4: Independent Features (Baseline)","what":"Running All Methods on Independent Features","title":"Perturbation-based Feature Importance Methods","text":"First PFI: Now CFI ARF sampler: RFI empty conditioning set, basically equivalent PFI different sampler: now visualize:","code":"# PFI pfi_ind <- PFI$new(     task = task_ind,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20 ) pfi_ind$compute() sampler_ind = ConditionalARFSampler$new(task = task_ind, finite_bounds = \"local\") cfi_ind <- CFI$new(     task = task_ind,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 20,     sampler = sampler_ind ) cfi_ind$compute() rfi_ind <- RFI$new(     task = task_ind,     learner = learner,     measure = measure,     resampling = resampling,     conditioning_set = character(0), # Empty set     n_repeats = 20,     sampler = sampler_ind ) rfi_ind$compute()"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"agreement-between-methods","dir":"Articles","previous_headings":"Scenario 4: Independent Features (Baseline)","what":"Agreement Between Methods","title":"Perturbation-based Feature Importance Methods","text":"Method Agreement Independent Features independent features complex relationships, three methods (PFI, CFI, RFI) produce similar importance estimates. confirms differences observe Scenarios 1 2 truly due interactions confounding, artifacts methods .","code":"# Calculate coefficient of variation for each feature across methods comp_ind_wide <- dcast(comp_ind_long, feature ~ method, value.var = \"importance\") comp_ind_wide[,     `:=`(         mean_importance = rowMeans(.SD),         sd_importance = apply(.SD, 1, sd),         cv = apply(.SD, 1, sd) / rowMeans(.SD)     ),     .SDcols = c(\"PFI\", \"CFI\", \"RFI\") ]  comp_ind_wide[, .(     feature,     mean_importance = round(mean_importance, 3),     cv = round(cv, 3),     agreement = ifelse(cv < 0.1, \"High\", ifelse(cv < 0.2, \"Moderate\", \"Low\")) )] |>     knitr::kable(         caption = \"Method Agreement on Independent Features\",         col.names = c(\"Feature\", \"Mean Importance\", \"Coef. of Variation\", \"Agreement Level\")     )"},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"key-takeaways","dir":"Articles","previous_headings":"","what":"Key Takeaways","title":"Perturbation-based Feature Importance Methods","text":"four scenarios, ’ve demonstrated: PFI simple fast can miss interaction effects, underestimate importance correlated features, affected confounding CFI captures feature dependencies interactions conditional sampling, correctly handling correlated features RFI allows targeted conditioning isolate specific relationships reveal redundancy Use PFI features believed independent (Scenario 4) want quick baseline importance ranking Use CFI suspect feature interactions, correlations, dependencies (Scenarios 1 & 3) want sophisticated analysis respects feature relationships Use RFI specific conditional questions: “important feature X given already know feature Y?” (Scenarios 1, 2 & 3). Essential feature selection understanding incremental value. methods benefit cross-validation multiple permutation iterations stability ARF-based conditional sampling (used CFI/RFI) computationally intensive marginal sampling choice conditioning set RFI requires domain knowledge","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/perturbation-importance.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further Reading","title":"Perturbation-based Feature Importance Methods","text":"details methods theoretical foundations, see: Breiman (2001) original PFI formulation Strobl et al. (2008) limitations PFI correlated features Watson & Wright (2021) conditional sampling ARF König et al. (2021) relative feature importance Ewald et al. (2024) comprehensive review feature importance methods","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Shapley Additive Global Importance (SAGE)","text":"Shapley Additive Global Importance (SAGE) feature importance method based cooperative game theory. uses Shapley values distribute model’s total prediction performance among features. Unlike perturbation-based methods (PFI/CFI) measure performance degradation features perturbed, SAGE measures feature’s contribution marginalization. key property SAGE provides complete decomposition: sum SAGE values equals difference model’s performance performance features marginalized. xplainfi provides two implementations: MarginalSAGE: Marginalizes features independently (standard SAGE implementation) ConditionalSAGE: Marginalizes features using conditional sampling Note interpretation: SAGE’s theoretical properties interpretation differ perturbation-based methods. PFI/CFI clearer interpretations terms predictive performance, SAGE’s results can challenging interpret, particularly using conditional sampling. choice conditional sampler can significantly affect results, limited empirical guidance best practices. vignette focuses demonstrating methods rather making strong interpretive claims. SAGE estimator implemented referred “permutation estimator” implementations. works first building n_permutations permutations feature vector successively evaluating prefixes sequence left right coalitions evaluated. task features (x1, x2, x3), one permutation (x2, x1, x3), resulting coalitions evaluated: (x2), (x2, x1), (x2, x1, x3). empty coalition always evaluated, resulting total number evaluations n_permutations * n_features + 1.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"demonstration-with-correlated-features","dir":"Articles","previous_headings":"","what":"Demonstration with Correlated Features","title":"Shapley Additive Global Importance (SAGE)","text":"showcase difference Marginal Conditional SAGE, ’ll use sim_dgp_correlated() function creates simple linear DGP two correlated features. Model: \\[(X_1, X_2)^T \\sim \\text{MVN}(0, \\Sigma)\\] \\(\\Sigma\\) 2×2 covariance matrix 1 diagonal correlation \\(r\\) (default 0.5) -diagonal. \\[X_3 \\sim N(0,1), \\quad X_4 \\sim N(0,1)\\] \\[Y = 2 \\cdot X_1 + X_3 + \\varepsilon\\] \\(\\varepsilon \\sim N(0, 0.2^2)\\). Data generating process: x1 direct effect y (β=2.0) x2 correlated x1 (r = 0.5) direct effect y x3 independent direct effect (β=1.0) x4 independent noise (β=0) DGP allows us observe two SAGE variants handle correlated features different roles data generating process. Let’s set learner measure. ’ll use random forest instantiate resampling ensure methods see data:","code":"set.seed(123) task = sim_dgp_correlated(n = 1000, r = 0.5)  # Check the correlation structure task_data = task$data() correlation_matrix = cor(task_data[, c(\"x1\", \"x2\", \"x3\", \"x4\")]) round(correlation_matrix, 3) #>        x1     x2     x3     x4 #> x1  1.000  0.447 -0.005 -0.048 #> x2  0.447  1.000 -0.049 -0.054 #> x3 -0.005 -0.049  1.000  0.051 #> x4 -0.048 -0.054  0.051  1.000 learner = lrn(\"regr.ranger\") measure = msr(\"regr.mse\") resampling = rsmp(\"holdout\") resampling$instantiate(task)"},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"marginal-sage","dir":"Articles","previous_headings":"","what":"Marginal SAGE","title":"Shapley Additive Global Importance (SAGE)","text":"Marginal SAGE marginalizes features independently averaging predictions subset n_samples observations drawn test dataset. use 15 permutations feature vector build coalitions, resulting 61 evaluated coalitions (15 * 4 + 1). Let’s visualize results:  can also keep track SAGE value approximation across permutations:","code":"# Create Marginal SAGE instance marginal_sage = MarginalSAGE$new(     task = task,     learner = learner,     measure = measure,     resampling = resampling,     n_permutations = 15L,     n_samples = 100L )  # Compute SAGE values marginal_sage$compute(batch_size = 5000L) marginal_sage$plot_convergence()"},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"early-stopping-based-on-convergence","dir":"Articles","previous_headings":"Marginal SAGE","what":"Early Stopping Based on Convergence","title":"Shapley Additive Global Importance (SAGE)","text":"SAGE supports early stopping save computation time importance values converged. default, early stopping enabled early_stopping = TRUE. Convergence detected monitoring standard error (SE) SAGE value estimates first resampling iteration. SAGE normalizes SE range values (max - min) make convergence detection scale-invariant across different loss metrics. Convergence detected : \\[ \\max_j \\left(\\frac{SE_j}{\\max_i(\\text{SAGE}_i) - \\min_i(\\text{SAGE}_i)}\\right) < \\text{threshold} \\] default threshold se_threshold = 0.01 (1%), meaning convergence occurs relative SE 1% importance range features, equivalent approach Python implementation fippy package. can customize convergence detection $compute(): computation, can check convergence status: resampling multiple iterations (.e., holdout) supplied, value n_permutations_used set value n_permutations subsequent iterations avoid computational overhead.","code":"# More strict convergence (requires more permutations) sage$compute(early_stopping = TRUE, se_threshold = 0.005, min_permutations = 5L)  # Disable early stopping to always run all permutations sage$compute(early_stopping = FALSE) marginal_sage$converged  # TRUE if converged early marginal_sage$n_permutations_used  # Actual permutations used"},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"conditional-sage","dir":"Articles","previous_headings":"","what":"Conditional SAGE","title":"Shapley Additive Global Importance (SAGE)","text":"Conditional SAGE uses conditional sampling (via ARF default) marginalize features preserving dependencies remaining features. can provide different insights, especially features correlated. Let’s visualize conditional SAGE results:","code":"# Create Conditional SAGE instance using a conditional sampler sampler_gaussian = ConditionalGaussianSampler$new(task)  conditional_sage = ConditionalSAGE$new(     task = task,     learner = learner,     measure = measure,     resampling = resampling,     n_permutations = 15L,     n_samples = 100L,     sampler = sampler_gaussian )  # Compute SAGE values conditional_sage$compute(batch_size = 5000L) conditional_sage$plot_convergence()"},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"comparison-of-methods","dir":"Articles","previous_headings":"","what":"Comparison of Methods","title":"Shapley Additive Global Importance (SAGE)","text":"Let’s compare two SAGE methods side side:","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"methodological-notes","dir":"Articles","previous_headings":"Comparison of Methods","what":"Methodological Notes","title":"Shapley Additive Global Importance (SAGE)","text":"difference two methods: MarginalSAGE: Marginalizes --coalition features simultaneously sampling marginal distribution, account conditional dependencies -coalition --coalition features. ConditionalSAGE: Uses conditional sampling “marginalize” --coalition features preserving conditional dependency structure -coalition --coalition features. interpretation SAGE values, particularly ConditionalSAGE, can affected specific conditional sampler used nature feature dependencies data. choice sampler can significantly affect results, currently limited empirical guidance best practices different settings.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/sage-methods.html","id":"comparison-with-pfi-and-cfi","dir":"Articles","previous_headings":"","what":"Comparison with PFI and CFI","title":"Shapley Additive Global Importance (SAGE)","text":"reference, let’s compare SAGE methods analogous PFI CFI methods data:  PFI/CFI MarginalSAGE/ConditionalSAGE distinguish marginal conditional approaches, method families measure fundamentally different quantities. PFI CFI measure drop predictive performance features perturbed, making interpretation terms prediction loss relatively straightforward. SAGE methods measure feature’s contribution overall performance Shapley value decomposition, involves different theoretical framework. results shown demonstrate methods data, direct comparisons numerical values made methodological differences mind.","code":"# Quick PFI and CFI comparison for context pfi = PFI$new(task, learner, measure) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 333) cfi = CFI$new(task, learner, measure, sampler = sampler_gaussian) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 333)  pfi$compute() cfi$compute() pfi_results = pfi$importance() cfi_results = cfi$importance()  # Create comparison data frame method_comparison = data.frame(     feature = rep(c(\"x1\", \"x2\", \"x3\", \"x4\"), 4),     importance = c(         pfi_results$importance,         cfi_results$importance,         marginal_results$importance,         conditional_results$importance     ),     method = rep(c(\"PFI\", \"CFI\", \"Marginal SAGE\", \"Conditional SAGE\"), each = 4),     approach = rep(c(\"Marginal\", \"Conditional\", \"Marginal\", \"Conditional\"), each = 4) )  # Create comparison plot #| fig.alt: \"Grouped bar chart with features on x-axis and importance on y-axis. Four colored bars per feature for PFI, CFI, Marginal SAGE, and Conditional SAGE.\" ggplot(method_comparison, aes(x = feature, y = importance, fill = method)) +     geom_col(position = \"dodge\", alpha = 0.8) +     scale_fill_manual(         values = c(             \"PFI\" = \"lightblue\",             \"CFI\" = \"blue\",             \"Marginal SAGE\" = \"lightcoral\",             \"Conditional SAGE\" = \"darkred\"         )     ) +     labs(         title = \"Comparison: PFI/CFI vs Marginal/Conditional SAGE\",         subtitle = \"Comparing perturbation-based and Shapley-based importance methods\",         x = \"Features\",         y = \"Importance Value\",         fill = \"Method\"     ) +     theme_minimal(base_size = 14) +     theme(axis.text.x = element_text(angle = 45, hjust = 1))"},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Simulation Settings for Feature Importance Methods","text":"xplainfi package provides several data generating processes (DGPs) designed illustrate specific strengths weaknesses different feature importance methods. DGP focuses one primary challenge make differences methods clear. article provides comprehensive overview simulation settings, including mathematical formulations causal structures visualized directed acyclic graphs (DAGs).","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"overview-of-simulation-settings","dir":"Articles","previous_headings":"","what":"Overview of Simulation Settings","title":"Simulation Settings for Feature Importance Methods","text":"Overview simulation settings expected method behavior","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"correlated-features-dgp","dir":"Articles","previous_headings":"","what":"1. Correlated Features DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP creates highly correlated spurious predictor illustrate fundamental difference marginal conditional importance methods.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model","dir":"Articles","previous_headings":"1. Correlated Features DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[(X_1, X_2)^T \\sim \\text{MVN}(0, \\Sigma)\\] \\(\\Sigma\\) \\(2 \\times 2\\) covariance matrix 1 diagonal correlation \\(r\\) (default 0.9) -diagonal. \\[X_3 \\sim N(0,1), \\quad X_4 \\sim N(0,1)\\] \\[Y = 2 \\cdot X_1 + X_3 + \\varepsilon\\] \\(\\varepsilon \\sim N(0, 0.2^2)\\).","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure","dir":"Articles","previous_headings":"1. Correlated Features DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG correlated features DGP","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example","dir":"Articles","previous_headings":"1. Correlated Features DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_correlated(n = 500)  # Check correlation between X1 and X2 cor(task$data()[, c(\"x1\", \"x2\")]) #>          x1       x2 #> x1 1.000000 0.885718 #> x2 0.885718 1.000000  # True coefficients: x1=2.0, x2=0, x3=1.0, x4=0 # Note: x2 is highly correlated with x1 but has NO causal effect!"},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"mediated-effects-dgp","dir":"Articles","previous_headings":"","what":"2. Mediated Effects DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP demonstrates difference total direct causal effects. features affect outcome mediators.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-1","dir":"Articles","previous_headings":"2. Mediated Effects DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[\\text{exposure} \\sim N(0,1), \\quad \\text{direct} \\sim N(0,1)\\] \\[\\text{mediator} = 0.8 \\cdot \\text{exposure} + 0.6 \\cdot \\text{direct} + \\varepsilon_m\\] \\[Y = 1.5 \\cdot \\text{mediator} + 0.5 \\cdot \\text{direct} + \\varepsilon\\] \\(\\varepsilon_m \\sim N(0, 0.3^2)\\) \\(\\varepsilon \\sim N(0, 0.2^2)\\).","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-1","dir":"Articles","previous_headings":"2. Mediated Effects DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG mediated effects DGP","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-1","dir":"Articles","previous_headings":"2. Mediated Effects DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_mediated(n = 500)  # Calculate total effect of exposure # Total effect = 0.8 * 1.5 = 1.2 (through mediator) # Direct effect = 0 (no direct path to Y)"},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"confounding-dgp","dir":"Articles","previous_headings":"","what":"3. Confounding DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP includes confounder affects feature outcome.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-2","dir":"Articles","previous_headings":"3. Confounding DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[H \\sim N(0,1) \\quad \\text{(confounder)}\\] \\[X_1 = H + \\varepsilon_1\\] \\[\\text{proxy} = H + \\varepsilon_p, \\quad \\text{independent} \\sim N(0,1)\\] \\[Y = H + X_1 + \\text{independent} + \\varepsilon\\] \\(\\varepsilon \\sim N(0, 0.5^2)\\) independently.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-2","dir":"Articles","previous_headings":"3. Confounding DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG confounding DGP","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-2","dir":"Articles","previous_headings":"3. Confounding DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) # Hidden confounder scenario (default) task_hidden <- sim_dgp_confounded(n = 500, hidden = TRUE) task_hidden$feature_names # proxy available but not confounder #> [1] \"independent\" \"proxy\"       \"x1\"  # Observable confounder scenario task_observed <- sim_dgp_confounded(n = 500, hidden = FALSE) task_observed$feature_names # both confounder and proxy available #> [1] \"confounder\"  \"independent\" \"proxy\"       \"x1\""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"interaction-effects-dgp","dir":"Articles","previous_headings":"","what":"4. Interaction Effects DGP","title":"Simulation Settings for Feature Importance Methods","text":"DGP demonstrates pure interaction effect features main effects.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-3","dir":"Articles","previous_headings":"4. Interaction Effects DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[Y = 2 \\cdot X_1 \\cdot X_2 + X_3 + \\varepsilon\\] \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.5^2)\\).","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-3","dir":"Articles","previous_headings":"4. Interaction Effects DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG interaction effects DGP","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-3","dir":"Articles","previous_headings":"4. Interaction Effects DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_interactions(n = 500)  # Note: X1 and X2 have NO main effects # Their importance comes ONLY through their interaction"},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"independent-features-dgp-baseline","dir":"Articles","previous_headings":"","what":"5. Independent Features DGP (Baseline)","title":"Simulation Settings for Feature Importance Methods","text":"baseline scenario features independent effects additive. importance methods give similar results.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-4","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[Y = 2.0 \\cdot X_1 + 1.0 \\cdot X_2 + 0.5 \\cdot X_3 + \\varepsilon\\] \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.2^2)\\).","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-4","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG independent features DGP","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-4","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"set.seed(123) task <- sim_dgp_independent(n = 500)  # All methods should rank features consistently: # important1 > important2 > important3 > unimportant1,2 (approx. 0)"},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"expected-behavior","dir":"Articles","previous_headings":"5. Independent Features DGP (Baseline)","what":"Expected Behavior","title":"Simulation Settings for Feature Importance Methods","text":"methods: rank features consistently true effect sizes Ground truth: important1 (2.0) > important2 (1.0) > important3 (0.5) > unimportant1,2 (0)","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"ewald-et-al--2024-dgp","dir":"Articles","previous_headings":"","what":"6. Ewald et al. (2024) DGP","title":"Simulation Settings for Feature Importance Methods","text":"Reproduces data generating process Ewald et al. (2024) benchmarking feature importance methods. Includes correlated features interaction effects.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"mathematical-model-5","dir":"Articles","previous_headings":"6. Ewald et al. (2024) DGP","what":"Mathematical Model","title":"Simulation Settings for Feature Importance Methods","text":"\\[X_1, X_3, X_5 \\sim \\text{Uniform}(0,1)\\] \\[X_2 = X_1 + \\varepsilon_2, \\quad \\varepsilon_2 \\sim N(0, 0.001)\\] \\[X_4 = X_3 + \\varepsilon_4, \\quad \\varepsilon_4 \\sim N(0, 0.1)\\] \\[Y = X_4 + X_5 + X_4 \\cdot X_5 + \\varepsilon, \\quad \\varepsilon \\sim N(0, 0.1)\\]","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"causal-structure-5","dir":"Articles","previous_headings":"6. Ewald et al. (2024) DGP","what":"Causal Structure","title":"Simulation Settings for Feature Importance Methods","text":"DAG Ewald et al. (2024) DGP","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/simulation-settings.html","id":"usage-example-5","dir":"Articles","previous_headings":"6. Ewald et al. (2024) DGP","what":"Usage Example","title":"Simulation Settings for Feature Importance Methods","text":"","code":"sim_dgp_ewald(n = 500) #>  #> ── <TaskRegr> (500x6) ────────────────────────────────────────────────────────── #> • Target: y #> • Properties: - #> • Features (5): #>   • dbl (5): x1, x2, x3, x4, x5"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"core-concepts","dir":"Articles","previous_headings":"","what":"Core Concepts","title":"Getting Started with xplainfi","text":"Feature importance methods xplainfi address different related questions: much feature contribute model performance? (Permutation Feature Importance) happens remove features retrain? (Leave-One-Covariate-) features depend ? (Conditional Relative methods) methods share common interface built mlr3, making easy use task, learner, measure, resampling strategy. general pattern call $compute() calculate importance (always re-computes), $importance() retrieve aggregated results, intermediate results available $scores() , chosen measure supports , $obs_loss().","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"basic-example","dir":"Articles","previous_headings":"","what":"Basic Example","title":"Getting Started with xplainfi","text":"Let’s use Friedman1 task demonstrate feature importance methods known ground truth: task 300 observations 10 features. Features important1 important5 truly affect target, unimportant1 unimportant5 pure noise. ’ll use random forest learner cross-validation stable estimates. target function : \\(y = 10 \\cdot \\operatorname{sin}(\\pi  x_1  x_2) + 20  (x_3 - 0.5)^2 + 10  x_4 + 5  x_5 + \\epsilon\\)","code":"task <- tgen(\"friedman1\")$generate(n = 300) learner <- lrn(\"regr.ranger\", num.trees = 100) measure <- msr(\"regr.mse\") resampling <- rsmp(\"cv\", folds = 3)"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"permutation-feature-importance-pfi","dir":"Articles","previous_headings":"","what":"Permutation Feature Importance (PFI)","title":"Getting Started with xplainfi","text":"PFI straightforward method: feature, permute (shuffle) values measure much model performance deteriorates. important features cause larger performance drops shuffled. importance column shows performance difference feature permuted. Higher values indicate important features. stable estimates, can use multiple permutation iterations per resampling fold n_repeats, set 30 default methods. Note case “”, clear “good enough” value, setting n_repeats small value like 1 definitely yield unreliable results. illustrate important, can take look variability PFI scores feature important2 within resampling iteration using individual importance scores via $score() (see ):  aggregated importance score feature approximately 8.3, across resamplings estimated PFI scores range 4.51 12.67, insufficient resampling low n_repeats, might - underestimated features PFI margin. can also use ratio performance scores instead difference importance calculation, meaning unimportant feature now expected get importance score 1 rather 0:","code":"pfi <- PFI$new(     task = task,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 10 )  pfi$compute() pfi$importance() #> Key: <feature> #>          feature   importance #>           <char>        <num> #>  1:   important1  5.545674370 #>  2:   important2  8.968101641 #>  3:   important3  1.261353831 #>  4:   important4 12.673269112 #>  5:   important5  2.005969867 #>  6: unimportant1  0.004208592 #>  7: unimportant2  0.093948304 #>  8: unimportant3  0.080041842 #>  9: unimportant4 -0.029783843 #> 10: unimportant5 -0.088944796 pfi_stable <- PFI$new(     task = task,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 50 )  pfi_stable$compute() pfi_stable$importance() #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  5.47062536 #>  2:   important2  8.31221894 #>  3:   important3  1.19280876 #>  4:   important4 12.24462886 #>  5:   important5  2.01412225 #>  6: unimportant1 -0.02250072 #>  7: unimportant2  0.12486533 #>  8: unimportant3  0.04175966 #>  9: unimportant4  0.05038402 #> 10: unimportant5 -0.09342671 pfi_stable$scores()[feature == \"important2\", ] |>     ggplot(aes(y = importance, x = factor(iter_rsmp))) +     geom_boxplot() +     labs(         title = \"PFI variability within resampling iterations\",         subtitle = \"Setting n_repeats higher improves PFI estimates\",         y = \"PFI score (important2)\",         x = \"Resampling iteration (3-fold CV)\"     ) +     theme_minimal() pfi_stable$importance(relation = \"ratio\") #> Key: <feature> #>          feature importance #>           <char>      <num> #>  1:   important1  1.8678890 #>  2:   important2  2.3092581 #>  3:   important3  1.1900256 #>  4:   important4  2.9573191 #>  5:   important5  1.3219625 #>  6: unimportant1  0.9969603 #>  7: unimportant2  1.0166104 #>  8: unimportant3  1.0070529 #>  9: unimportant4  1.0074279 #> 10: unimportant5  0.9873048"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"leave-one-covariate-out-loco","dir":"Articles","previous_headings":"","what":"Leave-One-Covariate-Out (LOCO)","title":"Getting Started with xplainfi","text":"LOCO measures importance retraining model without feature comparing performance full model. shows contribution feature features present. LOCO computationally expensive requires retraining feature, provides clear interpretation: higher values mean larger performance drop feature removed. However, distinguish direct effects indirect effects correlated features.","code":"loco <- LOCO$new(     task = task,     learner = learner,     measure = measure,     resampling = resampling,     n_repeats = 10 )  loco$compute() loco$importance() #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  3.62257756 #>  2:   important2  5.60038686 #>  3:   important3  0.96616913 #>  4:   important4  7.77084397 #>  5:   important5  0.91573094 #>  6: unimportant1 -0.21657782 #>  7: unimportant2 -0.10069618 #>  8: unimportant3 -0.07475981 #>  9: unimportant4 -0.13525518 #> 10: unimportant5 -0.23400214"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"feature-samplers","dir":"Articles","previous_headings":"","what":"Feature Samplers","title":"Getting Started with xplainfi","text":"advanced methods account feature dependencies, xplainfi provides different sampling strategies. PFI uses simple permutation (marginal sampling), conditional samplers can preserve feature relationships. Let’s demonstrate conditional sampling using adversarial random forests (ARF), preserves relationships features sampling: Now ’ll conditionally sample important1 feature given values important2 important3: conditional sampling essential methods like CFI RFI need preserve feature dependencies. See perturbation-importance article detailed comparisons vignette(\"feature-samplers\") details implemented samplers.","code":"arf_sampler <- ConditionalARFSampler$new(task)  sample_data <- task$data(rows = 1:5) sample_data[, .(important1, important2)] #>    important1  important2 #>         <num>       <num> #> 1:  0.2875775 0.784575267 #> 2:  0.7883051 0.009429905 #> 3:  0.4089769 0.779065883 #> 4:  0.8830174 0.729390652 #> 5:  0.9404673 0.630131853 sampled_conditional <- arf_sampler$sample_newdata(     feature = \"important1\",     newdata = sample_data,     conditioning_set = c(\"important2\", \"important3\") )  sample_data[, .(important1, important2, important3)] #>    important1  important2 important3 #>         <num>       <num>      <num> #> 1:  0.2875775 0.784575267  0.2372297 #> 2:  0.7883051 0.009429905  0.6864904 #> 3:  0.4089769 0.779065883  0.2258184 #> 4:  0.8830174 0.729390652  0.3184946 #> 5:  0.9404673 0.630131853  0.1739838 sampled_conditional[, .(important1, important2, important3)] #>    important1  important2 important3 #>         <num>       <num>      <num> #> 1:  0.7928991 0.784575267  0.2372297 #> 2:  0.6233185 0.009429905  0.6864904 #> 3:  0.1203356 0.779065883  0.2258184 #> 4:  0.8458796 0.729390652  0.3184946 #> 5:  0.8990316 0.630131853  0.1739838"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"detailed-scoring-information","dir":"Articles","previous_headings":"","what":"Detailed Scoring Information","title":"Getting Started with xplainfi","text":"methods store detailed scoring information resampling iteration analysis. Let’s examine structure PFI’s detailed scores: Detailed PFI scores (first 10 rows) can also summarize scoring structure: $importance() always gives us aggregated importances across multiple resampling- permutation-/refitting iterations, whereas $scores() gives individual scores calculated supplied measure corresponding importance calculated difference scores default. Analogous $importance(), can also use relation = \"ratio\" : PFI scores using ratio (first 10 rows)","code":"pfi$scores() |>     head(10) |>     knitr::kable(digits = 4, caption = \"Detailed PFI scores (first 10 rows)\") pfi$scores()[, .(     features = uniqueN(feature),     resampling_folds = uniqueN(iter_rsmp),     permutation_iters = uniqueN(iter_repeat),     total_scores = .N )] #>    features resampling_folds permutation_iters total_scores #>       <int>            <int>             <int>        <int> #> 1:       10                3                10          300 pfi$scores(relation = \"ratio\") |>     head(10) |>     knitr::kable(digits = 4, caption = \"PFI scores using the ratio (first 10 rows)\")"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"observation-wise-losses-and-importances","dir":"Articles","previous_headings":"","what":"Observation-wise losses and importances","title":"Getting Started with xplainfi","text":"methods importances calculated based observation-level comparisons decomposable measures, can also retrieve observation-level information $obs_loss(), works analogously $scores() $importance() even detailed level: Since computed PFI using mean squared error (msr(\"regr.mse\")), can use associated Measure$obs_loss(), squared error. resulting table see loss_baseline: loss (squared error) baseline model permutation loss_post: loss observation permutation (case LOCO, refit) obs_importance: difference (ratio relation = \"ratio\") two losses Note measures Measure$obs_loss(): measures like msr(\"classif.auc\") decomposable, observation-wise loss values available. cases, corresponding obs_loss() just yet implemented mlr3measures, likely future.","code":"pfi$obs_loss() #>             feature iter_rsmp iter_repeat row_ids loss_baseline  loss_post #>              <char>     <int>       <int>   <int>         <num>      <num> #>     1:   important1         1           1       1     3.3403244  2.5440536 #>     2:   important1         1           1       9     0.4640003  5.2814472 #>     3:   important1         1           1      11     1.0938319  0.4286004 #>     4:   important1         1           1      12     2.0091331  2.3334294 #>     5:   important1         1           1      15    11.4484770 41.8831124 #>    ---                                                                     #> 29996: unimportant5         3          10     290    16.8041217 15.2572740 #> 29997: unimportant5         3          10     294     0.4212832  0.5242628 #> 29998: unimportant5         3          10     295     8.0016602  9.3209027 #> 29999: unimportant5         3          10     296     0.2308082  0.1680013 #> 30000: unimportant5         3          10     298    18.8129904 19.7120160 #>        obs_importance #>                 <num> #>     1:    -0.79627076 #>     2:     4.81744691 #>     3:    -0.66523150 #>     4:     0.32429633 #>     5:    30.43463535 #>    ---                #> 29996:    -1.54684765 #> 29997:     0.10297959 #> 29998:     1.31924248 #> 29999:    -0.06280688 #> 30000:     0.89902555"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"statistical-inference","dir":"Articles","previous_headings":"","what":"Statistical Inference","title":"Getting Started with xplainfi","text":"importance methods support confidence intervals p-values via ci_method argument $importance(). Available approaches range empirical quantiles corrected t-tests (Nadeau & Bengio) resampling-based variability, observation-wise inference methods like CPI/cARFi (CFI) Lei et al. (2018) (LOCO). Multiplicity correction via p_adjust supported methods produce p-values. comprehensive guide covering inference methods, see Inference Feature Importance article.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"using-pre-trained-learners","dir":"Articles","previous_headings":"","what":"Using Pre-trained Learners","title":"Getting Started with xplainfi","text":"default, xplainfi trains learner internally via mlr3::resample(). However, already trained learner (example training expensive want explain specific model) can pass directly perturbation-based methods (PFI, CFI, RFI) SAGE methods. Refit-based methods (LOCO / WVIM) require retraining design warn given pretrained learner. requirement resampling must instantiated exactly one iteration (.e., single test set). necessary pre-trained learner corresponds single fitted model, meaningful way associate multiple resampling folds. holdout resampling natural choice . first train learner train set PFI calculate importance using trained learner corresponding test set defined resampling: common real-world scenario learner trained dataset want explain model entirely new, unseen data. case, create task new data (via as_task_regr() example) use rsmp(\"custom\") designate rows test set. resampling purely technicality used internal consistency, train set irrelevant since learner already trained. utility function rsmp_all_test() can used shortcut achieve goal. pass trained learner multi-fold non-instantiated resampling, get informative error construction time:","code":"resampling_holdout <- rsmp(\"holdout\")$instantiate(task) learner_trained <- lrn(\"regr.ranger\", num.trees = 100) learner_trained$train(task, row_ids = resampling_holdout$train_set(1))  pfi_pretrained <- PFI$new(     task = task,     learner = learner_trained,     measure = measure,     resampling = resampling_holdout,     n_repeats = 10 )  pfi_pretrained$compute() pfi_pretrained$importance() #> Key: <feature> #>          feature  importance #>           <char>       <num> #>  1:   important1  4.90596578 #>  2:   important2  9.70222982 #>  3:   important3  1.27523296 #>  4:   important4 13.27365797 #>  5:   important5  2.09879343 #>  6: unimportant1 -0.03733685 #>  7: unimportant2  0.14397918 #>  8: unimportant3  0.02300583 #>  9: unimportant4  0.05327077 #> 10: unimportant5 -0.01233854 # Simulate: learner was trained elsewhere, we have new data to use new_data <- tgen(\"friedman1\")$generate(n = 100)  # Same as rsmp_all_test(task) resampling_custom <- rsmp(\"custom\")$instantiate(     new_data,     train_sets = list(integer(0)),     test_sets = list(new_data$row_ids) )  pfi_newdata <- PFI$new(     task = new_data,     learner = learner_trained,     measure = measure,     resampling = resampling_custom,     n_repeats = 10 )  pfi_newdata$compute() pfi_newdata$importance() #> Key: <feature> #>          feature   importance #>           <char>        <num> #>  1:   important1  7.367319221 #>  2:   important2  6.605859756 #>  3:   important3  0.588785391 #>  4:   important4 15.696286629 #>  5:   important5  2.805833984 #>  6: unimportant1 -0.145515481 #>  7: unimportant2 -0.011919069 #>  8: unimportant3 -0.002688459 #>  9: unimportant4  0.166218449 #> 10: unimportant5  0.014330499 PFI$new(     task = task,     learner = learner_trained,     measure = measure,     resampling = rsmp(\"cv\", folds = 3) ) #> Error in `super$initialize()`: #> ! A pre-trained <Learner> requires an instantiated <Resampling> #> ℹ Instantiate the <Resampling> before passing it, e.g. #>   `rsmp(\"holdout\")$instantiate(task)`"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"Parallelization","title":"Getting Started with xplainfi","text":"PFI/CFI/RFI LOCO/WVIM support parallel execution speed computation working multiple features expensive learners. parallelization follows mlr3’s approach, allowing users choose mirai future backends.","code":""},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"example-with-future","dir":"Articles","previous_headings":"Parallelization","what":"Example with future","title":"Getting Started with xplainfi","text":"future package provides simple interface parallel distributed computing:","code":"library(future) plan(\"multisession\", workers = 2)  # PFI with parallelization across features pfi_parallel = PFI$new(     task,     learner = lrn(\"regr.ranger\"),     measure = msr(\"regr.mse\"),     n_repeats = 10 ) pfi_parallel$compute() pfi_parallel$importance()  # LOCO with parallelization (uses mlr3fselect internally) loco_parallel = LOCO$new(     task,     learner = lrn(\"regr.ranger\"),     measure = msr(\"regr.mse\") ) loco_parallel$compute() loco_parallel$importance()"},{"path":"https://mlr-org.github.io/xplainfi/articles/xplainfi.html","id":"example-with-mirai","dir":"Articles","previous_headings":"Parallelization","what":"Example with mirai","title":"Getting Started with xplainfi","text":"mirai package offers modern alternative parallel computing:","code":"library(mirai) daemons(n = 2)  # Same PFI/LOCO code works with mirai backend pfi_parallel = PFI$new(     task,     learner = lrn(\"regr.ranger\"),     measure = msr(\"regr.mse\"),     n_repeats = 10 ) pfi_parallel$compute() pfi_parallel$importance()  # Clean up daemons when done daemons(0)"},{"path":"https://mlr-org.github.io/xplainfi/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lukas Burk. Author, maintainer, copyright holder.","code":""},{"path":"https://mlr-org.github.io/xplainfi/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Burk L (2026). xplainfi: Feature Importance Methods Global Explanations. R package version 1.1.0, https://mlr-org.github.io/xplainfi/.","code":"@Manual{,   title = {xplainfi: Feature Importance Methods for Global Explanations},   author = {Lukas Burk},   year = {2026},   note = {R package version 1.1.0},   url = {https://mlr-org.github.io/xplainfi/}, }"},{"path":"https://mlr-org.github.io/xplainfi/index.html","id":"xplainfi","dir":"","previous_headings":"","what":"xplainfi: Feature importance methods","title":"xplainfi: Feature importance methods","text":"goal xplainfi collect common feature importance methods unified extensible interface. built around mlr3 available abstractions learners, tasks, measures, etc. greatly simplify implementation importance measures.","code":""},{"path":"https://mlr-org.github.io/xplainfi/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"xplainfi: Feature importance methods","text":"Install xplainfi CRAN: install R-universe: latest development version xplainfi can installed pak:","code":"install.packages(\"xplainfi\") install.packages(\"xplainfi\", repos = c(\"https://mlr-org.r-universe.dev\", \"https://cloud.r-project.org\")) # install.packages(pak) pak::pak(\"mlr-org/xplainfi\")"},{"path":"https://mlr-org.github.io/xplainfi/index.html","id":"example-pfi","dir":"","previous_headings":"","what":"Example: PFI","title":"xplainfi: Feature importance methods","text":"basic example calculate PFI untrained learner task, using cross-validation resampling computing PFI within resampling iteration 10 times friedman1 task (see ?mlbench::mlbench.friedman1). friedman1 task following structure: \\[y = 10 \\sin(\\pi x_1 x_2) + 20(x_3 - 0.5)^2 + 10x_4 + 5x_5 + \\varepsilon\\] \\(x_{\\{1,2,3,4,5\\}}\\) named important1 important5 Task, additional numbered unimportant features without effect \\(y\\). Compute print PFI scores: aids interpretation, importances can also calculated ratio rather difference baseline post-permutation losses: PFI computed based resampling multiple iterations, / multiple permutation iterations, individual scores can retrieved data.table: iter_rsmp corresponds resampling iteration, .e., 3 3-fold cross-validation, iter_repeat corresponds permutation iteration within resampling iteration, 5 case. pfi$importance() contains means across iterations, pfi$scores() allows manually visualize aggregate way see fit. example:  measure question needs maximized rather minimized (like \\(R^2\\)), internal importance calculation takes account via $minimize property measure calculates importances intuition “performance improvement” -> “higher importance score” still holds: See vignette(\"xplainfi\") examples.","code":"library(xplainfi) library(mlr3learners) #> Loading required package: mlr3  task = tgen(\"friedman1\")$generate(1000) learner = lrn(\"regr.ranger\", num.trees = 100) measure = msr(\"regr.mse\")  pfi = PFI$new(     task = task,     learner = learner,     measure = measure,     resampling = rsmp(\"cv\", folds = 3),     n_repeats = 30 ) pfi$compute() pfi$importance() #> Key: <feature> #>          feature   importance #>           <char>        <num> #>  1:   important1  8.183995584 #>  2:   important2  7.481268675 #>  3:   important3  1.571760349 #>  4:   important4 12.585739572 #>  5:   important5  2.810875567 #>  6: unimportant1  0.030667439 #>  7: unimportant2 -0.002837696 #>  8: unimportant3 -0.044922079 #>  9: unimportant4 -0.060054450 #> 10: unimportant5  0.060148388 pfi$importance(relation = \"ratio\") #> Key: <feature> #>          feature importance #>           <char>      <num> #>  1:   important1  2.6987668 #>  2:   important2  2.5598945 #>  3:   important3  1.3294180 #>  4:   important4  3.6278508 #>  5:   important5  1.5874860 #>  6: unimportant1  1.0067957 #>  7: unimportant2  0.9994507 #>  8: unimportant3  0.9905990 #>  9: unimportant4  0.9874657 #> 10: unimportant5  1.0126572 str(pfi$scores()) #> Classes 'data.table' and 'data.frame':   900 obs. of  6 variables: #>  $ feature          : chr  \"important1\" \"important1\" \"important1\" \"important1\" ... #>  $ iter_rsmp        : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ iter_repeat      : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ regr.mse_baseline: num  4.56 4.56 4.56 4.56 4.56 ... #>  $ regr.mse_post    : num  12.3 11.9 11.3 12.1 13.6 ... #>  $ importance       : num  7.77 7.33 6.74 7.56 9.06 ... #>  - attr(*, \".internal.selfref\")=<externalptr> library(ggplot2)  ggplot(     pfi$scores(),     aes(x = importance, y = reorder(feature, importance)) ) +     geom_boxplot(color = \"#f44560\", fill = alpha(\"#f44560\", 0.4)) +     labs(         title = \"Permutation Feature Importance on Friedman1\",         subtitle = \"Computed over 3-fold CV with 5 permutations per iteration using Random Forest\",         x = \"Importance\",         y = \"Feature\"     ) +     theme_minimal(base_size = 16) +     theme(         plot.title.position = \"plot\",         panel.grid.major.y = element_blank()     ) pfi = PFI$new(     task = task,     learner = learner,     measure = msr(\"regr.rsq\") ) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 333)  pfi$compute() pfi$importance() #> Key: <feature> #>          feature   importance #>           <char>        <num> #>  1:   important1  0.329915393 #>  2:   important2  0.297695022 #>  3:   important3  0.063613087 #>  4:   important4  0.493673768 #>  5:   important5  0.121794662 #>  6: unimportant1  0.003972813 #>  7: unimportant2  0.002157623 #>  8: unimportant3 -0.002780577 #>  9: unimportant4  0.001914150 #> 10: unimportant5  0.001366645"},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Feature Importance — CFI","title":"Conditional Feature Importance — CFI","text":"Implementation CFI using modular sampling approach","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Conditional Feature Importance — CFI","text":"CFI replaces feature values conditional samples distribution feature given features. ConditionalSampler KnockoffSampler can used.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"statistical-inference","dir":"Reference","previous_headings":"","what":"Statistical Inference","title":"Conditional Feature Importance — CFI","text":"Two approaches statistical inference primarily supported via $importance(ci_method = \"cpi\"): CPI (Watson & Wright, 2021): original Conditional Predictive Impact method, designed use knockoff samplers (KnockoffGaussianSampler). cARFi (Blesch et al., 2025): CFI ARF-based conditional sampling (ConditionalARFSampler), using CPI inference framework. require decomposable measure (e.g., MSE) --sample evaluation. CPI inference guaranteed valid holdout (single train/test split). cross-validation, test observations ..d. models fit overlapping training data, may affect inference coverage. bootstrap subsampling, non-..d. test observations overlapping training data can issue. See vignette(\"inference\", package = \"xplainfi\") details. Available tests: \"t\" (t-test), \"wilcoxon\" (signed-rank), \"fisher\" (permutation), \"binomial\" (sign test). Fisher test recommended. Method-agnostic inference methods (\"raw\", \"nadeau_bengio\", \"quantile\") also available; see FeatureImportanceMethod details. comprehensive overview inference methods including usage examples, see vignette(\"inference\", package = \"xplainfi\").","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conditional Feature Importance — CFI","text":"Watson D, Wright M (2021). “Testing Conditional Independence Supervised Learning Algorithms.” Machine Learning, 110(8), 2107–2129. doi:10.1007/s10994-021-06030-6 . Blesch K, Koenen N, Kapar J, Golchian P, Burk L, Loecher M, Wright M (2025). “Conditional Feature Importance Generative Modeling Using Adversarial Random Forests.” Proceedings AAAI Conference Artificial Intelligence, 39(15), 15596–15604. doi:10.1609/aaai.v39i15.33712 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Conditional Feature Importance — CFI","text":"xplainfi::FeatureImportanceMethod -> xplainfi::PerturbationImportance -> CFI","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Conditional Feature Importance — CFI","text":"xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores() xplainfi::PerturbationImportance$importance()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Conditional Feature Importance — CFI","text":"CFI$new() CFI$compute() CFI$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Conditional Feature Importance — CFI","text":"Creates new instance CFI class","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Importance — CFI","text":"","code":"CFI$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   groups = NULL,   relation = \"difference\",   n_repeats = 30L,   batch_size = NULL,   sampler = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Importance — CFI","text":"task, learner, measure, resampling, features, groups, relation, n_repeats, batch_size Passed PerturbationImportance. sampler (ConditionalSampler) Optional custom sampler. Defaults instantiating ConditionalARFSampler internally default parameters.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Conditional Feature Importance — CFI","text":"Compute CFI scores","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Importance — CFI","text":"","code":"CFI$compute(   n_repeats = NULL,   batch_size = NULL,   store_models = TRUE,   store_backends = TRUE )"},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Importance — CFI","text":"n_repeats (integer(1)) Number permutation iterations. NULL, uses stored value. batch_size (integer(1) | NULL: NULL) Maximum number rows predict . NULL, uses stored value. store_models, store_backends (logical(1): TRUE) Whether store fitted models / data backends, passed mlr3::resample internally initial fit learner. may required certain measures recommended leave enabled unless really necessary.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Conditional Feature Importance — CFI","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Importance — CFI","text":"","code":"CFI$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Importance — CFI","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/CFI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional Feature Importance — CFI","text":"","code":"library(mlr3)  task <- sim_dgp_correlated(n = 200)  # Using default ConditionalARFSampler cfi <- CFI$new(   task = task,   learner = lrn(\"regr.rpart\"),   measure = msr(\"regr.mse\"),   sampler = ConditionalGaussianSampler$new(task),   n_repeats = 5 ) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 67) cfi$compute() cfi$importance() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x1   1.042835 #> 2:      x2   0.000000 #> 3:      x3   1.553294 #> 4:      x4   0.000000"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"ARF-based Conditional Sampler — ConditionalARFSampler","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"Implements conditional sampling using Adversarial Random Forests (ARF). ARF can handle mixed data types (continuous categorical) provides flexible conditional sampling modeling joint distribution.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"ConditionalARFSampler fits Adversarial Random Forest model task data, uses generate samples \\(P(X_j | X_{-j})\\) \\(X_j\\) feature interest \\(X_{-j}\\) conditioning features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"Watson D, Blesch K, Kapar J, Wright M (2023). “Adversarial Random Forests Density Estimation Generative Modeling.” Proceedings 26th International Conference Artificial Intelligence Statistics, 5357–5375. https://proceedings.mlr.press/v206/watson23a.html. Blesch K, Koenen N, Kapar J, Golchian P, Burk L, Loecher M, Wright M (2025). “Conditional Feature Importance Generative Modeling Using Adversarial Random Forests.” Proceedings AAAI Conference Artificial Intelligence, 39(15), 15596–15604. doi:10.1609/aaai.v39i15.33712 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"xplainfi::FeatureSampler -> xplainfi::ConditionalSampler -> ConditionalARFSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"feature_types (character()) Feature types supported sampler. checked provided mlr3::Task ensure compatibility. arf_model Adversarial Random Forest model created arf::adversarial_rf. psi Distribution parameters estimated arf::forde.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"xplainfi::FeatureSampler$print()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"ConditionalARFSampler$new() ConditionalARFSampler$sample() ConditionalARFSampler$sample_newdata() ConditionalARFSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"Creates new instance ConditionalARFSampler class. fit ARF parallel, register parallel backend first (see arf::arf) set parallel = TRUE.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"","code":"ConditionalARFSampler$new(   task,   conditioning_set = NULL,   num_trees = 10L,   min_node_size = 20L,   finite_bounds = \"no\",   epsilon = 1e-15,   round = TRUE,   stepsize = 0,   verbose = FALSE,   parallel = FALSE,   ... )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"task (mlr3::Task) Task sample . conditioning_set (character | NULL) Default conditioning set use $sample(). parameter affects sampling behavior, ARF model fitting. num_trees (integer(1): 10L) Number trees ARF. Passed arf::adversarial_rf. min_node_size (integer(1): 20L) Minimum node size ARF. Passed arf::adversarial_rf turn ranger::ranger. increased 20 mitigate overfitting. finite_bounds (character(1): \"\") handle variable bounds. Passed arf::forde. Default \"\" compatibility. \"local\" may improve extrapolation can cause issues data. epsilon (numeric(1): 0) Slack parameter finite_bounds != \"\". Passed arf::forde. round (logical(1): TRUE) Whether round continuous variables back original precision sampling. Can overridden $sample() calls. stepsize (numeric(1): 0) Number rows evidence process time parallel TRUE. Default (0) spreads evidence evenly registered workers. Can overridden $sample() calls. verbose (logical(1): FALSE) Whether print progress messages. Default FALSE (arf's default TRUE). Can overridden $sample() calls. parallel (logical(1): FALSE) Whether use parallel processing via foreach. See examples arf::forge(). Can overridden $sample() calls. ... Additional arguments passed arf::adversarial_rf.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"Sample stored task. Parameters use hierarchical resolution: function argument > stored param_set value > hard-coded default.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"","code":"ConditionalARFSampler$sample(   feature,   row_ids = NULL,   conditioning_set = NULL,   round = NULL,   stepsize = NULL,   verbose = NULL,   parallel = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"feature (character) Feature(s) sample. row_ids (integer() | NULL) Row IDs use. NULL, uses rows. conditioning_set (character | NULL) Features condition . round (logical(1) | NULL) Round continuous variables. stepsize (numeric(1) | NULL) Batch size parallel processing. verbose (logical(1) | NULL) Print progress messages. parallel (logical(1) | NULL) Use parallel processing.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"method-sample-newdata-","dir":"Reference","previous_headings":"","what":"Method sample_newdata()","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"Sample external data. See $sample() parameter details.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"","code":"ConditionalARFSampler$sample_newdata(   feature,   newdata,   conditioning_set = NULL,   round = NULL,   stepsize = NULL,   verbose = NULL,   parallel = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"feature (character) Feature(s) sample. newdata (data.table) External data use. conditioning_set (character | NULL) Features condition . round (logical(1) | NULL) Round continuous variables. stepsize (numeric(1) | NULL) Batch size parallel processing. verbose (logical(1) | NULL) Print progress messages. parallel (logical(1) | NULL) Use parallel processing.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"","code":"ConditionalARFSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalARFSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ARF-based Conditional Sampler — ConditionalARFSampler","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100) # Create sampler with default parameters sampler = ConditionalARFSampler$new(task, conditioning_set = \"x2\", verbose = FALSE) # Sample using row_ids from stored task sampled_data = sampler$sample(\"x1\", row_ids = 1:10) # Or use external data data = task$data() sampled_data_ext = sampler$sample_newdata(\"x1\", newdata = data, conditioning_set = \"x2\")  # Example with custom ARF parameters sampler_custom = ConditionalARFSampler$new(   task,   min_node_size = 10L,   finite_bounds = \"local\",   verbose = FALSE ) sampled_custom = sampler_custom$sample(\"x1\", conditioning_set = \"x2\")"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"Implements conditional sampling using conditional inference trees (ctree). Builds tree predicting target features conditioning features, samples terminal node corresponding test observation.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"sampler approximates conditional distribution \\(P(X_B | X_A = x_A)\\) : Building conditional inference tree \\(X_B\\) response \\(X_A\\) predictors test observation, finding terminal (leaf) node tree Sampling uniformly training observations terminal node Conditional inference trees (ctree) use permutation tests determine splits, helps avoid overfitting handles mixed feature types naturally. tree partitions feature space based conditioning variables, creating local neighborhoods respect conditional distribution structure. Key advantages samplers: Handles mixed feature types (continuous categorical) Non-parametric (distributional assumptions) Automatic feature selection (splits informative features) Can capture non-linear conditional relationships Statistically principled splitting criteria Hyperparameters control tree complexity: mincriterion: Significance level splits (higher = fewer splits) minsplit: Minimum observations required split minbucket: Minimum observations terminal nodes implementation inspired shapr's ctree approach simplified use case (build trees -demand rather pre-computing subsets). Advantages: Works feature types Robust outliers Interpretable tree structure Handles high-dimensional conditioning Limitations: Requires model fitting (slower kNN) Can produce duplicates terminal nodes small Tree building time increases data size","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"Hothorn T, Hornik K, Zeileis (2006). “Unbiased Recursive Partitioning: Conditional Inference Framework.” Journal Computational Graphical Statistics, 15(3), 651–674. doi:10.1198/106186006X133933 . Aas K, Jullum M, Løland (2021). “Explaining Individual Predictions Features Dependent: Accurate Approximations Shapley Values.” Artificial Intelligence, 298, 103502. doi:10.1016/j.artint.2021.103502 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"xplainfi::FeatureSampler -> xplainfi::ConditionalSampler -> ConditionalCtreeSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"feature_types (character()) Feature types supported sampler. checked provided mlr3::Task ensure compatibility. tree_cache (environment) Cache fitted ctree models.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"xplainfi::FeatureSampler$print() xplainfi::ConditionalSampler$sample() xplainfi::ConditionalSampler$sample_newdata()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"ConditionalCtreeSampler$new() ConditionalCtreeSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"Creates new ConditionalCtreeSampler.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"","code":"ConditionalCtreeSampler$new(   task,   conditioning_set = NULL,   mincriterion = 0.95,   minsplit = 20L,   minbucket = 7L,   use_cache = TRUE )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"task (mlr3::Task) Task sample . conditioning_set (character | NULL) Default conditioning set use $sample(). mincriterion (numeric(1): 0.95) Significance level threshold splitting (1 - p-value). Higher values result fewer splits (simpler trees). minsplit (integer(1): 20L) Minimum number observations required split. minbucket (integer(1): 7L) Minimum number observations terminal nodes. use_cache (logical(1): TRUE) Whether cache fitted trees.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"","code":"ConditionalCtreeSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalCtreeSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"(experimental) Conditional Inference Tree Conditional Sampler — ConditionalCtreeSampler","text":"","code":"# \\donttest{ library(mlr3) task = tgen(\"friedman1\")$generate(n = 100)  # Create sampler with default parameters sampler = ConditionalCtreeSampler$new(task)  # Sample features conditioned on others test_data = task$data(rows = 1:5) sampled = sampler$sample_newdata(   feature = c(\"important2\", \"important3\"),   newdata = test_data,   conditioning_set = \"important1\" ) # }"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian Conditional Sampler — ConditionalGaussianSampler","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"Implements conditional sampling assuming features follow multivariate Gaussian distribution. Computes conditional distributions analytically using standard formulas multivariate normal distributions.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"joint Gaussian distribution \\(X \\sim N(\\mu, \\Sigma)\\), partitioned \\(X = (X_A, X_B)\\), conditional distribution : $$X_B | X_A = x_A \\sim N(\\mu_{B|}, \\Sigma_{B|})$$ : $$\\mu_{B|} = \\mu_B + \\Sigma_{BA} \\Sigma_{AA}^{-1} (x_A - \\mu_A)$$ $$\\Sigma_{B|} = \\Sigma_{BB} - \\Sigma_{BA} \\Sigma_{AA}^{-1} \\Sigma_{AB}$$ equivalent regression formulation used fippy: $$\\beta = \\Sigma_{BA} \\Sigma_{AA}^{-1}$$ $$\\mu_{B|} = \\mu_B + \\beta (x_A - \\mu_A)$$ $$\\Sigma_{B|} = \\Sigma_{BB} - \\beta \\Sigma_{AB}$$ Assumptions: Features approximately multivariate normal continuous features supported Advantages: fast (closed-form solution) Deterministic (given seed) hyperparameters Memory efficient Limitations: Strong distributional assumption May produce --range values bounded features handle categorical features Integer features treated continuous rounded back integers","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"Anderson T (2003). Introduction Multivariate Statistical Analysis, 3rd edition. Wiley-Interscience, Hoboken, NJ. ISBN 9780471360919.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"xplainfi::FeatureSampler -> xplainfi::ConditionalSampler -> ConditionalGaussianSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"feature_types (character()) Feature types supported sampler. mu (numeric()) Mean vector estimated training data. sigma (matrix()) Covariance matrix estimated training data.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"xplainfi::FeatureSampler$print() xplainfi::ConditionalSampler$sample() xplainfi::ConditionalSampler$sample_newdata()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"ConditionalGaussianSampler$new() ConditionalGaussianSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"Creates new ConditionalGaussianSampler.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"","code":"ConditionalGaussianSampler$new(task, conditioning_set = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"task (mlr3::Task) Task sample . Must numeric/integer features. conditioning_set (character | NULL) Default conditioning set use $sample().","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"","code":"ConditionalGaussianSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalGaussianSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gaussian Conditional Sampler — ConditionalGaussianSampler","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 100) sampler = ConditionalGaussianSampler$new(task)  # Sample x2, x3 conditioned on x1 test_data = task$data(rows = 1:5) sampled = sampler$sample_newdata(   feature = c(\"important2\", \"important3\"),   newdata = test_data,   conditioning_set = \"important1\" )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"Implements conditional sampling using k-nearest neighbors (kNN). observation, finds k similar observations based conditioning features, samples target features neighbors.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"sampler approximates conditional distribution \\(P(X_B | X_A = x_A)\\) : Finding k nearest neighbors \\(x_A\\) training data Sampling uniformly target feature values \\(X_B\\) k neighbors simple, non-parametric approach : Requires distributional assumptions Handles mixed feature types (numeric, integer, factor, ordered, logical) computationally efficient (model fitting required) Adapts locally data structure method related hot-deck imputation kNN imputation techniques used missing data problems. \\(k \\\\infty\\) \\(k/n \\0\\), kNN conditional distribution converges true conditional distribution mild regularity conditions (Lipschitz continuity). Distance Metrics: sampler supports two distance metrics: Euclidean: numeric/integer features . Standardizes features computing distances. Gower: mixed feature types. Handles numeric, factor, ordered, logical features. Numeric features range-normalized, categorical features use exact matching (0/1). distance parameter controls metric use: \"auto\" (default): Automatically selects Euclidean -numeric features, Gower otherwise \"euclidean\": Forces Euclidean distance (errors non-numeric features present) \"gower\": Forces Gower distance (works feature types) Advantages: fast (model training) Works feature types Automatic distance metric selection Naturally respects local data structure Limitations: Sensitive choice k full task data required prediction Can produce duplicates k small May extrapolate well new regions","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"Little R, Rubin D (2019). Statistical Analysis Missing Data, 3rd edition. John Wiley & Sons, Hoboken, NJ. ISBN 9780470526798. Troyanskaya O, Cantor M, Sherlock G, Brown P, Hastie T, Tibshirani R, Botstein D, Altman R (2001). “Missing Value Estimation Methods DNA Microarrays.” Bioinformatics, 17(6), 520–525. doi:10.1093/bioinformatics/17.6.520 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"xplainfi::FeatureSampler -> xplainfi::ConditionalSampler -> ConditionalKNNSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"feature_types (character()) Feature types supported sampler.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"xplainfi::FeatureSampler$print()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"ConditionalKNNSampler$new() ConditionalKNNSampler$sample() ConditionalKNNSampler$sample_newdata() ConditionalKNNSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"Creates new ConditionalKNNSampler.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"","code":"ConditionalKNNSampler$new(task, conditioning_set = NULL, k = 5L)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"task (mlr3::Task) Task sample . conditioning_set (character | NULL) Default conditioning set use $sample(). k (integer(1): 5L) Number nearest neighbors sample .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"Sample features kNN-based conditional distribution.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"","code":"ConditionalKNNSampler$sample(   feature,   row_ids = NULL,   conditioning_set = NULL,   k = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"feature (character()) Feature name(s) sample. row_ids (integer() | NULL) Row IDs task use conditioning values. conditioning_set (character() | NULL) Features condition . NULL, samples marginal distribution (random sampling training data). k (integer(1) | NULL) Number neighbors. NULL, uses stored parameter.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"method-sample-newdata-","dir":"Reference","previous_headings":"","what":"Method sample_newdata()","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"Sample external data conditionally.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"","code":"ConditionalKNNSampler$sample_newdata(   feature,   newdata,   conditioning_set = NULL,   k = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"feature (character()) Feature(s) sample. newdata (data.table) External data use. conditioning_set (character() | NULL) Features condition . k (integer(1) | NULL) Number neighbors. NULL, uses stored parameter.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"","code":"ConditionalKNNSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalKNNSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"k-Nearest Neighbors Conditional Sampler — ConditionalKNNSampler","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 100) sampler = ConditionalKNNSampler$new(task, k = 5)  # Sample features conditioned on others test_data = task$data(rows = 1:5) sampled = sampler$sample_newdata(   feature = c(\"important2\", \"important3\"),   newdata = test_data,   conditioning_set = \"important1\" )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional SAGE — ConditionalSAGE","title":"Conditional SAGE — ConditionalSAGE","text":"SAGE conditional sampling (features \"marginalized\" conditionally). Uses ConditionalARFSampler default ConditionalSampler.","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Conditional SAGE — ConditionalSAGE","text":"xplainfi::FeatureImportanceMethod -> xplainfi::SAGE -> ConditionalSAGE","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Conditional SAGE — ConditionalSAGE","text":"sampler (ConditionalSampler) Sampler conditional marginalization.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Conditional SAGE — ConditionalSAGE","text":"xplainfi::FeatureImportanceMethod$importance() xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores() xplainfi::SAGE$compute() xplainfi::SAGE$plot_convergence()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Conditional SAGE — ConditionalSAGE","text":"ConditionalSAGE$new() ConditionalSAGE$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Conditional SAGE — ConditionalSAGE","text":"Creates new instance ConditionalSAGE class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional SAGE — ConditionalSAGE","text":"","code":"ConditionalSAGE$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   n_permutations = 10L,   sampler = NULL,   batch_size = 5000L,   n_samples = 100L,   early_stopping = FALSE,   se_threshold = 0.01,   min_permutations = 10L,   check_interval = 1L )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional SAGE — ConditionalSAGE","text":"task, learner, measure, resampling, features, n_permutations, batch_size, n_samples, early_stopping, se_threshold, min_permutations, check_interval Passed SAGE. sampler (ConditionalSampler) Optional custom sampler. Defaults ConditionalARFSampler.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Conditional SAGE — ConditionalSAGE","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional SAGE — ConditionalSAGE","text":"","code":"ConditionalSAGE$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional SAGE — ConditionalSAGE","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSAGE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conditional SAGE — ConditionalSAGE","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 100)  # \\donttest{ # Using default ConditionalARFSampler (also handles all mixed data) sage = ConditionalSAGE$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\"),   n_permutations = 3L,   n_samples = 20 ) #> ℹ No <ConditionalSampler> provided, using <ConditionalARFSampler> with default settings. #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 33) sage$compute() # } # \\donttest{ # For alternative conditional samplers: custom_sampler = ConditionalGaussianSampler$new(   task = task ) sage_custom = ConditionalSAGE$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\"),   n_permutations = 5L,   n_samples = 20,   sampler = custom_sampler ) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 33) sage_custom$compute() # }"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Conditional Feature Sampler — ConditionalSampler","title":"Conditional Feature Sampler — ConditionalSampler","text":"Base class conditional sampling methods features sampled conditionally features. abstract class extended concrete implementations.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Conditional Feature Sampler — ConditionalSampler","text":"xplainfi::FeatureSampler -> ConditionalSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Conditional Feature Sampler — ConditionalSampler","text":"xplainfi::FeatureSampler$print()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Conditional Feature Sampler — ConditionalSampler","text":"ConditionalSampler$new() ConditionalSampler$sample() ConditionalSampler$sample_newdata() ConditionalSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Conditional Feature Sampler — ConditionalSampler","text":"Creates new instance ConditionalSampler class","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Sampler — ConditionalSampler","text":"","code":"ConditionalSampler$new(task, conditioning_set = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Sampler — ConditionalSampler","text":"task (mlr3::Task) Task sample conditioning_set (character | NULL) Default conditioning set use $sample().","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Conditional Feature Sampler — ConditionalSampler","text":"Sample stored task conditionally features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Sampler — ConditionalSampler","text":"","code":"ConditionalSampler$sample(   feature,   row_ids = NULL,   conditioning_set = NULL,   ... )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Sampler — ConditionalSampler","text":"feature (character) Feature(s) sample. row_ids (integer() | NULL) Row IDs use. NULL, uses rows. conditioning_set (character | NULL) Features condition . ... Additional arguments passed sampler implementation.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Conditional Feature Sampler — ConditionalSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"method-sample-newdata-","dir":"Reference","previous_headings":"","what":"Method sample_newdata()","title":"Conditional Feature Sampler — ConditionalSampler","text":"Sample external data conditionally.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Sampler — ConditionalSampler","text":"","code":"ConditionalSampler$sample_newdata(   feature,   newdata,   conditioning_set = NULL,   ... )"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Sampler — ConditionalSampler","text":"feature (character) Feature(s) sample. newdata (data.table) External data use. conditioning_set (character | NULL) Features condition . ... Additional arguments passed sampler implementation.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Conditional Feature Sampler — ConditionalSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Conditional Feature Sampler — ConditionalSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Conditional Feature Sampler — ConditionalSampler","text":"","code":"ConditionalSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/ConditionalSampler.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conditional Feature Sampler — ConditionalSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature Importance Method Class — FeatureImportanceMethod","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Feature Importance Method Class Feature Importance Method Class","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Nadeau C, Bengio Y (2003). “Inference Generalization Error.” Machine Learning, 52(3), 239–281. doi:10.1023/:1024068626366 . Molnar C, Freiesleben T, König G, Herbinger J, Reisinger T, Casalicchio G, Wright M, Bischl B (2023). “Relating Partial Dependence Plot Permutation Feature Importance Data Generating Process.” Longo L (ed.), Explainable Artificial Intelligence, 456–479. ISBN 978-3-031-44064-9, doi:10.1007/978-3-031-44064-9_24 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"label (character(1)) Method label. task (mlr3::Task) learner (mlr3::Learner) measure (mlr3::Measure) resampling (mlr3::Resampling), instantiated upon construction. resample_result (mlr3::ResampleResult) original learner task, used baseline scores. features (character: NULL) Features interest. default, importances computed feature task, optionally can restricted least one feature. Ignored groups specified. groups (list: NULL) (named) list features (names indices task). groups specified, features ignored. Importances calculated group features time, e.g., PFI one group features permuted step. Analogously WVIM, group features left () model refit. methods support groups (e.g., SAGE). param_set (paradox::ps()) predictions (data.table) Feature-specific prediction objects provided methods (PFI, WVIM). Contains columns feature interest, resampling iteration, refit perturbation iteration, mlr3::Prediction objects.","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"FeatureImportanceMethod$new() FeatureImportanceMethod$compute() FeatureImportanceMethod$importance() FeatureImportanceMethod$obs_loss() FeatureImportanceMethod$reset() FeatureImportanceMethod$print() FeatureImportanceMethod$scores() FeatureImportanceMethod$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Creates new instance R6 class. typically intended use derived classes.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   groups = NULL,   param_set = paradox::ps(),   label )"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"task, learner, measure, resampling, features, groups, param_set, label Used set fields","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Compute feature importance scores","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$compute(store_backends = TRUE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"store_backends (logical(1): TRUE) Whether store backends.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-importance-","dir":"Reference","previous_headings":"","what":"Method importance()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Get aggregated importance scores. stored measure object's aggregator (default: mean) used aggregated importance scores across resampling iterations , depending method use, permutations (PerturbationImportance refits LOCO).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$importance(   relation = NULL,   standardize = FALSE,   ci_method = c(\"none\", \"raw\", \"nadeau_bengio\", \"quantile\"),   conf_level = 0.95,   alternative = c(\"two.sided\", \"greater\"),   p_adjust = \"none\",   ... )"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"relation (character(1)) relate perturbed scores originals (\"difference\" \"ratio\"). NULL, uses stored parameter value. applicable methods importance based relation baseline post-modification loss, .e. PerturbationImportance methods PFI WVIM / LOCO. available SAGE methods. standardize (logical(1): FALSE) TRUE, importances standardized highest score scores fall [-1, 1]. ci_method (character(1): \"none\") confidence interval estimation method use, defaulting omitting variance estimation (\"none\"). \"raw\", uncorrected (narrow) CIs provided purely informative purposes. \"nadeau_bengio\", variance correction performed according Nadeau & Bengio (2003) suggested Molnar et al. (2023). \"quantile\", empirical quantiles used construct confidence-like intervals. methods model-agnostic rely suitable resamplings, e.g. subsampling 15 repeats \"nadeau_bengio\". See details. conf_level (numeric(1): 0.95) Confidence level use confidence interval construction ci_method != \"none\". alternative (character(1): \"two.sided\") Type alternative hypothesis statistical tests. \"greater\" tests H0: importance <= 0 vs H1: importance > 0 (one-sided). \"two.sided\" tests H0: importance = 0 vs H1: importance != 0. used ci_method != \"none\". p_adjust (character(1): \"none\") Method p-value adjustment multiple comparisons. Accepts method supported stats::p.adjust.methods, e.g. \"holm\", \"bonferroni\", \"BH\", \"none\". Applied p-values \"raw\" \"nadeau_bengio\" methods. \"bonferroni\", confidence intervals also adjusted (alpha/k). correction methods (e.g. \"holm\", \"BH\"), p-values adjusted; confidence intervals remain nominal conf_level sequential/adaptive procedures clean per-comparison alpha CI construction. ... Additional arguments passed specialized methods, .","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"confidence-interval-methods","dir":"Reference","previous_headings":"","what":"Confidence Interval Methods","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"parametric methods (\"raw\", \"nadeau_bengio\") return standard error (se), test statistic (statistic), p-value (p.value), confidence bounds (conf_lower, conf_upper). \"quantile\" method returns lower upper bounds. \"raw\": Uncorrected (!) t-test Uses standard t-test assuming independence resampling iterations. SE = sd(resampling scores) / sqrt(n_iters) Test statistic: t = importance / SE df = n_iters - 1 P-value: t-distribution (one-sided two-sided depending alternative) CIs: importance +/- qt(1 - alpha, df) * SE Warning: CIs narrow resampling iterations share training data independent. method included demonstration purposes. \"nadeau_bengio\": Corrected t-test Applies Nadeau & Bengio (2003) correction account correlation resampling iterations due overlapping training sets. Correction factor: (1/n_iters + n_test/n_train) SE = sqrt(correction_factor * var(resampling scores)) Test statistic p-value: \"raw\", corrected SE Recommended bootstrap subsampling (>= 10 iterations). \"quantile\": Non-parametric empirical method Uses resampling distribution directly without parametric assumptions. CIs: Empirical quantiles resampling distribution method provide se, statistic, p.value.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-specific-ci-methods","dir":"Reference","previous_headings":"","what":"Method-Specific CI Methods","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"importance methods provide additional CI methods tailored approach: CFI: Adds \"cpi\" (Conditional Predictive Impact), uses observation-wise loss differences holdout resampling. Supports t-test, Wilcoxon, Fisher permutation, binomial tests. See Watson & Wright (2021).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"practical-recommendations","dir":"Reference","previous_headings":"","what":"Practical Recommendations","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Variance estimates importance scores biased due resampling procedure. Molnar et al. (2023) suggest using Nadeau & Bengio correction approximately 15 iterations subsampling. Bootstrapping can cause information leakage learners bootstrap internally (e.g., Random Forests), observations may appear train test sets. Prefer subsampling cases:   \"nadeau_bengio\" correction validated PFI; use methods like LOCO SAGE experimental.","code":"PFI$new(   task = sim_dgp_interactions(n = 1000),   learner = lrn(\"regr.ranger\", num.trees = 100),   measure = msr(\"regr.mse\"),   resampling = rsmp(\"subsampling\", repeats = 15),   n_repeats = 20 )"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"(data.table) Aggregated importance scores columns \"feature\", \"importance\", depending ci_method also \"se\", \"statistic\", \"p.value\", \"conf_lower\", \"conf_upper\".","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-obs-loss-","dir":"Reference","previous_headings":"","what":"Method obs_loss()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Calculate observation-wise importance scores. Requires $compute() run measure decomposable observation-wise loss (Measure$obs_loss()) associated . case measure like classif.auc, decomposable.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$obs_loss(relation = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"relation (character(1)) relate perturbed scores originals (\"difference\" \"ratio\"). NULL, uses stored parameter value. applicable methods importance based relation baseline post-modification loss, .e. PerturbationImportance methods PFI WVIM / LOCO. available SAGE methods.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"(data.table) Observation-wise losses importance scores columns \"feature\", \"iter_rsmp\", \"iter_repeat\" (applicable), \"row_ids\", \"loss_baseline\", \"loss_post\", \"obs_importance\".","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-reset-","dir":"Reference","previous_headings":"","what":"Method reset()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Resets stored fields populated $compute: $resample_result, $scores, $obs_losses, $predictions.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$reset()"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Print importance scores","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$print(...)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"... Passed print()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-scores-","dir":"Reference","previous_headings":"","what":"Method scores()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"Calculate importance scores resampling iteration sub-iterations (iter_rsmp PFI example). Iteration-wise importance computed fly depending chosen relation (difference ratio) avoid re-computation different relation needed.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$scores(relation = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"relation (character(1)) relate perturbed scores originals (\"difference\" \"ratio\"). NULL, uses stored parameter value. applicable methods importance based relation baseline post-modification loss, .e. PerturbationImportance methods PFI WVIM / LOCO. available SAGE methods.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"(data.table) Iteration-wise importance scores columns \"feature\", iteration indices, baseline post-modification scores, \"importance\".","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"","code":"FeatureImportanceMethod$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureImportanceMethod.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Importance Method Class — FeatureImportanceMethod","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Feature Sampler Class — FeatureSampler","title":"Feature Sampler Class — FeatureSampler","text":"Base class implementing different sampling strategies feature importance methods like PFI CFI","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Feature Sampler Class — FeatureSampler","text":"task (mlr3::Task) Original task. label (character(1)) Name sampler. feature_types (character()) Feature types supported sampler. checked provided mlr3::Task ensure compatibility. param_set (paradox::ParamSet) Parameter set sampler.","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Feature Sampler Class — FeatureSampler","text":"FeatureSampler$new() FeatureSampler$sample() FeatureSampler$sample_newdata() FeatureSampler$print() FeatureSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Feature Sampler Class — FeatureSampler","text":"Creates new instance FeatureSampler class","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$new(task)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"task (mlr3::Task) Task sample ","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Feature Sampler Class — FeatureSampler","text":"Sample values feature(s) stored task","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$sample(feature, row_ids = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"feature (character) Feature name(s) sample (can single multiple). Must match stored Task. row_ids (integer(): NULL) Row IDs stored Task use basis sampling.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Feature Sampler Class — FeatureSampler","text":"Modified copy input features feature(s) sampled: data.table number columns one row matching supplied row_ids","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"method-sample-newdata-","dir":"Reference","previous_headings":"","what":"Method sample_newdata()","title":"Feature Sampler Class — FeatureSampler","text":"Sample values feature(s) using external data","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$sample_newdata(feature, newdata)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"feature (character) Feature name(s) sample (can single multiple) newdata (data.table ) External data use sampling.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Feature Sampler Class — FeatureSampler","text":"Print sampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$print(...)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"... Ignored.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Feature Sampler Class — FeatureSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Feature Sampler Class — FeatureSampler","text":"","code":"FeatureSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/FeatureSampler.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Feature Sampler Class — FeatureSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"KnockoffSampler defaulting second-order Gaussian knockoffs created knockoff::create.second_order.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"equivalent KnockoffSampler using default knockoff_fun.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"Watson D, Wright M (2021). “Testing Conditional Independence Supervised Learning Algorithms.” Machine Learning, 110(8), 2107–2129. doi:10.1007/s10994-021-06030-6 . Blesch K, Watson D, Wright M (2023). “Conditional Feature Importance Mixed Data.” AStA Advances Statistical Analysis, 108(2), 259–278. doi:10.1007/s10182-023-00477-9 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"xplainfi::FeatureSampler -> xplainfi::KnockoffSampler -> KnockoffGaussianSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"feature_types (character()) Feature types supported sampler. checked provided mlr3::Task ensure compatibility. x_tilde Knockoff matrix","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"xplainfi::FeatureSampler$print() xplainfi::FeatureSampler$sample_newdata() xplainfi::KnockoffSampler$sample()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"KnockoffGaussianSampler$new() KnockoffGaussianSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"Creates new instance using Gaussian knockoffs via knockoff::create.second_order.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"","code":"KnockoffGaussianSampler$new(task, iters = 1)"},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"task (mlr3::Task) Task sample . iters (integer(1): 1) Number repetitions knockoff_fun applied create multiple x_tilde instances per observation.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"","code":"KnockoffGaussianSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffGaussianSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Gaussian Knockoff Conditional Sampler — KnockoffGaussianSampler","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100) # Create sampler sampler = KnockoffGaussianSampler$new(task) # Sample using row_ids from stored task sampled_data = sampler$sample(\"x1\")"},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Knockoff Sampler — KnockoffSampler","title":"Knockoff Sampler — KnockoffSampler","text":"Implements conditional sampling using Knockoffs.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Knockoff Sampler — KnockoffSampler","text":"KnockoffSampler samples Knockoffs based task data. class allows arbitrary knockoff_fun, also means input checking supported feature types can done. Use KnockoffGaussianSampler Gaussian knockoff sampler numeric features. Alternative knockoff samplers include knockoff_seq() seqknockoff package available GitHub: https://github.com/kormama1/seqknockoff. Knockoffs related ConditionalSampler familty, key differences: allow specifying conditioning_set","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Knockoff Sampler — KnockoffSampler","text":"Watson D, Wright M (2021). “Testing Conditional Independence Supervised Learning Algorithms.” Machine Learning, 110(8), 2107–2129. doi:10.1007/s10994-021-06030-6 . Blesch K, Watson D, Wright M (2023). “Conditional Feature Importance Mixed Data.” AStA Advances Statistical Analysis, 108(2), 259–278. doi:10.1007/s10182-023-00477-9 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Knockoff Sampler — KnockoffSampler","text":"xplainfi::FeatureSampler -> KnockoffSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Knockoff Sampler — KnockoffSampler","text":"x_tilde Knockoff matrix one (iters) row(s) per original observation task.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Knockoff Sampler — KnockoffSampler","text":"xplainfi::FeatureSampler$print() xplainfi::FeatureSampler$sample_newdata()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Knockoff Sampler — KnockoffSampler","text":"KnockoffSampler$new() KnockoffSampler$sample() KnockoffSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Knockoff Sampler — KnockoffSampler","text":"Creates new instance KnockoffSampler class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Knockoff Sampler — KnockoffSampler","text":"","code":"KnockoffSampler$new(   task,   knockoff_fun = function(x) knockoff::create.second_order(as.matrix(x)),   iters = 1 )"},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Knockoff Sampler — KnockoffSampler","text":"task (mlr3::Task) Task sample knockoff_fun (function) Function used create knockoff matrix. Default second-order Gaussian knockoffs (knockoff::create.second_order()) iters (integer(1): 1) Number repetitions knockoff_fun applied create multiple x_tilde instances per observation.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Knockoff Sampler — KnockoffSampler","text":"Sample stored task using knockoff values. Replaces specified feature(s) knockoff counterparts pre-generated knockoff matrix.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Knockoff Sampler — KnockoffSampler","text":"","code":"KnockoffSampler$sample(feature, row_ids = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Knockoff Sampler — KnockoffSampler","text":"feature (character) Feature(s) sample. row_ids (integer() | NULL) Row IDs use. NULL, uses rows.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Knockoff Sampler — KnockoffSampler","text":"Modified copy knockoff feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Knockoff Sampler — KnockoffSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Knockoff Sampler — KnockoffSampler","text":"","code":"KnockoffSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Knockoff Sampler — KnockoffSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/KnockoffSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Knockoff Sampler — KnockoffSampler","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 100) # Create sampler with default parameters sampler = KnockoffSampler$new(task) # Sample using row_ids from stored task sampled_data = sampler$sample(\"x1\")"},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":null,"dir":"Reference","previous_headings":"","what":"Leave-One-Covariate-Out (LOCO) — LOCO","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"Calculates Leave-One-Covariate-(LOCO) scores.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"LOCO measures feature importance comparing model performance without feature. feature, model retrained without feature performance difference (reduced_model_loss - full_model_loss) indicates feature's importance. Higher values indicate important features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"Lei J, G'Sell M, Rinaldo , Tibshirani R, Wasserman L (2018). “Distribution-Free Predictive Inference Regression.” Journal American Statistical Association, 113(523), 1094–1111. doi:10.1080/01621459.2017.1307116 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"xplainfi::FeatureImportanceMethod -> xplainfi::WVIM -> LOCO","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores() xplainfi::WVIM$importance()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"LOCO$new() LOCO$compute() LOCO$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"","code":"LOCO$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   n_repeats = 30L )"},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"task (mlr3::Task) Task compute importance . learner (mlr3::Learner) Learner use prediction. measure (mlr3::Measure: NULL) Measure use scoring. Defaults classif.ce classification regr.mse regression. resampling (mlr3::Resampling) Resampling strategy. Defaults holdout. features (character()) Features compute importance . Defaults features. n_repeats (integer(1): 30L) Number refit iterations per resampling iteration.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"Compute LOCO importances.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"","code":"LOCO$compute(store_models = TRUE, store_backends = TRUE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"store_models, store_backends (logical(1): TRUE) Whether store fitted models / data backends, passed mlr3::resample internally","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"","code":"LOCO$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/LOCO.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Leave-One-Covariate-Out (LOCO) — LOCO","text":"","code":"library(mlr3) library(mlr3learners)  task <- sim_dgp_correlated(n = 500)  loco <- LOCO$new(   task = task,   learner = lrn(\"regr.rpart\"),   measure = msr(\"regr.mse\"),   n_repeats = 5 ) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 167) loco$compute() loco$importance() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x1  0.7297937 #> 2:      x2  0.0000000 #> 3:      x3  0.6154164 #> 4:      x4  0.0000000"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal Permutation Sampler — MarginalPermutationSampler","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"Implements marginal permutation-based sampling Permutation Feature Importance (PFI). specified feature randomly shuffled (permuted) independently, breaking relationship feature target well rows.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"permutation sampler randomly shuffles feature values across observations: feature permuted independently within column association feature values target values broken association feature values across rows broken marginal distribution feature preserved Important distinction SAGE's \"marginal\" approach: MarginalPermutationSampler: Shuffles features independently, breaking row structure MarginalSAGE: Uses reference data keeps rows intact (features coalition stay together) classic approach used Permutation Feature Importance (PFI) assumes features independent.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"xplainfi::FeatureSampler -> xplainfi::MarginalSampler -> MarginalPermutationSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"xplainfi::FeatureSampler$print() xplainfi::MarginalSampler$sample() xplainfi::MarginalSampler$sample_newdata()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"MarginalPermutationSampler$new() MarginalPermutationSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"Creates new instance MarginalPermutationSampler class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"","code":"MarginalPermutationSampler$new(task)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"task (mlr3::Task) Task sample .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"","code":"MarginalPermutationSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalPermutationSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Marginal Permutation Sampler — MarginalPermutationSampler","text":"","code":"library(mlr3) task = tgen(\"2dnormals\")$generate(n = 10) task$data() #>          y         x1          x2 #>     <fctr>      <num>       <num> #>  1:      A  1.7773348  2.26175298 #>  2:      B -2.0069035  0.05488779 #>  3:      A  1.4039615  0.91474000 #>  4:      A  1.6617618  2.08743917 #>  5:      A  1.2320348  2.61978528 #>  6:      A  1.9551732 -0.87643544 #>  7:      B  0.3575475 -1.20454686 #>  8:      B -0.2497584 -1.51014735 #>  9:      B -0.2405774 -1.17051133 #> 10:      A -0.4771260  0.31415101 sampler = MarginalPermutationSampler$new(task)  # Sample using row_ids from stored task sampler$sample(\"x1\") #>          y         x1          x2 #>     <fctr>      <num>       <num> #>  1:      A  1.6617618  2.26175298 #>  2:      B  1.4039615  0.05488779 #>  3:      A -0.4771260  0.91474000 #>  4:      A  1.7773348  2.08743917 #>  5:      A  0.3575475  2.61978528 #>  6:      A  1.9551732 -0.87643544 #>  7:      B -0.2497584 -1.20454686 #>  8:      B  1.2320348 -1.51014735 #>  9:      B -2.0069035 -1.17051133 #> 10:      A -0.2405774  0.31415101  # Or use external data data = task$data() sampler$sample_newdata(\"x1\", newdata = data) #>          y         x1          x2 #>     <fctr>      <num>       <num> #>  1:      A  1.7773348  2.26175298 #>  2:      B  1.4039615  0.05488779 #>  3:      A  0.3575475  0.91474000 #>  4:      A -0.2497584  2.08743917 #>  5:      A -0.4771260  2.61978528 #>  6:      A  1.9551732 -0.87643544 #>  7:      B -0.2405774 -1.20454686 #>  8:      B  1.6617618 -1.51014735 #>  9:      B -2.0069035 -1.17051133 #> 10:      A  1.2320348  0.31415101"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal Reference Sampler — MarginalReferenceSampler","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"Samples complete observations reference data replace feature values. approach samples marginal distribution preserving within-row feature dependencies.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"sampler implements called \"marginal imputation\" SAGE literature (Covert et al. 2020). observation, samples complete row reference data takes specified feature values row. approach: Samples marginal distribution \\(P(X_S)\\) S set features Preserves dependencies within sampled reference row Breaks dependencies test reference data Terminology note: SAGE literature, called \"marginal imputation\" features outside coalition \"imputed\" sampling marginal distribution. use MarginalReferenceSampler avoid confusion missing data imputation clarify samples reference data. Comparison samplers: MarginalPermutationSampler: Shuffles feature independently, breaking row structure MarginalReferenceSampler: Samples complete rows, preserving within-row dependencies ConditionalSampler: Samples \\(P(X_S | X_{-S})\\), conditioning features Use SAGE: default approach MarginalSAGE. test observation x features marginalize S, samples reference row x_ref creates \"hybrid\" observation combining x's coalition features x_ref's marginalized features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"Covert , Lundberg S, Lee S (2020). “Understanding Global Feature Contributions Additive Importance Measures.” Advances Neural Information Processing Systems, volume 33, 17212–17223. https://proceedings.neurips.cc/paper/2020/hash/c7bf0b7c1a86d5eb3be2c722cf2cf746-Abstract.html.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"xplainfi::FeatureSampler -> xplainfi::MarginalSampler -> MarginalReferenceSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"reference_data (data.table) Reference data sample marginalization.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"xplainfi::FeatureSampler$print() xplainfi::MarginalSampler$sample() xplainfi::MarginalSampler$sample_newdata()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"MarginalReferenceSampler$new() MarginalReferenceSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"Creates new instance MarginalReferenceSampler class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"","code":"MarginalReferenceSampler$new(task, n_samples = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"task (mlr3::Task) Task sample . n_samples (integer(1) | NULL) Number reference samples use. NULL, uses task data reference.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"","code":"MarginalReferenceSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalReferenceSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Marginal Reference Sampler — MarginalReferenceSampler","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 100)  # Default: uses all task data as reference sampler = MarginalReferenceSampler$new(task) sampled = sampler$sample(\"important1\", row_ids = 1:10)  # Subsample reference data to 50 rows sampler_subsampled = MarginalReferenceSampler$new(task, n_samples = 50L) sampled2 = sampler_subsampled$sample(\"important1\", row_ids = 1:10)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal SAGE — MarginalSAGE","title":"Marginal SAGE — MarginalSAGE","text":"SAGE marginal sampling (features marginalized independently). standard SAGE implementation.","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Marginal SAGE — MarginalSAGE","text":"xplainfi::FeatureImportanceMethod -> xplainfi::SAGE -> MarginalSAGE","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Marginal SAGE — MarginalSAGE","text":"xplainfi::FeatureImportanceMethod$importance() xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores() xplainfi::SAGE$compute() xplainfi::SAGE$plot_convergence()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Marginal SAGE — MarginalSAGE","text":"MarginalSAGE$new() MarginalSAGE$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Marginal SAGE — MarginalSAGE","text":"Creates new instance MarginalSAGE class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal SAGE — MarginalSAGE","text":"","code":"MarginalSAGE$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   n_permutations = 10L,   batch_size = 5000L,   n_samples = 100L,   early_stopping = FALSE,   se_threshold = 0.01,   min_permutations = 10L,   check_interval = 1L )"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal SAGE — MarginalSAGE","text":"task, learner, measure, resampling, features, n_permutations, batch_size, n_samples, early_stopping, se_threshold, min_permutations, check_interval Passed SAGE.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Marginal SAGE — MarginalSAGE","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal SAGE — MarginalSAGE","text":"","code":"MarginalSAGE$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal SAGE — MarginalSAGE","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSAGE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Marginal SAGE — MarginalSAGE","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 100) sage = MarginalSAGE$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 50),   measure = msr(\"regr.mse\"),   n_permutations = 3L,   n_samples = 20 ) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 33) sage$compute()"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal Sampler Base Class — MarginalSampler","title":"Marginal Sampler Base Class — MarginalSampler","text":"Abstract base class marginal sampling strategies condition features. Marginal samplers sample P(X_S), marginal distribution features S, ignoring dependencies features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Marginal Sampler Base Class — MarginalSampler","text":"class provides common interface different marginal sampling approaches: MarginalPermutationSampler: Shuffles features independently within dataset MarginalReferenceSampler: Samples complete rows reference data approaches sample marginal distribution P(X_S), differ preserve break within-row dependencies: Permutation breaks dependencies (target features) Reference sampling preserves WITHIN-row dependencies breaks dependencies test data Comparison ConditionalSampler: MarginalSampler: Samples \\(P(X_S)\\) - conditioning ConditionalSampler: Samples \\(P(X_S | X_{-S})\\)- conditions features base class implements public $sample() $sample_newdata() methods, delegating private .sample_marginal() subclasses must implement.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Marginal Sampler Base Class — MarginalSampler","text":"xplainfi::FeatureSampler -> MarginalSampler","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Marginal Sampler Base Class — MarginalSampler","text":"xplainfi::FeatureSampler$initialize() xplainfi::FeatureSampler$print()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Marginal Sampler Base Class — MarginalSampler","text":"MarginalSampler$sample() MarginalSampler$sample_newdata() MarginalSampler$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"method-sample-","dir":"Reference","previous_headings":"","what":"Method sample()","title":"Marginal Sampler Base Class — MarginalSampler","text":"Sample features marginal distribution.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Sampler Base Class — MarginalSampler","text":"","code":"MarginalSampler$sample(feature, row_ids = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Sampler Base Class — MarginalSampler","text":"feature (character()) Feature name(s) sample. row_ids (integer() | NULL) Row IDs task use.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Marginal Sampler Base Class — MarginalSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"method-sample-newdata-","dir":"Reference","previous_headings":"","what":"Method sample_newdata()","title":"Marginal Sampler Base Class — MarginalSampler","text":"Sample external data.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Sampler Base Class — MarginalSampler","text":"","code":"MarginalSampler$sample_newdata(feature, newdata)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Sampler Base Class — MarginalSampler","text":"feature (character()) Feature(s) sample. newdata (data.table) External data use.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Marginal Sampler Base Class — MarginalSampler","text":"Modified copy sampled feature(s).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Marginal Sampler Base Class — MarginalSampler","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal Sampler Base Class — MarginalSampler","text":"","code":"MarginalSampler$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/MarginalSampler.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal Sampler Base Class — MarginalSampler","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutation Feature Importance — PFI","title":"Permutation Feature Importance — PFI","text":"Implementation Permutation Feature Importance (PFI) using modular sampling approach. PFI measures importance feature calculating increase model error feature's values randomly permuted, breaking relationship feature target variable.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Permutation Feature Importance — PFI","text":"Permutation Feature Importance originally introduced Breiman (2001) part Random Forest algorithm. method works : Computing baseline model performance original dataset feature, randomly permuting values keeping features unchanged Computing model performance permuted dataset Calculating importance difference (ratio) permuted original performance","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Permutation Feature Importance — PFI","text":"Breiman L (2001). “Random Forests.” Machine Learning, 45(1), 5–32. doi:10.1023/:1010933404324 . Fisher , Rudin C, Dominici F (2019). “Models Wrong, Many Useful: Learning Variable's Importance Studying Entire Class Prediction Models Simultaneously.” Journal Machine Learning Research, 20, 177. https://pmc.ncbi.nlm.nih.gov/articles/PMC8323609/. Strobl C, Boulesteix , Kneib T, Augustin T, Zeileis (2008). “Conditional Variable Importance Random Forests.” BMC Bioinformatics, 9(1), 307. doi:10.1186/1471-2105-9-307 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Permutation Feature Importance — PFI","text":"xplainfi::FeatureImportanceMethod -> xplainfi::PerturbationImportance -> PFI","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Permutation Feature Importance — PFI","text":"xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores() xplainfi::PerturbationImportance$importance()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Permutation Feature Importance — PFI","text":"PFI$new() PFI$compute() PFI$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Permutation Feature Importance — PFI","text":"Creates new instance PFI class","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Feature Importance — PFI","text":"","code":"PFI$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   groups = NULL,   relation = \"difference\",   n_repeats = 30L,   batch_size = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Feature Importance — PFI","text":"task, learner, measure, resampling, features, groups, relation, n_repeats, batch_size Passed PerturbationImportance","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Permutation Feature Importance — PFI","text":"Compute PFI scores","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Feature Importance — PFI","text":"","code":"PFI$compute(   n_repeats = NULL,   batch_size = NULL,   store_models = TRUE,   store_backends = TRUE )"},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Feature Importance — PFI","text":"n_repeats (integer(1); NULL) Number permutation iterations. NULL, uses stored value. batch_size (integer(1) | NULL: NULL) Maximum number rows predict . NULL, uses stored value. store_models, store_backends (logical(1): TRUE) Whether store fitted models / data backends, passed mlr3::resample internally initial fit learner. may required certain measures recommended leave enabled unless really necessary.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Permutation Feature Importance — PFI","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Feature Importance — PFI","text":"","code":"PFI$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Feature Importance — PFI","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PFI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Permutation Feature Importance — PFI","text":"","code":"library(mlr3)  task <- sim_dgp_correlated(n = 500)  pfi <- PFI$new(   task = task,   learner = lrn(\"regr.rpart\"),   measure = msr(\"regr.mse\"), n_repeats = 5 ) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 167) pfi$compute() pfi$importance() #> Key: <feature> #>    feature importance #>     <char>      <num> #> 1:      x1   7.429244 #> 2:      x2   0.000000 #> 3:      x3   1.380869 #> 4:      x4   0.000000"},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":null,"dir":"Reference","previous_headings":"","what":"Perturbation Feature Importance Base Class — PerturbationImportance","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"Abstract base class perturbation-based importance methods PFI, CFI, RFI","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"xplainfi::FeatureImportanceMethod -> PerturbationImportance","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"sampler (FeatureSampler) Sampler object feature perturbation","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"xplainfi::FeatureImportanceMethod$compute() xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"PerturbationImportance$new() PerturbationImportance$importance() PerturbationImportance$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"Creates new instance PerturbationImportance class","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"","code":"PerturbationImportance$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   groups = NULL,   sampler = NULL,   relation = \"difference\",   n_repeats = 30L,   batch_size = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"task, learner, measure, resampling, features, groups Passed FeatureImportanceMethod. sampler (FeatureSampler) Sampler use feature perturbation. relation (character(1): \"difference\") relate perturbed baseline scores. Can also \"ratio\". n_repeats (integer(1): 30L) Number permutation/conditional sampling iterations. Can also overridden $compute(). batch_size (integer(1) | NULL: NULL) Maximum number rows predict . NULL, predicts test_size * n_repeats rows one call. Use smaller values reduce memory usage cost prediction calls. Can overridden $compute().","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"method-importance-","dir":"Reference","previous_headings":"","what":"Method importance()","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"Get aggregated importance scores. Extends base $importance() method support ci_method = \"cpi\". details, see CFI, sub-method known valid.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"","code":"PerturbationImportance$importance(   relation = NULL,   standardize = FALSE,   ci_method = c(\"none\", \"raw\", \"nadeau_bengio\", \"quantile\", \"cpi\"),   conf_level = 0.95,   alternative = c(\"two.sided\", \"greater\"),   test = c(\"t\", \"wilcoxon\", \"fisher\", \"binomial\"),   B = 1999,   p_adjust = \"none\",   ... )"},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"relation (character(1)) relate perturbed scores originals (\"difference\" \"ratio\"). NULL, uses stored parameter value. standardize (logical(1): FALSE) TRUE, importances standardized highest score scores fall [-1, 1]. ci_method (character(1): \"none\") Variance estimation method. addition base methods (\"none\", \"raw\", \"nadeau_bengio\", \"quantile\"), perturbation methods support \"cpi\" (Conditional Predictive Impact). CPI specifically designed CFI knockoff samplers uses one-sided hypothesis tests. conf_level (numeric(1): 0.95) Confidence level confidence intervals ci_method != \"none\". alternative (character(1): \"two.sided\") Type alternative hypothesis statistical tests. \"greater\" tests H0: importance <= 0 vs H1: importance > 0 (one-sided). \"two.sided\" tests H0: importance = 0 vs H1: importance != 0. test (character(1): \"t\") Test use CPI. One \"t\", \"wilcoxon\", \"fisher\", \"binomial\". used ci_method = \"cpi\". B (integer(1): 1999) Number replications Fisher test. used ci_method = \"cpi\" test = \"fisher\". p_adjust (character(1): \"none\") Method p-value adjustment multiple comparisons. Accepts method supported stats::p.adjust.methods, e.g. \"holm\", \"bonferroni\", \"BH\", \"none\". \"bonferroni\", confidence intervals also adjusted (alpha/k). correction methods (e.g. \"holm\", \"BH\"), p-values adjusted; confidence intervals remain nominal conf_level sequential/adaptive procedures clean per-comparison alpha CI construction. ... Additional arguments passed base method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"(data.table) Aggregated importance scores.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"","code":"PerturbationImportance$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/PerturbationImportance.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perturbation Feature Importance Base Class — PerturbationImportance","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":null,"dir":"Reference","previous_headings":"","what":"Relative Feature Importance — RFI","title":"Relative Feature Importance — RFI","text":"RFI generalizes CFI PFI arbitrary conditioning sets samplers.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Relative Feature Importance — RFI","text":"König G, Molnar C, Bischl B, Grosse-Wentrup M (2021). “Relative Feature Importance.” 2020 25th International Conference Pattern Recognition (ICPR), 9318–9325. doi:10.1109/ICPR48806.2021.9413090 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Relative Feature Importance — RFI","text":"xplainfi::FeatureImportanceMethod -> xplainfi::PerturbationImportance -> RFI","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Relative Feature Importance — RFI","text":"xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores() xplainfi::PerturbationImportance$importance()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Relative Feature Importance — RFI","text":"RFI$new() RFI$compute() RFI$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Relative Feature Importance — RFI","text":"Creates new instance RFI class","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Feature Importance — RFI","text":"","code":"RFI$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   groups = NULL,   conditioning_set = NULL,   relation = \"difference\",   n_repeats = 30L,   batch_size = NULL,   sampler = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Feature Importance — RFI","text":"task, learner, measure, resampling, features, groups, relation, n_repeats, batch_size Passed PerturbationImportance. conditioning_set (character()) Set features condition . Can overridden $compute(). Default (character(0)) equivalent PFI. CFI, set features except interest. sampler (ConditionalSampler) Optional custom sampler. Defaults ConditionalARFSampler.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Relative Feature Importance — RFI","text":"Compute RFI scores","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Feature Importance — RFI","text":"","code":"RFI$compute(   conditioning_set = NULL,   n_repeats = NULL,   batch_size = NULL,   store_models = TRUE,   store_backends = TRUE )"},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Feature Importance — RFI","text":"conditioning_set (character()) Set features condition . NULL, uses stored parameter value. n_repeats (integer(1)) Number permutation iterations. NULL, uses stored value. batch_size (integer(1) | NULL: NULL) Maximum number rows predict . NULL, uses stored value. store_models, store_backends (logical(1): TRUE) Whether store fitted models / data backends, passed mlr3::resample internally initial fit learner. may required certain measures recommended leave enabled unless really necessary.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Relative Feature Importance — RFI","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Relative Feature Importance — RFI","text":"","code":"RFI$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relative Feature Importance — RFI","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/RFI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relative Feature Importance — RFI","text":"","code":"library(mlr3) task = tgen(\"friedman1\")$generate(n = 200) rfi = RFI$new(   task = task,   learner = lrn(\"regr.rpart\"),   measure = msr(\"regr.mse\"),   conditioning_set = c(\"important1\"),   sampler = ConditionalGaussianSampler$new(task),   n_repeats = 5 ) #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 67) rfi$compute() rfi$importance() #> Key: <feature> #>          feature importance #>           <char>      <num> #>  1:   important1  0.0000000 #>  2:   important2 14.8407870 #>  3:   important3  0.0000000 #>  4:   important4 15.4262312 #>  5:   important5  2.7794557 #>  6: unimportant1  0.0000000 #>  7: unimportant2  0.1232188 #>  8: unimportant3  0.0000000 #>  9: unimportant4  0.0000000 #> 10: unimportant5  0.0000000"},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":null,"dir":"Reference","previous_headings":"","what":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Base class SAGE (Shapley Additive Global Importance) feature importance based Shapley values marginalization. abstract class - use MarginalSAGE ConditionalSAGE.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"SAGE uses Shapley values fairly distribute total prediction performance among features. Unlike perturbation-based methods, SAGE marginalizes features integrating distribution. approximated averaging predictions reference dataset. Standard Error Calculation: standard errors (SE) reported $convergence_history reflect uncertainty Shapley value estimation across different random permutations within single resampling iteration. SEs quantify Monte Carlo sampling error fixed trained model valid inference importance features specific model. capture broader uncertainty model variability across different train/test splits resampling iterations.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Covert , Lundberg S, Lee S (2020). “Understanding Global Feature Contributions Additive Importance Measures.” Advances Neural Information Processing Systems, volume 33, 17212–17223. https://proceedings.neurips.cc/paper/2020/hash/c7bf0b7c1a86d5eb3be2c722cf2cf746-Abstract.html.","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"xplainfi::FeatureImportanceMethod -> SAGE","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"n_permutations (integer(1)) Number permutations sample. convergence_history (data.table) History SAGE values computation. converged (logical(1)) Whether convergence detected. n_permutations_used (integer(1)) Actual number permutations used.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"xplainfi::FeatureImportanceMethod$importance() xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"SAGE$new() SAGE$compute() SAGE$plot_convergence() SAGE$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Creates new instance SAGE class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   n_permutations = 10L,   batch_size = 5000L,   n_samples = 100L,   early_stopping = TRUE,   se_threshold = 0.01,   min_permutations = 10L,   check_interval = 1L )"},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"task, learner, measure, resampling, features Passed FeatureImportanceMethod. n_permutations (integer(1): 10L) Number permutations sample SAGE value estimation. total number evaluated coalitions 1 (empty) + n_permutations * n_features. batch_size (integer(1): 5000L) Maximum number observations process single prediction call. n_samples (integer(1): 100L) Number samples use marginalizing --coalition features. MarginalSAGE, number marginal data samples (\"background data\" implementations). ConditionalSAGE, number conditional samples per test instance retrieved sampler. early_stopping (logical(1): TRUE) Whether enable early stopping based convergence detection. se_threshold (numeric(1): 0.01) Convergence threshold relative standard error. Convergence detected maximum relative SE across features falls threshold. Relative SE calculated SE divided range importance values (max - min), making scale-invariant across different loss metrics. Default 0.01 means convergence relative SE 1% importance range. min_permutations (integer(1): 10L) Minimum permutations checking convergence. Convergence judged based standard errors estimated SAGE values, requires sufficiently large number samples (.e., evaluated coalitions). check_interval (integer(1): 1L) Check convergence every N permutations.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Compute SAGE values.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$compute(   store_backends = TRUE,   batch_size = NULL,   early_stopping = NULL,   se_threshold = NULL,   min_permutations = NULL,   check_interval = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"store_backends (logical(1)) Whether store data backends. batch_size (integer(1): 5000L) Maximum number observations process single prediction call. early_stopping (logical(1): TRUE) Whether check convergence stop early. se_threshold (numeric(1): 0.01) Convergence threshold relative standard error. SE normalized range importance values (max - min) make convergence detection scale-invariant. Default 0.01 means convergence relative SE < 1%. min_permutations (integer(1): 10L) Minimum permutations checking convergence. check_interval (integer(1): 1L) Check convergence every N permutations.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"method-plot-convergence-","dir":"Reference","previous_headings":"","what":"Method plot_convergence()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"Plot convergence history SAGE values.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$plot_convergence(features = NULL)"},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"features (character | NULL) Features plot. NULL, plots features.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"ggplot2 object","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"","code":"SAGE$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/SAGE.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shapley Additive Global Importance (SAGE) Base Class — SAGE","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":null,"dir":"Reference","previous_headings":"","what":"Williamson's Variable Importance Measure (WVIM) — WVIM","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"Base class generalizing refit-based variable importance measures. Default corresponds leaving feature n_repeats times, corresponds LOCO (Leave One Covariate ).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"Lei J, G'Sell M, Rinaldo , Tibshirani R, Wasserman L (2018). “Distribution-Free Predictive Inference Regression.” Journal American Statistical Association, 113(523), 1094–1111. doi:10.1080/01621459.2017.1307116 .","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"xplainfi::FeatureImportanceMethod -> WVIM","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"direction (character(1)) Either \"leave-\" \"leave-\". design (logical()) Feature selection design matrix TRUE equals \"left \" FALSE \"left \". Columns correspond task$feature_names number rows corresponds length(features) * n_repeats. base matrix created wvim_design_matrix replicated n_repeats times . instance (FSelectInstanceBatchSingleCrit) mlr3fselect feature selection instance containing also archive evaluations, possible useful future use. stored store_instance TRUE.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"xplainfi::FeatureImportanceMethod$obs_loss() xplainfi::FeatureImportanceMethod$print() xplainfi::FeatureImportanceMethod$reset() xplainfi::FeatureImportanceMethod$scores()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"WVIM$new() WVIM$importance() WVIM$compute() WVIM$clone()","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"","code":"WVIM$new(   task,   learner,   measure = NULL,   resampling = NULL,   features = NULL,   groups = NULL,   direction = c(\"leave-out\", \"leave-in\"),   label = \"Williamson's Variable Importance Measure (WVIM)\",   n_repeats = 30L )"},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"task, learner, measure, resampling, features, groups Passed FeatureImportanceMethod construction. direction (character(1)) Either \"leave-\" \"leave-\". label (character(1)) Method label. n_repeats (integer(1): 30L) Number refit iterations per resampling iteration.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"method-importance-","dir":"Reference","previous_headings":"","what":"Method importance()","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"Get aggregated importance scores. Extends base $importance() method support ci_method = \"lei\". implements distribution-free inference based Lei et al. (2018), testing observation-wise loss differences using Wilcoxon signed-rank test default. Lei et al. (2018) proposed method specifically LOCO L1 (absolute) loss, median aggregation, single train/test split (holdout). inference conditional training data, requiring ..d. test observations single split. aggregation function, statistical test, resampling strategy parameterizable, deviating defaults may invalidate theoretical guarantees. comprehensive overview inference methods, see vignette(\"inference\", package = \"xplainfi\").","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"","code":"WVIM$importance(   relation = NULL,   standardize = FALSE,   ci_method = c(\"none\", \"raw\", \"nadeau_bengio\", \"quantile\", \"lei\"),   conf_level = 0.95,   alternative = c(\"two.sided\", \"greater\"),   test = c(\"wilcoxon\", \"t\", \"fisher\", \"binomial\"),   B = 1999,   aggregator = NULL,   p_adjust = \"none\",   ... )"},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"relation (character(1)) relate perturbed scores originals (\"difference\" \"ratio\"). NULL, uses stored parameter value. standardize (logical(1): FALSE) TRUE, importances standardized highest score scores fall [-1, 1]. ci_method (character(1): \"none\") Variance estimation method. addition base methods (\"none\", \"raw\", \"nadeau_bengio\", \"quantile\"), WVIM methods support \"lei\" distribution-free inference (Lei et al., 2018). conf_level (numeric(1): 0.95) Confidence level use confidence interval construction ci_method != \"none\". alternative (character(1): \"two.sided\") Type alternative hypothesis statistical tests. \"greater\" tests H0: importance <= 0 vs H1: importance > 0 (one-sided). \"two.sided\" tests H0: importance = 0 vs H1: importance != 0. test (character(1): \"wilcoxon\") Test use Lei et al. inference. One \"wilcoxon\", \"t\", \"fisher\", \"binomial\". used ci_method = \"lei\". B (integer(1): 1999) Number replications Fisher test. used ci_method = \"lei\" test = \"fisher\". aggregator (function: stats::median) Aggregation function computing point estimate observation-wise importance values. Defaults stats::median proposed Lei et al. (2018). used ci_method = \"lei\". p_adjust (character(1): \"none\") Method p-value adjustment multiple comparisons. Accepts method supported stats::p.adjust.methods, e.g. \"holm\", \"bonferroni\", \"BH\", \"none\". \"bonferroni\", confidence intervals also adjusted (alpha/k). correction methods (e.g. \"holm\", \"BH\"), p-values adjusted; confidence intervals remain nominal conf_level sequential/adaptive procedures clean per-comparison alpha CI construction. ... Additional arguments passed base method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"(data.table) Aggregated importance scores.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"method-compute-","dir":"Reference","previous_headings":"","what":"Method compute()","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"Computes leave-leave-feature importance. wvim_design_matrix(task$feature_names, \"leave-\") corresponds LOCO.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"","code":"WVIM$compute(   store_models = TRUE,   store_backends = TRUE,   store_instance = FALSE )"},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"store_models, store_backends (logical(1): TRUE) Whether store fitted models / data backends, passed mlr3::resample internally backends resample result. Required measures, may increase memory footprint. store_instance (logical(1): FALSE) Whether store mlr3fselect::mlr3fselect instance $instance.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"objects class cloneable method.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"","code":"WVIM$clone(deep = FALSE)"},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/WVIM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Williamson's Variable Importance Measure (WVIM) — WVIM","text":"","code":"library(mlr3) library(mlr3learners)  task <- sim_dgp_correlated(n = 500)  # Group correlated features together, independent features separately groups <- list(   correlated = c(\"x1\", \"x2\"),   independent = c(\"x3\", \"x4\") )  wvim <- WVIM$new(   task = task,   learner = lrn(\"regr.ranger\", num.trees = 10),   groups = groups,   n_repeats = 1 ) #> ℹ No <Measure> provided, using `measure = msr(\"regr.mse\")` #> ℹ No <Resampling> provided, using `resampling = rsmp(\"holdout\", ratio = 2/3)` #>   (test set size: 167) wvim$compute() wvim$importance() #> Key: <feature> #>        feature importance #>         <char>      <num> #> 1:  correlated  4.1381889 #> 2: independent  0.8908186"},{"path":"https://mlr-org.github.io/xplainfi/reference/check_groups.html","id":null,"dir":"Reference","previous_headings":"","what":"Check group specification — check_groups","title":"Check group specification — check_groups","text":"Check group specification","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/check_groups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check group specification — check_groups","text":"","code":"check_groups(groups, all_features)"},{"path":"https://mlr-org.github.io/xplainfi/reference/check_groups.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check group specification — check_groups","text":"groups (list) (named) list groups all_features (character()) available feature names task.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/check_groups.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check group specification — check_groups","text":"input list group, element now named.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/check_groups.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check group specification — check_groups","text":"","code":"task <- sim_dgp_interactions(n = 100) task$feature_names #> [1] \"noise1\" \"noise2\" \"x1\"     \"x2\"     \"x3\"      # Intended use groups1 = list(effects = c(\"x1\", \"x2\", \"x3\"), noise = c(\"noise1\", \"noise2\")) check_groups(groups1, task$feature_names) #> $effects #> [1] \"x1\" \"x2\" \"x3\" #>  #> $noise #> [1] \"noise1\" \"noise2\" #>   # Names are auto-generated where needed check_groups(list(a = \"x1\",  c(\"x2\", \"x1\")), task$feature_names) #> ! Feature is specified in multiple groups: \"x1\" #> Not all groups are named #> ℹ Group \"2\" is named automatically #> $a #> [1] \"x1\" #>  #> $GroupB #> [1] \"x2\" \"x1\" #>   # Examples for cases that throw errors:  # Unexpected features groups2 = list(effects = c(\"x1\", \"foo\", \"bar\", \"x1\")) try(check_groups(groups2, task$feature_names)) #> ! Feature is specified in multiple groups: \"x1\" #> Error in check_groups(groups2, task$feature_names) :  #>   Features specified in `groups` not in provided <Task>: \"foo\" and \"bar\" # Too deeply nested groups3 = list(effects = c(\"x1\", \"x2\", \"x3\"), noise = c(\"noise1\", list(c(\"noise2\")))) try(check_groups(groups2, task$feature_names)) #> ! Feature is specified in multiple groups: \"x1\" #> Error in check_groups(groups2, task$feature_names) :  #>   Features specified in `groups` not in provided <Task>: \"foo\" and \"bar\""},{"path":"https://mlr-org.github.io/xplainfi/reference/op-null-default.html","id":null,"dir":"Reference","previous_headings":"","what":"Default value for NULL — op-null-default","title":"Default value for NULL — op-null-default","text":"backport %||% available R versions 4.4.0.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/op-null-default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default value for NULL — op-null-default","text":"","code":"x %||% y"},{"path":"https://mlr-org.github.io/xplainfi/reference/op-null-default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Default value for NULL — op-null-default","text":"x, y x NULL length 0, return y; otherwise returns x.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/op-null-default.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Default value for NULL — op-null-default","text":"","code":"1 %||% 2 #> [1] 1 NULL %||% 2 #> [1] 2"},{"path":"https://mlr-org.github.io/xplainfi/reference/print_bib.html","id":null,"dir":"Reference","previous_headings":"","what":"Print an Rd-formatted bib entry — print_bib","title":"Print an Rd-formatted bib entry — print_bib","text":"Print Rd-formatted bib entry","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/print_bib.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print an Rd-formatted bib entry — print_bib","text":"","code":"print_bib(...)"},{"path":"https://mlr-org.github.io/xplainfi/reference/print_bib.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print an Rd-formatted bib entry — print_bib","text":"... (character) One quoted names bibentries print.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/rsmp_all_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a resampling with all data being test data — rsmp_all_test","title":"Create a resampling with all data being test data — rsmp_all_test","text":"Utility use pretrained learner importance methods support ","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/rsmp_all_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a resampling with all data being test data — rsmp_all_test","text":"","code":"rsmp_all_test(task)"},{"path":"https://mlr-org.github.io/xplainfi/reference/rsmp_all_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a resampling with all data being test data — rsmp_all_test","text":"task (mlr3::Task)","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/rsmp_all_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a resampling with all data being test data — rsmp_all_test","text":"mlr3::Resampling empty train_set single test_set identical given Task.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/rsmp_all_test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a resampling with all data being test data — rsmp_all_test","text":"Note resulting Resampling empty train set, making useless purpose use pretrained learner.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/rsmp_all_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a resampling with all data being test data — rsmp_all_test","text":"","code":"library(mlr3) # Create custom task from some data.frame custom_task <- as_task_regr(mtcars, target = \"mpg\") # Create matching Resampling with all-test data resampling_custom <- rsmp_all_test(custom_task)"},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_aggregate_predictions.html","id":null,"dir":"Reference","previous_headings":"","what":"Aggregate Predictions by Coalition and Test Instance — sage_aggregate_predictions","title":"Aggregate Predictions by Coalition and Test Instance — sage_aggregate_predictions","text":"Averages predictions across multiple samples (reference data conditional samples) unique combination coalition test instance.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_aggregate_predictions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Aggregate Predictions by Coalition and Test Instance — sage_aggregate_predictions","text":"","code":"sage_aggregate_predictions(   combined_data,   predictions,   task_type,   class_names = NULL )"},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_aggregate_predictions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Aggregate Predictions by Coalition and Test Instance — sage_aggregate_predictions","text":"combined_data (data.table) Data columns .coalition_id, .test_instance_id, feature columns. predictions (matrix numeric) classification: matrix class probabilities. regression: numeric vector predictions. task_type (character(1)) Task type, either \"classif\" \"regr\". class_names (character() NULL: NULL) Character vector class names. Required classification, ignored regression.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_aggregate_predictions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Aggregate Predictions by Coalition and Test Instance — sage_aggregate_predictions","text":"data.table columns: .coalition_id: Coalition identifier (integer) .test_instance_id: Test instance identifier (integer) classification: One column per class averaged probabilities (numeric) regression: avg_pred column averaged predictions (numeric)","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_batch_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch Predict for SAGE — sage_batch_predict","title":"Batch Predict for SAGE — sage_batch_predict","text":"Performs batched prediction combined data manage memory usage. Supports classification (probability predictions) regression.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_batch_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch Predict for SAGE — sage_batch_predict","text":"","code":"sage_batch_predict(learner, combined_data, task, batch_size, task_type)"},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_batch_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch Predict for SAGE — sage_batch_predict","text":"learner (Learner) Trained mlr3 learner. combined_data (data.table) Data feature columns predict . task (Task) mlr3 task object. batch_size (integer(1) NULL) Batch size predictions. NULL total_rows <= batch_size, processes data . task_type (character(1)) Task type, either \"classif\" \"regr\".","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sage_batch_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch Predict for SAGE — sage_batch_predict","text":"classification: matrix class probabilities (n_rows x n_classes). regression: numeric vector predictions (length n_rows).","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"Reproduces data generating process Ewald et al. (2024) benchmarking feature importance methods. Includes correlated features interaction effects.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"","code":"sim_dgp_ewald(n = 500)"},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"n (integer(1)) Number samples create.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"regression task (mlr3::TaskRegr) data.table backend.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"Mathematical Model: $$X_1, X_3, X_5 \\sim \\text{Uniform}(0,1)$$ $$X_2 = X_1 + \\varepsilon_2, \\quad \\varepsilon_2 \\sim N(0, \\sqrt{0.001})$$ $$X_4 = X_3 + \\varepsilon_4, \\quad \\varepsilon_4 \\sim N(0, \\sqrt{0.1})$$ $$Y = X_4 + X_5 + X_4 \\cdot X_5 + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sqrt{0.1})$$ Feature Properties: X1, X3, X5: Independent uniform(0,1) distributions X2: Nearly perfect copy X1 (correlation approximately 0.99) X4: Noisy copy X3 (correlation approximately 0.94) Y depends X4, X5, interaction","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"Ewald F, Bothmann L, Wright M, Bischl B, Casalicchio G, König G (2024). “Guide Feature Importance Methods Scientific Inference.” Longo L, Lapuschkin S, Seifert C (eds.), Explainable Artificial Intelligence, 440–464. ISBN 978-3-031-63797-1, doi:10.1007/978-3-031-63797-1_22 .","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_ewald.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate data as in Ewald et al. (2024) — sim_dgp_ewald","text":"","code":"sim_dgp_ewald(100) #>  #> ── <TaskRegr> (100x6) ────────────────────────────────────────────────────────── #> • Target: y #> • Properties: - #> • Features (5): #>   • dbl (5): x1, x2, x3, x4, x5"},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"data generating processes (DGPs) designed illustrate specific strengths weaknesses different feature importance methods like PFI, CFI, RFI. DGP focuses one primary challenge make differences methods clear.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"","code":"sim_dgp_correlated(n = 500L, r = 0.9)  sim_dgp_mediated(n = 500L)  sim_dgp_confounded(n = 500L, hidden = TRUE)  sim_dgp_interactions(n = 500L)  sim_dgp_independent(n = 500L)"},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"n (integer(1): 500L) Number observations generate. r (numeric(1): 0.9) Correlation x1 x2. Must -1 1. hidden (logical(1): TRUE) Whether hide confounder returned task. FALSE, confounder included feature, allowing direct adjustment. TRUE (default), proxy available, simulating unmeasured confounding.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"regression task (mlr3::TaskRegr) data.table backend.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"Correlated Features DGP: DGP creates highly correlated predictors PFI show artificially low importance due redundancy, CFI correctly identify feature's conditional contribution. Mathematical Model: $$(X_1, X_2)^T \\sim \\text{MVN}(0, \\Sigma)$$ \\(\\Sigma\\) \\(2 \\times 2\\) covariance matrix 1 diagonal correlation \\(r\\) -diagonal. $$X_3 \\sim N(0,1), \\quad X_4 \\sim N(0,1)$$ $$Y = 2 \\cdot X_1 + X_3 + \\varepsilon$$ \\(\\varepsilon \\sim N(0, 0.2^2)\\). Feature Properties: x1: Standard normal MVN, direct causal effect y (\\(\\beta = 2.0\\)) x2: Correlated x1 (correlation = r), causal effect y (\\(\\beta = 0\\)) x3: Independent standard normal, direct causal effect y (\\(\\beta = 1.0\\)) x4: Independent standard normal, effect y (\\(\\beta = 0\\)) Expected Behavior: depend used learner strength correlation (r) Marginal methods (PFI, Marginal SAGE): falsely assign importance x2 due correlation x1 CFI correctly assign near-zero importance x2 x2 \"spurious predictor\" - correlated causal feature causal Mediated Effects DGP: DGP demonstrates difference total direct causal effects. features affect outcome mediators. Mathematical Model: $$\\text{exposure} \\sim N(0,1), \\quad \\text{direct} \\sim N(0,1)$$ $$\\text{mediator} = 0.8 \\cdot \\text{exposure} + 0.6 \\cdot \\text{direct} + \\varepsilon_m$$ $$Y = 1.5 \\cdot \\text{mediator} + 0.5 \\cdot \\text{direct} + \\varepsilon$$ \\(\\varepsilon_m \\sim N(0, 0.3^2)\\) \\(\\varepsilon \\sim N(0, 0.2^2)\\). Feature Properties: exposure: direct effect y, mediator (total effect = 1.2) mediator: Mediates effect exposure y direct: direct effect y effect mediator noise: causal relationship y Causal Structure: exposure -> mediator -> y <- direct -> mediator Confounding DGP: DGP includes confounder affects feature outcome. Uses simple coefficients easy interpretation. Mathematical Model: $$H \\sim N(0,1)$$ $$X_1 = H + \\varepsilon_1$$ $$\\text{proxy} = H + \\varepsilon_p, \\quad \\text{independent} \\sim N(0,1)$$ $$Y = H + X_1 + \\text{independent} + \\varepsilon$$ \\(\\varepsilon \\sim N(0, 0.5^2)\\) independently. Model Structure: Confounder H ~ N(0,1) (potentially unobserved) x1 = H + noise (affected confounder) proxy = H + noise (noisy measurement confounder) independent ~ N(0,1) (truly independent) y = H + x1 + independent + noise Expected Behavior: PFI: show inflated importance x1 due confounding CFI: partially account confounding conditional sampling reduce importance RFI conditioning proxy: reduce confounding bias conditioning proxy Interaction Effects DGP: DGP demonstrates pure interaction effect features main effects. Mathematical Model: $$Y = 2 \\cdot X_1 \\cdot X_2 + X_3 + \\varepsilon$$ \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.5^2)\\). Feature Properties: x1, x2: Independent features interaction effect (main effects) x3: Independent feature main effect noise1, noise2: causal effects Expected Behavior: depend used learner ability model interactions Independent Features DGP: baseline scenario features independent effects additive. importance methods give similar results. Mathematical Model: $$Y = 2.0 \\cdot X_1 + 1.0 \\cdot X_2 + 0.5 \\cdot X_3 + \\varepsilon$$ \\(X_j \\sim N(0,1)\\) independently \\(\\varepsilon \\sim N(0, 0.2^2)\\). Feature Properties: important1-3: Independent features different effect sizes unimportant1-2: Independent noise features effect Expected Behavior: methods: rank features consistently true effect sizes Ground truth: important1 > important2 > important3 > unimportant1,2 (approximately 0)","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"sim_dgp_correlated(): Correlated features demonstrating PFI's limitations sim_dgp_mediated(): Mediated effects showing direct vs total importance sim_dgp_confounded(): Confounding scenario conditional sampling sim_dgp_interactions(): Interaction effects features sim_dgp_independent(): Independent features baseline scenario","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"Ewald F, Bothmann L, Wright M, Bischl B, Casalicchio G, König G (2024). “Guide Feature Importance Methods Scientific Inference.” Longo L, Lapuschkin S, Seifert C (eds.), Explainable Artificial Intelligence, 440–464. ISBN 978-3-031-63797-1, doi:10.1007/978-3-031-63797-1_22 .","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/sim_dgp_scenarios.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulation DGPs for Feature Importance Method Comparison — sim_dgp_scenarios","text":"","code":"task = sim_dgp_correlated(200) task$data() #>               y         x1          x2         x3         x4 #>           <num>      <num>       <num>      <num>      <num> #>   1:  3.5086228  2.3739115  2.27854973 -1.3047030  0.6802114 #>   2:  0.3160273 -0.1202408  0.06809580  0.6088383 -0.2426855 #>   3: -0.4240733  0.2912600  0.70182318 -0.8120685 -1.7451540 #>   4: -2.3854714 -1.2964999 -1.21193492  0.1129467 -1.2339886 #>   5: -3.8124078 -1.5808545 -1.69392605 -0.7195614 -1.5326436 #>  ---                                                         #> 196: -0.1507502  0.3032338  0.07070621 -0.5115658 -0.5943514 #> 197: -3.5240129 -1.1180362 -0.79817788 -1.7354896  0.2559298 #> 198: -0.6150132 -0.4483046 -0.21201158  0.6259426  1.0133033 #> 199: -0.8411900 -1.2288842 -0.82844550  1.5638294  0.7094097 #> 200:  2.8645297  1.1586435  0.80932928  0.3835062 -0.4330287  # With different correlation task_high_cor = sim_dgp_correlated(200, r = 0.95) cor(task_high_cor$data()$x1, task_high_cor$data()$x2) #> [1] 0.9564631 task = sim_dgp_mediated(200) task$data() #>               y     direct    exposure   mediator       noise #>           <num>      <num>       <num>      <num>       <num> #>   1: -0.2558845 -0.5057433 -0.08422492 -0.2397263  0.09434266 #>   2:  1.0393519  0.6875956 -0.26377055  0.2807641  1.32043610 #>   3:  0.5308421  0.2445455 -0.09941249  0.2227038 -1.26496285 #>   4: -0.9744397  0.3688921 -0.95921452 -0.7233904 -1.38435569 #>   5:  2.1191797  1.6281251 -0.70571995  0.7087117  0.36034382 #>  ---                                                          #> 196: -1.3883831 -0.8353297 -0.75175012 -0.8588457  0.84785152 #> 197:  1.0555357  0.1279330  0.36881596  0.7659646 -1.93111685 #> 198: -1.5379470 -0.8819137  0.49303526 -0.6153501 -0.30007875 #> 199:  0.6150672 -0.2541678  0.28897104  0.3669280 -1.99027409 #> 200: -3.4979095 -2.5336223 -0.11831229 -1.5071141 -0.78867698 # Hidden confounder scenario (traditional) task_hidden = sim_dgp_confounded(200, hidden = TRUE) task_hidden$feature_names  # proxy available but not confounder #> [1] \"independent\" \"proxy\"       \"x1\"           # Observable confounder scenario task_observed = sim_dgp_confounded(200, hidden = FALSE) task_observed$feature_names  # both confounder and proxy available #> [1] \"confounder\"  \"independent\" \"proxy\"       \"x1\"          task = sim_dgp_interactions(200) task$data() #>               y     noise1      noise2          x1         x2          x3 #>           <num>      <num>       <num>       <num>      <num>       <num> #>   1: -0.3971196 -1.8192357  0.79114256 -0.44424984  0.7581484 -0.43514143 #>   2: -0.6789936 -1.6299329 -0.52576664 -0.27084066  1.1168832 -0.99744643 #>   3:  0.5754177  0.1806157  0.83344835  0.04275333 -0.2912638 -0.02597943 #>   4: -1.4122238  1.1225485 -0.08880987 -0.61932878  0.3976816 -0.38121805 #>   5:  1.9419322 -0.1630032  3.28775219 -1.19920201 -0.5820011  0.32708470 #>  ---                                                                      #> 196:  1.1639690  0.3378486 -0.40120306 -0.05358271  1.1390647  0.62218729 #> 197:  1.0178608  0.5210962 -0.53933062 -0.54136614 -0.4919935  1.47962775 #> 198: -1.0102450 -1.1650768 -0.33386391 -0.76410271  0.9499500  0.31168673 #> 199: -2.3841177  1.2184696 -1.40755729  1.26971537 -0.7294755 -0.59292000 #> 200:  0.3209181  0.8152151 -0.20269233  0.05039309 -1.0028597  0.59789395 task = sim_dgp_independent(200) task$data() #>                y   important1  important2  important3 unimportant1 unimportant2 #>            <num>        <num>       <num>       <num>        <num>        <num> #>   1:  3.13523909  1.666734751 -0.38765171  0.99263220    0.2093885  -0.34725051 #>   2: -2.08894306 -1.127438412  0.56408760 -0.57866520    1.0783999   0.36043175 #>   3:  1.76698647  0.935045868  0.23149435 -0.92350142    1.2106659   0.95039878 #>   4: -0.03669799  0.002857508 -0.05402223 -0.28340443   -1.2764969  -0.40547383 #>   5: -0.87739922 -0.350128886 -1.27703343  1.95848462    0.4922211  -0.44497006 #>  ---                                                                            #> 196: -1.94600678 -0.184542802 -1.54249380 -0.30439462   -1.3168109  -0.08652575 #> 197: -1.78772004 -0.500364562 -0.17941744 -1.18814761    0.3482896  -0.76049254 #> 198: -0.14197559 -0.116753185 -0.21794212  0.81566808    1.8662782   1.38713285 #> 199: -0.64029723 -0.075530836 -0.28996242 -0.08903692    0.6890199   1.53675260 #> 200: -0.41772275 -0.270928522  0.44214285 -1.00297048   -1.4577478  -0.11013045"},{"path":"https://mlr-org.github.io/xplainfi/reference/wvim_design_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Feature Selection Design Matrix — wvim_design_matrix","title":"Create Feature Selection Design Matrix — wvim_design_matrix","text":"Creates logical design matrix leave-leave-feature evaluation. Used internally mlr3fselect evaluate feature subsets.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/wvim_design_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Feature Selection Design Matrix — wvim_design_matrix","text":"","code":"wvim_design_matrix(   all_features,   feature_names = all_features,   direction = c(\"leave-out\", \"leave-in\") )"},{"path":"https://mlr-org.github.io/xplainfi/reference/wvim_design_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Feature Selection Design Matrix — wvim_design_matrix","text":"all_features (character()) available feature names task. feature_names (character() | list character()) Features feature groups evaluate. Can vector individual features named list grouped features. Defaults all_features unspecified. direction (character(1)) Either \"leave-\" \"leave-\" (default). Controls features selected design matrix. \"leave-\" sets features interest FALSE, \"leave-\" analogously sets TRUE.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/wvim_design_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Feature Selection Design Matrix — wvim_design_matrix","text":"data.table logical columns feature all_features length(feature_names) rows, one entry feature_names","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/wvim_design_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Feature Selection Design Matrix — wvim_design_matrix","text":"","code":"task = mlr3::tsk(\"mtcars\")  # Individual features feature_names = task$feature_names[1:3] wvim_design_matrix(task$feature_names, feature_names, \"leave-in\") #>        am   carb    cyl   disp   drat   gear     hp   qsec     vs     wt #>    <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> #> 1:   TRUE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE #> 2:  FALSE   TRUE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE #> 3:  FALSE  FALSE   TRUE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE wvim_design_matrix(task$feature_names, feature_names, \"leave-out\") #>        am   carb    cyl   disp   drat   gear     hp   qsec     vs     wt #>    <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> #> 1:  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE #> 2:   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE #> 3:   TRUE   TRUE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE  # Feature groups feature_groups = list(   A = task$feature_names[1:2],   B = task$feature_names[3:5] ) wvim_design_matrix(task$feature_names, feature_groups, \"leave-out\") #>        am   carb    cyl   disp   drat   gear     hp   qsec     vs     wt #>    <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> <lgcl> #> 1:  FALSE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE #> 2:   TRUE   TRUE  FALSE  FALSE  FALSE   TRUE   TRUE   TRUE   TRUE   TRUE"},{"path":"https://mlr-org.github.io/xplainfi/reference/xplain_opt.html","id":null,"dir":"Reference","previous_headings":"","what":"xplainfi Package Options — xplain_opt","title":"xplainfi Package Options — xplain_opt","text":"Get set package-level options xplainfi.","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/xplain_opt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"xplainfi Package Options — xplain_opt","text":"","code":"xplain_opt(...)"},{"path":"https://mlr-org.github.io/xplainfi/reference/xplain_opt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"xplainfi Package Options — xplain_opt","text":"... Option names retrieve (character strings) options set (named arguments). get option: xplain_opt(\"verbose\") returns current value set option: xplain_opt(verbose = FALSE) sets value get options: xplain_opt() returns named list options","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/xplain_opt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"xplainfi Package Options — xplain_opt","text":"getting single option: option value (logical) getting multiple options: named list option values setting options: previous values (invisibly)","code":""},{"path":"https://mlr-org.github.io/xplainfi/reference/xplain_opt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"xplainfi Package Options — xplain_opt","text":"Options can set three ways (order precedence): Using xplain_opt(option_name = value) (recommended) Using options(\"xplain.option_name\" = value) Using environment variables XPLAIN_OPTION_NAME=value","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/xplain_opt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"xplainfi Package Options — xplain_opt","text":"","code":"# Get current value of an option xplain_opt(\"verbose\") #> [1] TRUE  # Get all options xplain_opt() #> $verbose #> [1] TRUE #>  #> $progress #> [1] FALSE #>  #> $sequential #> [1] FALSE #>  #> $debug #> [1] FALSE #>   # Set an option (returns previous value invisibly) old <- xplain_opt(verbose = FALSE) xplain_opt(\"verbose\")  # Now FALSE #> [1] FALSE  # Restore previous value xplain_opt(verbose = old$verbose)  # Temporary option change with withr if (requireNamespace(\"withr\", quietly = TRUE)) {   withr::with_options(     list(\"xplain.verbose\" = FALSE),     {       # Code here runs with verbose = FALSE       xplain_opt(\"verbose\")     }   ) } #> [1] FALSE"},{"path":"https://mlr-org.github.io/xplainfi/reference/xplainfi-package.html","id":null,"dir":"Reference","previous_headings":"","what":"xplainfi: Feature Importance Methods for Global Explanations — xplainfi-package","title":"xplainfi: Feature Importance Methods for Global Explanations — xplainfi-package","text":"Provides consistent interface common feature importance methods described Ewald et al. (2024) doi:10.1007/978-3-031-63797-1_22 , including permutation feature importance (PFI), conditional relative feature importance (CFI, RFI), leave one covariate (LOCO), Shapley additive global importance (SAGE), well feature sampling mechanisms support conditional importance methods.","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/reference/xplainfi-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"xplainfi: Feature Importance Methods for Global Explanations — xplainfi-package","text":"Maintainer: Lukas Burk cran@lukasburk.de (ORCID) [copyright holder]","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"new-features-1-1-0","dir":"Changelog","previous_headings":"","what":"New features","title":"xplainfi 1.1.0","text":"Requires provided Resampling instantiated consist single iteration, e.g. must 1 test set. rsmp_all_test(task) utility can used construct single-iteration Resampling object given Task observations alligned test set train set empty. likely refine API around future. Internally, ResampleResult constructed given learner, task, resampling arguments, consistent previous default performing resample() get trained learners resampling iteration.","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"inference-1-1-0","dir":"Changelog","previous_headings":"","what":"Inference","title":"xplainfi 1.1.0","text":"New ci_method = \"lei\" WVIM/LOCO: distribution-free inference based Lei et al. (2018), testing observation-wise loss differences. Defaults Wilcoxon signed-rank test median aggregation. Supports t-test, Fisher permutation, binomial (sign) tests. Requires decomposable measure ($obs_loss()). New p_adjust parameter $importance() multiplicity correction across ci_methods produce p-values (\"raw\", \"nadeau_bengio\", \"cpi\", \"lei\"). Accepts method stats::p.adjust.methods (e.g. \"holm\", \"bonferroni\", \"BH\"). Default \"none\". \"bonferroni\", confidence intervals also adjusted (alpha/k). methods, p-values adjusted sequential/adaptive procedures lack clean per-comparison alpha CI construction. Parametric ci_methods (\"raw\", \"nadeau_bengio\") return se, statistic, p.value, conf_lower, conf_upper columns. \"quantile\" method returns conf_lower conf_upper (se, statistic, p.value). Parametric ci_methods support alternative = \"greater\" (one-sided) alternative = \"two.sided\" (default) test H0: importance <= 0 vs H1: importance > 0, H0: importance = 0 vs H1: importance != 0, respectively. \"quantile\", alternative controls whether interval one-sided (\"greater\": finite lower bound, conf_upper = Inf) two-sided (bounds finite). Improved documentation CI methods FeatureImportanceMethod, explaining p-values confidence intervals calculated method. CFI documentation distinguishes CPI (knockoff-based inference, Watson & Wright 2021) cARFi (ARF-based inference, Blesch et al. 2025).","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"minor-user-facing-changes-1-1-0","dir":"Changelog","previous_headings":"","what":"Minor user-facing changes","title":"xplainfi 1.1.0","text":"PerturbationImportance methods (PFI, CFI, RFI): n_repeats now 30 LOCO WVIM: n_repeats now 30 well. Since refitting methods expensive perturbation-based methods, users decrease value runtime becomes impractical, now least package default longer n_repeats = 1, obviously small.","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"testing-improvements-1-1-0","dir":"Changelog","previous_headings":"","what":"Testing improvements","title":"xplainfi 1.1.0","text":"Replaced ranger rpart tests flexible learner unnecessary. Added omnibus expect_method_output() expectation validates three main outputs ($importance(), $scores(), $obs_loss()) computed method. Removed overly abstract test helper functions (test_basic_workflow, test_with_resampling, test_custom_sampler) inlined logic call sites better readability. Use ConditionalGaussianSampler instead ConditionalARFSampler tests don’t specifically test ARF functionality. Set explicit n_repeats values tests (1L functional, 5L plausibility).","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"xplainfi-100---initial-cran-release","dir":"Changelog","previous_headings":"","what":"xplainfi 1.0.0 - Initial CRAN release","title":"xplainfi 1.0.0 - Initial CRAN release","text":"CRAN release: 2026-01-30 major version bump largely mark occasion package now considered “released”.","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"minor-changes-1-0-0","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"xplainfi 1.0.0 - Initial CRAN release","text":"Removed fippy comparison article since comprehensive comparison now available xplainfi-benchmark. Clean various documentation issues metadata. Adjusted min_permutations default SAGE methods 10 rather 3, since previous value found lead spurious early stopping. Fix sim_dgp_ewald lading erroneous variances compared settings. Reduce runtime tests (mostly using less ARF mor Gaussian sampling) Remove KnockoffSequentialSampler seqknockoff package available CRAN R-universe. KnockoffSampler corresponding knockoff_fun = seqknockoff::knockoffs_seq still works.","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"xplainfi-021","dir":"Changelog","previous_headings":"","what":"xplainfi 0.2.1","title":"xplainfi 0.2.1","text":"Simplify sim_dgp_confounded, removing x2 doesn’t add anything interesting x1. Ensure integers preserved Gaussian samplers Fix compatibility mlr3 >= 1.3.0 due change way obs_loss() computed (see https://github.com/mlr-org/mlr3/pull/1411). Methods allowing measure unspecified falling back task_type-specific default measure","code":""},{"path":[]},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"importance-aggregation-and-confidence-intervals-0-2-0","dir":"Changelog","previous_headings":"User-facing API improvements","what":"Importance aggregation and confidence intervals","title":"xplainfi 0.2.0","text":"\"none\" (default): Simple aggregation without confidence intervals \"raw\": Uncorrected variance estimates (informative , CIs narrow) \"nadeau_bengio\": Variance correction Nadeau & Bengio (2003) recommended Molnar et al. (2023) \"quantile\": Empirical quantile-based confidence intervals \"cpi\": Conditional Predictive Impact perturbation methods (PFI/CFI/RFI), supporting t-, Wilcoxon-, Fisher-, binomial tests CPI now properly scoped PerturbationImportance methods (available WVIM/LOCO SAGE) $importance() gains standardize parameter normalize scores [-1, 1] range Moved $compute() avoid recomputing predictions/refits changing aggregation method","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"data-simulation-helpers-0-2-0","dir":"Changelog","previous_headings":"User-facing API improvements","what":"Data simulation helpers","title":"xplainfi 0.2.0","text":"sim_dgp_independent(): Baseline additive independent effects sim_dgp_correlated(): Highly correlated features (PFI fails, CFI succeeds) sim_dgp_mediated(): Mediation structure (total vs direct effects) sim_dgp_confounded(): Confounding structure sim_dgp_interactions(): Interaction effects features DGP illustrates specific methodological challenges importance methods","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"observation-wise-losses-and-predictions-0-2-0","dir":"Changelog","previous_headings":"User-facing API improvements","what":"Observation-wise losses and predictions","title":"xplainfi 0.2.0","text":"$obs_loss() computes observation-wise importance scores measure Measure$obs_loss() method $predictions field stores prediction objects analysis","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"grouped-feature-importance-0-2-0","dir":"Changelog","previous_headings":"User-facing API improvements","what":"Grouped feature importance","title":"xplainfi 0.2.0","text":"Example: groups = list(effects = c(\"x1\", \"x2\", \"x3\"), noise = c(\"noise1\", \"noise2\")) output, feature column contains group names instead individual features Allows measuring importance feature sets rather individual features","code":""},{"path":[]},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"wvim-williamsons-variable-importance-measure-0-2-0","dir":"Changelog","previous_headings":"Method-specific improvements","what":"WVIM (Williamson’s Variable Importance Measure)","title":"xplainfi 0.2.0","text":"Generalizes LOCO (Leave-One-Covariate-) LOCI (Leave-One-Covariate-) Implemented using mlr3fselect cleaner internals Parameter renamed: iters_refit → n_repeats consistency","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"perturbationimportance-pfi-cfi-rfi-0-2-0","dir":"Changelog","previous_headings":"Method-specific improvements","what":"PerturbationImportance (PFI, CFI, RFI)","title":"xplainfi 0.2.0","text":"Uses learner$predict_newdata_fast() faster predictions (requires mlr3 >= 1.1.0) Batches permutation iterations internally reduce sampler$sample() calls New batch_size parameter control memory usage large datasets Parallel execution via mirai future backends Set mirai::daemons() future::plan() Parallelizes across features within resampling iteration Parameter renamed: iters_perm → n_repeats consistency","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"feature-samplers-0-2-0","dir":"Changelog","previous_headings":"Method-specific improvements","what":"Feature Samplers","title":"xplainfi 0.2.0","text":"$sample(feature, row_ids): Samples stored task using row IDs $sample_newdata(feature, newdata): Samples external data PermutationSampler → MarginalPermutationSampler ARFSampler → ConditionalARFSampler GaussianConditionalSampler → ConditionalGaussianSampler KNNConditionalSampler → ConditionalKNNSampler CtreeConditionalSampler → ConditionalCtreeSampler Standardized parameter name: conditioning_set features condition MarginalSampler: Base class marginal sampling methods MarginalReferenceSampler: Samples complete rows reference data (SAGE) Convenience wrappers: KnockoffGaussianSampler, KnockoffSequentialSampler Supports row_ids-based sampling iters parameter multiple knockoff iterations Compatible CFI (RFI/SAGE)","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"sage-shapley-additive-global-importance-0-2-0","dir":"Changelog","previous_headings":"Method-specific improvements","what":"SAGE (Shapley Additive Global Importance)","title":"xplainfi 0.2.0","text":"Bug fix: ConditionalSAGE now properly uses conditional sampling (accidentally using marginal sampling) Performance improvements: Uses learner$predict_newdata_fast() faster predictions batch_size parameter controls memory usage large coalitions Convergence tracking (#29, #33): Enable early_stopping = TRUE Stops relative standard error falls se_threshold (default: 0.01) Requires least min_permutations (default: 3) Checks convergence every check_interval permutations (default: 1) $converged: Boolean indicating convergence reached $n_permutations_used: Actual permutations used (may less requested) $convergence_history: Per-feature importance SE permutations $plot_convergence(): Visualize convergence curves Convergence tracked first resampling iteration ","code":""},{"path":"https://mlr-org.github.io/xplainfi/news/index.html","id":"xplainfi-010","dir":"Changelog","previous_headings":"","what":"xplainfi 0.1.0","title":"xplainfi 0.1.0","text":"PFI CFI RFI (via arf-powered conditional sampling) SAGE (marginal conditional, latter via arf) LOCO LOCI Includes comparison reference implementation Python via fippy","code":""}]
