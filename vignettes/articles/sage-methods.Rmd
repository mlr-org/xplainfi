---
title: "Shapley Additive Global Importance (SAGE)"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
	collapse = TRUE,
	comment = "#>",
	fig.width = 8,
	fig.height = 6
)
set.seed(123)
options("xplain.progress" = interactive())
```

```{r setup}
library(xplainfi)
library(mlr3)
library(mlr3learners)
library(ggplot2)
```

## Introduction

Shapley Additive Global Importance (SAGE) is a feature importance method based on cooperative game theory that uses Shapley values to fairly distribute the total prediction performance among all features. Unlike permutation-based methods that measure the drop in performance when features are perturbed, SAGE measures how much each feature contributes to the model's overall performance by marginalizing (removing) features.

The key insight of SAGE is that it provides a complete decomposition of the model's performance: the sum of all SAGE values equals the difference between the model's performance and the performance when all features are marginalized.

**xplainfi** provides two implementations of SAGE:

- **MarginalSAGE**: Marginalizes features independently (standard SAGE)
- **ConditionalSAGE**: Marginalizes features conditionally using ARF sampling

## Demonstration with Correlated Features

To showcase the difference between Marginal and Conditional SAGE, we'll use the `sim_dgp_correlated()` function which creates a simple linear DGP with two correlated features.

**Model:**
$$(X_1, X_2)^T \sim \text{MVN}(0, \Sigma)$$

where $\Sigma$ is a 2×2 covariance matrix with 1 on the diagonal and correlation $r$ (default 0.5) on the off-diagonal.

$$X_3 \sim N(0,1), \quad X_4 \sim N(0,1)$$
$$Y = 2 \cdot X_1 + X_3 + \varepsilon$$

where $\varepsilon \sim N(0, 0.2^2)$.

**Key properties:**

- `x1` has a direct causal effect on y (β=2.0)
- `x2` is correlated with x1 (r = 0.5 from MVN) but has **no causal effect** on y
- `x3` is independent with a causal effect (β=1.0)
- `x4` is independent noise (β=0)

```{r data-setup}
set.seed(123)
task = sim_dgp_correlated(n = 1000, r = 0.5)

# Check the correlation structure
task_data = task$data()
correlation_matrix = cor(task_data[, c("x1", "x2", "x3", "x4")])
round(correlation_matrix, 3)
```

**Expected behavior:**

- **Marginal SAGE**: Should show high importance for both x1 and x2 due to their correlation, even though x2 has no causal effect
- **Conditional SAGE**: Should show high importance for x1 but near-zero importance for x2 (correctly identifying the spurious predictor)

Let's set up our learner and measure. We'll use a random forest and instantiate a resampling to ensure both methods see the same data:

```{r learner-setup}
learner = lrn("regr.ranger")
measure = msr("regr.mse")
resampling = rsmp("holdout")
resampling$instantiate(task)
```

## Marginal SAGE

Marginal SAGE marginalizes features independently by averaging predictions over a reference dataset. This is the standard SAGE implementation described in the original paper.

```{r marginal-sage}
# Create Marginal SAGE instance
marginal_sage = MarginalSAGE$new(
	task = task,
	learner = learner,
	measure = measure,
	resampling = resampling,
	n_permutations = 15L,
	n_samples = 100L
)

# Compute SAGE values
marginal_sage$compute(batch_size = 5000L)
```

Let's visualize the results:

```{r marginal-sage-plot}
#| echo: false
# Extract importance scores
marginal_results = marginal_sage$importance()
marginal_results$method = "Marginal SAGE"

# Create a factor with proper ordering
marginal_results$feature = factor(
	marginal_results$feature,
	levels = marginal_results$feature[order(marginal_results$importance, decreasing = TRUE)]
)

# Color features by type (causal vs noise)
marginal_results$feature_type = ifelse(
	marginal_results$feature %in% c("x1", "x3"),
	"Causal",
	"Noise"
)

# Create bar plot
ggplot(marginal_results, aes(x = feature, y = importance)) +
	geom_col(aes(fill = feature_type), alpha = 0.8) +
	scale_fill_manual(
		values = c("Causal" = "steelblue", "Noise" = "lightcoral"),
		name = "Feature type"
	) +
	labs(
		title = "Marginal SAGE Feature Importance",
		subtitle = "Correlated features: x1 and x2 are correlated (r = 0.9 from MVN)\nx2 has no effect on y",
		x = "Features",
		y = "SAGE Value"
	) +
	theme_minimal(base_size = 14)
```

We can also keep track of the SAGE value approximation across permutations:

```{r convergance-marginal}
marginal_sage$plot_convergence()
```

### Early Stopping Based on Convergence

SAGE supports early stopping to save computation time when the importance values have converged. By default, early stopping is enabled with `early_stopping = TRUE`. Convergence is detected by monitoring the standard error (SE) of the SAGE value estimates in the first resampling iteration.

**Convergence criterion**: SAGE normalizes the SE by the range of their values (max - min) to make convergence detection scale-invariant across different loss metrics. Convergence is detected when:

$$\max_j \left(\frac{SE_j}{\max_i(\text{SAGE}_i) - \min_i(\text{SAGE}_i)}\right) < \text{threshold}$$

The default threshold is `se_threshold = 0.01` (1%), meaning convergence occurs when the relative SE is below 1% of the importance range for all features.

You can customize convergence detection in `$compute()`:

```r
# More strict convergence (requires more permutations)
sage$compute(early_stopping = TRUE, se_threshold = 0.005, min_permutations = 5L)

# Disable early stopping to always run all permutations
sage$compute(early_stopping = FALSE)
```

After computation, you can check convergence status:

```r
marginal_sage$converged  # TRUE if converged early
marginal_sage$n_permutations_used  # Actual permutations used
```

If a resampling with multiple iterations (i.e., not holdout) is supplied, the value of `n_permutations_used` will be set as the value for `n_permutations` in all subsequent iterations to avoid some computational overhead.

## Conditional SAGE

Conditional SAGE uses conditional sampling (via ARF by default) to marginalize features while preserving dependencies between the remaining features. This can provide different insights, especially when features are correlated.

```{r conditional-sage}
# Create Conditional SAGE instance using a conditional sampler
sampler_gaussian = ConditionalGaussianSampler$new(task)

conditional_sage = ConditionalSAGE$new(
	task = task,
	learner = learner,
	measure = measure,
	resampling = resampling,
	n_permutations = 15L,
	n_samples = 100L,
	sampler = sampler_gaussian
)

# Compute SAGE values
conditional_sage$compute(batch_size = 5000L)
```

Let's visualize the conditional SAGE results:

```{r conditional-sage-plot}
#| echo: false

# Extract importance scores
conditional_results = conditional_sage$importance()
conditional_results$method = "Conditional SAGE"

# Create a factor with proper ordering
conditional_results$feature = factor(
	conditional_results$feature,
	levels = conditional_results$feature[order(conditional_results$importance, decreasing = TRUE)]
)

# Color features by type (causal vs noise)
conditional_results$feature_type = ifelse(
	conditional_results$feature %in% c("x1", "x3"),
	"Causal",
	"Noise"
)

# Create bar plot
ggplot(conditional_results, aes(x = feature, y = importance)) +
	geom_col(aes(fill = feature_type), alpha = 0.8) +
	scale_fill_manual(
		values = c("Causal" = "steelblue", "Noise" = "lightcoral"),
		name = "Feature type"
	) +
	labs(
		title = "Conditional SAGE Feature Importance",
		subtitle = "Conditional sampling should reduce importance of redundant correlated features",
		x = "Features",
		y = "SAGE Value"
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "bottom")
```


```{r convergance-conditional}
conditional_sage$plot_convergence()
```

## Comparison of Methods

Let's compare the two SAGE methods side by side:

```{r comparison}
#| echo: false

# Ensure both datasets have feature_type for consistency
marginal_results$feature_type = ifelse(
	marginal_results$feature %in% c("x1", "x3"),
	"Causal",
	"Noise"
)

# Combine results
combined_results = rbind(marginal_results, conditional_results)

# Create comparison plot
ggplot(combined_results, aes(x = feature, y = importance, fill = method)) +
	facet_wrap(~feature_type, scales = "free_x") +
	geom_col(position = "dodge", alpha = 0.8) +
	scale_fill_manual(values = c("Marginal SAGE" = "steelblue", "Conditional SAGE" = "darkgreen")) +
	labs(
		title = "Marginal vs Conditional SAGE Comparison",
		x = "Features",
		y = "SAGE Value",
		fill = "Method"
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "bottom")
```

### Interpretation

<!-- ((This explanation is wrong and doesn't match what we observe and needs to be corrected (and substantiated))) -->

<!-- The results demonstrate the difference between marginal and conditional SAGE:

1. **Marginal SAGE** treats each feature independently, so highly correlated features x1 and x2 both receive substantial importance scores reflecting their individual marginal contributions.

2. **Conditional SAGE** accounts for feature dependencies through conditional sampling. When marginalizing x1, it conditions on x2 (and vice versa), leading to lower importance scores for the correlated features since they provide redundant information.

3. **Independent feature x3** shows similar importance in both methods since it doesn't depend on other features.

4. **Noise feature x4** correctly receives near-zero importance in both methods.

This pattern mirrors the difference between PFI and CFI: marginal methods show inflated importance for correlated features, while conditional methods provide a more accurate assessment of each feature's unique contribution. -->

## Comparison with PFI and CFI

For reference, let's compare SAGE methods with the analogous PFI and CFI methods on the same data:

```{r pfi-cfi-comparison}
# Quick PFI and CFI comparison for context
pfi = PFI$new(task, learner, measure)
cfi = CFI$new(task, learner, measure, sampler = sampler_gaussian)

pfi$compute()
cfi$compute()
pfi_results = pfi$importance()
cfi_results = cfi$importance()

# Create comparison data frame
method_comparison = data.frame(
	feature = rep(c("x1", "x2", "x3", "x4"), 4),
	importance = c(
		pfi_results$importance,
		cfi_results$importance,
		marginal_results$importance,
		conditional_results$importance
	),
	method = rep(c("PFI", "CFI", "Marginal SAGE", "Conditional SAGE"), each = 4),
	approach = rep(c("Marginal", "Conditional", "Marginal", "Conditional"), each = 4)
)

# Create comparison plot
ggplot(method_comparison, aes(x = feature, y = importance, fill = method)) +
	geom_col(position = "dodge", alpha = 0.8) +
	scale_fill_manual(
		values = c(
			"PFI" = "lightblue",
			"CFI" = "blue",
			"Marginal SAGE" = "lightcoral",
			"Conditional SAGE" = "darkred"
		)
	) +
	labs(
		title = "Comparison: PFI/CFI vs Marginal/Conditional SAGE",
		subtitle = "Both pairs show similar patterns: marginal methods inflate correlated feature importance",
		x = "Features",
		y = "Importance Value",
		fill = "Method"
	) +
	theme_minimal(base_size = 14) +
	theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


<!-- ((This similarly needs to be corrected))

**Key Observations:**

- **Marginal methods** (PFI, Marginal SAGE) both assign high importance to correlated features x1 and x2
- **Conditional methods** (CFI, Conditional SAGE) both reduce importance for correlated features, accounting for redundancy
- **Independent feature x3** receives consistent importance across all methods
- **Noise feature x4** is correctly identified as unimportant by all methods

This demonstrates that the marginal vs conditional distinction is a fundamental concept that applies across different importance method families. -->
