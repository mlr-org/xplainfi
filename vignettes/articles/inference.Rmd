---
title: "Inference for Feature Importance"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Inference for Feature Importance}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
	collapse = TRUE,
	comment = "#>",
	fig.width = 8,
	fig.height = 6
)
set.seed(123)
# Quiet down
lgr::get_logger("mlr3")$set_threshold("warn")
options("xplain.progress" = interactive())
```

```{r pkg}
library(xplainfi)
library(mlr3learners)
library(data.table)
library(ggplot2)
```

There are multiple (work in progress) inference methods available with the underlying implementation, but the API around them is still being worked out.

## Setup

We use a simple linear DGP for demonstration purposes where

- $X_1$ and $X_2$ are strongly correlated (r = 0.7)
- $X_1$ and $X_3$ have an effect on Y
- $X_2$ and $X_4$ don't have an effect 

```{r setup-data}
task = sim_dgp_correlated(n = 2000, r = 0.7)
learner = lrn("regr.ranger", num.trees = 500)
measure = msr("regr.mse")
```

```{r dag-correlated, echo=FALSE, fig.cap="DAG for correlated features DGP", fig.width=10, fig.height=4}
#| fig.alt: "Directed acyclic graph with four features X1-X4 and outcome Y. Bidirectional edge between X1 and X2. Arrows from X1 and X3 to Y."
DiagrammeR::grViz(
	"
  digraph Correlated {
    rankdir=LR;
    graph [ranksep=1.5];
    node [shape=circle, style=filled, fontsize=14, width=1.2];
    
    X1 [fillcolor='lightcoral', label='X₁\n(β=2.0)'];
    X2 [fillcolor='pink', label='X₂\n(β=0)'];
    X3 [fillcolor='lightblue', label='X₃\n(β=1.0)'];
    X4 [fillcolor='lightgray', label='X₄\n(β=0)'];
    Y [fillcolor='greenyellow', label='Y', width=1.5];
    
    X1 -> X2 [color=red, style=bold, dir=both, label='r = 0.7'];
    X1 -> Y [label='2.0'];
    X2 -> Y [style=dashed, color=gray, label='0'];
    X3 -> Y [label='1.0'];
    X4 -> Y [style=dashed, color=gray];
    
    {rank=source; X1; X3; X4}
    {rank=same; X2}
    {rank=sink; Y}
  }"
)
```

## Corrected t-test (Nadeau & Bengio)

When we calculate PFI using an appropriate resampling, such as subsampling with 15 repeats, we can use the approach recommended by Molnar et al. (2023) based on the proposed correction by Nadeau & Bengio (2003).

By default, any importance measures' `$importance()` method will not output any variances or confidence intervals, it will merely compute averages over resampling iterations and repeats within resamplings (`iter_repeat` here).

```{r pfi-importance}
pfi = PFI$new(
	task = task,
	learner = learner,
	resampling = rsmp("subsampling", repeats = 15),
	measure = measure,
	n_repeats = 20 # for stability within resampling iters
)

pfi$compute()
pfi$importance()
```

If we want **unadjusted** confidence intervals based on a t-distribution, we can ask for them, but note these are too narrow / optimistic and hence invalid for inference:

```{r pfi-ci-raw}
pfi_ci_raw = pfi$importance(ci_method = "raw", alternative = "two.sided")
pfi_ci_raw
```

The parametric CI methods (`"raw"` and `"nadeau_bengio"`) return `se`, `statistic`, `p.value`, `conf_lower`, and `conf_upper`.
The `alternative` parameter controls whether a one-sided test (`"greater"`, the default, testing H0: importance <= 0) or two-sided test (`"two.sided"`) is performed.
Here we use `alternative = "two.sided"` for visualization purposes so that `conf_upper` is finite.

Analogously we can retrieve the **Nadeau & Bengio**-adjusted standard errors and derived confidence intervals which were demonstrated to have better (but still imperfect) coverage:

```{r pfi-ci-nadeau-bengio}
pfi_ci_corrected = pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided")
pfi_ci_corrected
```

## Empirical quantiles

Both `"raw"` and `"nadeau_bengio"` methods assume normally distributed importance scores and use parametric confidence intervals based on the t-distribution. As an alternative, we can use empirical quantiles to construct confidence-like intervals without parametric assumptions.

The `"quantile"` method returns only confidence bounds (`conf_lower`, `conf_upper`) without `se`, `statistic`, or `p.value`, as empirical quantiles are not statistical tests and even `conf_*` implies a mild misnomer, but the variable name is just kept for consistency with other methods to ease visualization.

```{r pfi-ci-quantile}
pfi_ci_quantile = pfi$importance(ci_method = "quantile", alternative = "two.sided")
pfi_ci_quantile
```

To highlight the differences between parametric and empirical approaches, we visualize all methods:

```{r pfi-ci-comparison}
#| fig.alt: "Point-and-whisker plot with features on y-axis and importance on x-axis. Three colored points for raw, nadeau_bengio, and quantile CI methods, each showing point estimates with horizontal error bars."
pfi_cis = rbindlist(
	list(
		pfi_ci_raw[, type := "raw"],
		pfi_ci_corrected[, type := "nadeau_bengio"],
		pfi_ci_quantile[, type := "quantile"]
	),
	fill = TRUE
)

ggplot(pfi_cis, aes(y = feature, color = type)) +
	geom_errorbar(
		aes(xmin = conf_lower, xmax = conf_upper),
		position = position_dodge(width = 0.6),
		width = .5
	) +
	geom_point(aes(x = importance), position = position_dodge(width = 0.6)) +
	scale_color_brewer(palette = "Set2") +
	labs(
		title = "Parametric & non-parametric CI methods",
		subtitle = "RF with 15 subsampling iterations",
		color = NULL
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "bottom")
```

The results highlight just how optimistic the unadjusted, raw confidence intervals are.

## Multiplicity correction

When testing many features simultaneously, p-values should be adjusted to control the overall error rate. The `p_adjust` parameter accepts any method from `stats::p.adjust.methods` and defaults to `"none"` (no adjustment).

```{r p-adjust-pfi}
pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided", p_adjust = "none")
```

With Bonferroni correction, both p-values and confidence intervals are adjusted. The confidence intervals use an adjusted significance level of $\alpha / k$ where $k$ is the number of features, ensuring family-wise error rate control:

```{r p-adjust-bonferroni}
pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided", p_adjust = "bonferroni")
```

For other adjustment methods such as Holm or Benjamini-Hochberg, only p-values are adjusted. These sequential or adaptive procedures do not yield a clean per-comparison $\alpha$ that could be used for CI construction, so confidence intervals remain at the nominal level:

```{r p-adjust-bh}
pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided", p_adjust = "BH")
```

This applies to all `ci_method`s that produce p-values (`"raw"`, `"nadeau_bengio"`, `"cpi"`, `"lei"`).


## Conditional predictive impact (CPI)

CPI (Conditional Predictive Impact) was introduced by Watson & Wright (2021) for statistical inference with conditional feature importance using knockoffs. Two main approaches are supported:

- **CPI with knockoffs**: The original method using model-X knockoffs for conditional sampling.
- **cARFi** (Blesch et al., 2025): Uses Adversarial Random Forests for conditional sampling, which works without Gaussian assumptions and supports mixed data.
CPI is also implemented by [the cpi package](https://bips-hb.github.io/cpi/articles/intro.html).
It works with `mlr3` and its output on our data looks like this:

```{r cpi-setup}
library(cpi)

resampling = rsmp("cv", folds = 5)
resampling$instantiate(task)
```

```{r cpi-result}
cpi_res = cpi(
	task = task,
	learner = learner,
	resampling = resampling,
	measure = measure,
	test = "t"
)
setDT(cpi_res)
setnames(cpi_res, "Variable", "feature")
cpi_res[, method := "CPI"]

cpi_res
```

### CPI with knockoffs

Since `xplainfi` also includes knockoffs via the `KnockoffSampler` and the `KnockoffGaussianSampler`, the latter implementing the second order Gaussian knockoffs also used by default in `{cpi}`, we can recreate its results using `CFI` with the corresponding `sampler`.

`CFI` with a knockoff sampler supports CPI inference directly via `ci_method = "cpi"`.

```{r cfi-knockoff}
knockoff_gaussian = KnockoffGaussianSampler$new(task)

cfi = CFI$new(
	task = task,
	learner = learner,
	resampling = resampling,
	measure = measure,
	sampler = knockoff_gaussian,
	n_repeats = 1 # generate 1 knockoff matrix, like cpi()
)

cfi$compute()

cfi_cpi_res = cfi$importance(ci_method = "cpi")
cfi_cpi_res

# Rename columns to match cpi package output for comparison
setnames(cfi_cpi_res, c("importance", "conf_lower"), c("CPI", "ci.lo"))
cfi_cpi_res[, method := "CFI+Knockoffs"]
```

The results should be very similar to those computed by `cpi()`, so let's compare them:

```{r cpi-cfi-plot}
#| fig.alt: "Point-and-whisker plot with features on y-axis and CPI values on x-axis. Two colored points for CPI and CFI+Knockoffs methods with horizontal error bars."
rbindlist(list(cpi_res, cfi_cpi_res), fill = TRUE) |>
	ggplot(aes(y = feature, x = CPI, color = method)) +
	geom_point(position = position_dodge(width = 0.3)) +
	geom_errorbar(
		aes(xmin = CPI, xmax = ci.lo),
		position = position_dodge(width = 0.3),
		width = 0.5
	) +
	scale_color_brewer(palette = "Dark2") +
	labs(
		title = "CPI and CFI with Knockoff sampler",
		subtitle = "RF with 5-fold CV",
		color = NULL
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "top")

```

A noteable caveat of the knockoff approach is that they are not readily available for mixed data (with categorical features).

### cARFi: CPI with ARF

An alternative is available using ARF as conditional sampler rather than knockoffs. This approach, called cARFi, was introduced by [Blesch et al. (2025)](https://doi.org/10.1609/aaai.v39i15.33712) and works without Gaussian assumptions:

```{r cfi-arf}
arf_sampler = ConditionalARFSampler$new(
	task = task,
	finite_bounds = "local",
	min_node_size = 20,
	epsilon = 1e-15
)

cfi_arf = CFI$new(
	task = task,
	learner = learner,
	resampling = resampling,
	measure = measure,
	sampler = arf_sampler
)

cfi_arf$compute()

# CPI inference with ARF sampler (cARFi)
cfi_arf_res = cfi_arf$importance(ci_method = "cpi")
cfi_arf_res

# Rename columns to match cpi package output for comparison
setnames(cfi_arf_res, c("importance", "conf_lower"), c("CPI", "ci.lo"))
cfi_arf_res[, method := "CFI+ARF"]
```

We can now compare all three methods:

```{r cpi-cfi-arf-plot}
#| fig.alt: "Point-and-whisker plot with features on y-axis and CPI values on x-axis. Three colored points for CPI, CFI+Knockoffs, and CFI+ARF methods with horizontal error bars."
rbindlist(list(cpi_res, cfi_cpi_res, cfi_arf_res), fill = TRUE) |>
	ggplot(aes(y = feature, x = CPI, color = method)) +
	geom_point(position = position_dodge(width = 0.3)) +
	geom_errorbar(
		aes(xmin = CPI, xmax = ci.lo),
		position = position_dodge(width = 0.3),
		width = 0.5
	) +
	scale_color_brewer(palette = "Dark2") +
	labs(
		title = "CPI and CFI with Knockoffs and ARF",
		subtitle = "RF with 5-fold CV",
		color = NULL
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "top")
```

As expected, the ARF-based approach differs more from both knockoff-based approaches, but they are all roughly in agreement.

### Statistical tests with CPI

CPI can also perform additional tests besides the default t-test, specifically the Wilcoxon-, Fisher-, or binomial test:

```{r cpi-tests}
#| fig.alt: "Point-and-whisker plot with features on y-axis and importance on x-axis. Four colored series for t, Wilcoxon, Fisher, and Binomial tests with horizontal error bars."
(cpi_res_wilcoxon = cfi_arf$importance(ci_method = "cpi", test = "wilcoxon"))
# Fisher test with same default for B as in cpi()
(cpi_res_fisher = cfi_arf$importance(ci_method = "cpi", test = "fisher", B = 1999))
(cpi_res_binom = cfi_arf$importance(ci_method = "cpi", test = "binomial"))

rbindlist(
	list(
		cfi_arf$importance(ci_method = "cpi")[, test := "t"],
		cpi_res_wilcoxon[, test := "Wilcoxon"],
		cpi_res_fisher[, test := "Fisher"],
		cpi_res_binom[, test := "Binomial"]
	),
	fill = TRUE
) |>
	ggplot(aes(y = feature, x = importance, color = test)) +
	geom_point(position = position_dodge(width = 0.3)) +
	geom_errorbar(
		aes(xmin = importance, xmax = conf_lower),
		position = position_dodge(width = 0.3),
		width = 0.5
	) +
	scale_color_brewer(palette = "Dark2") +
	labs(
		title = "CPI test with CFI/ARF",
		subtitle = "RF with 5-fold CV",
		color = "Test"
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "top")
```

The choice of test depends on distributional assumptions: the t-test assumes normality, while Fisher and Wilcoxon are non-parametric alternatives.

Following Watson & Wright (2021), we can additionaly apply Benjamini-Hochberg FDR control (`p_adjust = "BH"`), which is often desirable in exploratory settings where many features are tested simultaneously:


```{r cpi-fdr-control}
cfi_arf$importance(ci_method = "cpi", p_adjust = "BH")
```

Note that multiplcity adjustment is in the general case limited to p-values, and only the general Bonferroni method is easily applicable to confidence intervals.


## Distribution-free inference for LOCO (Lei et al., 2018)

[Lei et al. (2018)](https://doi.org/10.1080/01621459.2017.1307116) proposed distribution-free inference for LOCO importance based on observation-wise loss differences.
The idea is to test whether the excess test error from removing feature $j$ is significantly greater than zero:

$$
\theta_j = \mathrm{med}\left(
	|Y - \hat{f}_{n_1}^{-j}(X)| - |Y - \hat{f}_{n_1}(X)| \big| D_1
\right)
$$

The paper proposes:

- **L1 (absolute) loss** as the measure
- **Median** as the aggregation function
- A **test set** with unique new observations (e.g., holdout set or cross-validation)
- A **Wilcoxon signed-rank test** (or sign test) for inference
- **Bonferroni correction** for multiple comparisons

This is available in `xplainfi` via `ci_method = "lei"`:

```{r loco-lei}
# mae has L1 loss on observation-level, the "mean" aggregation is ignored here
measure_mae = msr("regr.mae")

loco = LOCO$new(
	task = sim_dgp_correlated(n = 2000, r = 0.7),
	learner = lrn("regr.ranger", num.trees = 500),
	resampling = rsmp("holdout"),
	measure = measure_mae
)

loco$compute()
loco$importance(
	ci_method = "lei",
	alternative = "two.sided",
	p_adjust = "bonferroni",
	aggregator = median # default spelled out explicitly
)
```

The `ci_method = "lei"` method works on observation-wise loss differences internally, using the median as the default point estimate and the Wilcoxon signed-rank test for confidence intervals and p-values.

### Configurable parameters

All components proposed by Lei et al. are the defaults but can be customized:

- `test`: `"wilcoxon"` (default), `"t"`, `"fisher"`, or `"binomial"` (sign test)
- `aggregator`: defaults to `stats::median`, can be changed (e.g. `mean`)
- `p_adjust`: p-value correction for multiple comparisons, accepts any method from `stats::p.adjust.methods` (e.g. `"bonferroni"`, `"holm"`, `"BH"`, `"none"`). Default is `"none"`. When `"bonferroni"`, confidence intervals are also adjusted (alpha/k). For other methods like `"holm"` or `"BH"`, only p-values are adjusted because these sequential/adaptive procedures do not yield a clean per-comparison alpha for CI construction.

```{r loco-lei-ttest}
loco$importance(
	ci_method = "lei",
	test = "t",
	aggregator = mean,
	alternative = "two.sided",
	p_adjust = "holm"
)
```

<!-- 
### Why `ci_method = "lei"` and not just MAE + median?

One subtlety is that the standard `$importance()` pipeline (with `ci_method = "none"` or `ci_method = "raw"`) computes the aggregation function per resampling iteration first, then takes the difference:

$$
\mathrm{med}\left(|Y - \hat{f}^{-j}(X)|\right) - \mathrm{med}\left(|Y - \hat{f}(X)|\right)
$$

This is *not* what the paper proposes. Lei et al. take the difference first, then aggregate:

$$
\mathrm{med}\left(|Y - \hat{f}^{-j}(X)| - |Y - \hat{f}(X)|\right)
$$

For the arithmetic mean these are equivalent, but for the median they are not.
The `ci_method = "lei"` implementation handles this correctly by operating on observation-wise loss differences directly. 
-->
