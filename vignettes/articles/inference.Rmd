---
title: "Inference for Feature Importance"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Inference for Feature Importance}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
	collapse = TRUE,
	comment = "#>",
	fig.width = 8,
	fig.height = 6
)
set.seed(123)
# Quiet down
lgr::get_logger("mlr3")$set_threshold("warn")
options("xplain.progress" = interactive())
```

```{r pkg}
library(xplainfi)
library(mlr3learners)
library(data.table)
library(ggplot2)
```

There are multiple inference methods available for different importance methods and estimation targets.
The approaches fall into two broad categories:

1. **Resampling-based variability**: When importance is estimated via resampling (e.g., subsampling), we obtain multiple importance estimates across iterations. We can summarize this variability descriptively (empirical quantiles), or use parametric methods, either naive (raw CIs) or corrected for the dependence structure of resampling (Nadeau & Bengio).

2. **Observation-wise inference on test data**: When a dedicated test set with i.i.d. observations is available (e.g., holdout), we can use observation-wise loss differences for formal statistical inference. This is the basis for CPI/cARFi (conditional importance) and the Lei et al. method (LOCO inference).

## Setup

We use a simple linear DGP for demonstration purposes where

- $X_1$ and $X_2$ are strongly correlated (r = 0.7)
- $X_1$ and $X_3$ have an effect on Y
- $X_2$ and $X_4$ don't have an effect

```{r setup-data}
task = sim_dgp_correlated(n = 2000, r = 0.7)
learner = lrn("regr.ranger", num.trees = 500)
measure = msr("regr.mse")
```

```{r dag-correlated, echo=FALSE, fig.cap="DAG for correlated features DGP", fig.width=10, fig.height=4}
#| fig.alt: "Directed acyclic graph with four features X1-X4 and outcome Y. Bidirectional edge between X1 and X2. Arrows from X1 and X3 to Y."
DiagrammeR::grViz(
	"
  digraph Correlated {
    rankdir=LR;
    graph [ranksep=1.5];
    node [shape=circle, style=filled, fontsize=14, width=1.2];

    X1 [fillcolor='lightcoral', label='X₁\n(β=2.0)'];
    X2 [fillcolor='pink', label='X₂\n(β=0)'];
    X3 [fillcolor='lightblue', label='X₃\n(β=1.0)'];
    X4 [fillcolor='lightgray', label='X₄\n(β=0)'];
    Y [fillcolor='greenyellow', label='Y', width=1.5];

    X1 -> X2 [color=red, style=bold, dir=both, label='r = 0.7'];
    X1 -> Y [label='2.0'];
    X2 -> Y [style=dashed, color=gray, label='0'];
    X3 -> Y [label='1.0'];
    X4 -> Y [style=dashed, color=gray];

    {rank=source; X1; X3; X4}
    {rank=same; X2}
    {rank=sink; Y}
  }"
)
```

## Resampling-based variability

When we compute feature importance with resampling — for example, subsampling with multiple repeats — each resampling iteration yields a separate importance estimate.
By default, `$importance()` simply averages these estimates without reporting any measure of variability:

```{r pfi-importance}
pfi = PFI$new(
	task = task,
	learner = learner,
	resampling = rsmp("subsampling", repeats = 15),
	measure = measure,
	n_repeats = 20 # for stability within resampling iters
)

pfi$compute()
pfi$importance()
```

There are several ways to quantify the variability of these estimates, ranging from purely descriptive to parametric inference.

### Empirical quantiles

The simplest approach is to look at the empirical distribution of importance scores across resampling iterations.
This is not a formal inference method — it merely describes the observed variability without any distributional assumptions.

The `"quantile"` method returns only confidence bounds (`conf_lower`, `conf_upper`) without `se`, `statistic`, or `p.value`, since empirical quantiles are not a statistical test. The `conf_*` naming is kept for consistency with other methods to ease visualization.

```{r pfi-ci-quantile}
pfi_ci_quantile = pfi$importance(ci_method = "quantile", alternative = "two.sided")
pfi_ci_quantile
```

### Raw confidence intervals

A natural next step is to assume the importance scores are approximately normally distributed across resampling iterations and compute t-based confidence intervals.
However, these **unadjusted** confidence intervals are too narrow and hence **invalid for inference**: the resampling iterations share overlapping training sets, violating the independence assumption underlying the t-distribution.

```{r pfi-ci-raw}
pfi_ci_raw = pfi$importance(ci_method = "raw", alternative = "two.sided")
pfi_ci_raw
```

The parametric CI methods (`"raw"` and `"nadeau_bengio"`) return `se`, `statistic`, `p.value`, `conf_lower`, and `conf_upper`.
The `alternative` parameter controls whether a one-sided test (`"greater"`, the default, testing H0: importance <= 0) or two-sided test (`"two.sided"`) is performed.
Here we use `alternative = "two.sided"` for visualization purposes so that `conf_upper` is finite.

### Corrected t-test (Nadeau & Bengio)

To account for the dependence between resampling iterations, we can use the correction proposed by [Nadeau & Bengio (2003)](https://doi.org/10.1023/A:1024068626366) and recommended by [Molnar et al. (2023)](https://doi.org/10.1007/s10618-023-00925-z).
This inflates the variance estimate to account for the overlap between training sets, yielding wider (and more honest) confidence intervals:

```{r pfi-ci-nadeau-bengio}
pfi_ci_corrected = pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided")
pfi_ci_corrected
```

The Nadeau-Bengio correction provides better (but still imperfect) coverage compared to the raw approach.

### Comparison

To highlight the differences between all three approaches, we visualize them side by side:

```{r pfi-ci-comparison}
#| fig.alt: "Point-and-whisker plot with features on y-axis and importance on x-axis. Three colored points for raw, nadeau_bengio, and quantile CI methods, each showing point estimates with horizontal error bars."
pfi_cis = rbindlist(
	list(
		pfi_ci_raw[, type := "raw"],
		pfi_ci_corrected[, type := "nadeau_bengio"],
		pfi_ci_quantile[, type := "quantile"]
	),
	fill = TRUE
)

ggplot(pfi_cis, aes(y = feature, color = type)) +
	geom_errorbar(
		aes(xmin = conf_lower, xmax = conf_upper),
		position = position_dodge(width = 0.6),
		width = .5
	) +
	geom_point(aes(x = importance), position = position_dodge(width = 0.6)) +
	scale_color_brewer(palette = "Set2") +
	labs(
		title = "Parametric & non-parametric CI methods",
		subtitle = "RF with 15 subsampling iterations",
		color = NULL
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "bottom")
```

The results highlight just how optimistic the unadjusted, raw confidence intervals are.

### Multiplicity correction

When testing many features simultaneously, p-values should be adjusted to control the overall error rate.
There are two main frameworks for multiplicity correction:

- **Family-wise error rate (FWER)**: The probability of making *at least one* false rejection among all tests. FWER control is the more conservative approach and is appropriate when any single false positive would be costly. Methods include Bonferroni ($\alpha / k$) and Holm (a step-down variant that is uniformly more powerful than Bonferroni while still controlling FWER).

- **False discovery rate (FDR)**: The expected *proportion* of false rejections among all rejected hypotheses. FDR control is less conservative and better suited for exploratory analyses where some false positives are tolerable. The most common method is Benjamini-Hochberg (BH).

The `p_adjust` parameter accepts any method from `stats::p.adjust.methods` and defaults to `"none"` (no adjustment).

```{r p-adjust-pfi}
pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided", p_adjust = "none")
```

With Bonferroni correction (FWER control), both p-values and confidence intervals are adjusted. The confidence intervals use an adjusted significance level of $\alpha / k$ where $k$ is the number of features:

```{r p-adjust-bonferroni}
pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided", p_adjust = "bonferroni")
```

For sequential or adaptive procedures such as Holm (FWER) or Benjamini-Hochberg (FDR), only p-values are adjusted. These methods do not yield a clean per-comparison $\alpha$ that could be used for CI construction, so confidence intervals remain at the nominal level:

```{r p-adjust-bh}
pfi$importance(ci_method = "nadeau_bengio", alternative = "two.sided", p_adjust = "BH")
```

This applies to all `ci_method`s that produce p-values (`"raw"`, `"nadeau_bengio"`, `"cpi"`, `"lei"`).


## Observation-wise inference on test data

The resampling-based approaches above quantify variability due to the choice of train/test split.
A different approach uses **observation-wise loss differences** on a test set for formal statistical inference.

The idea is straightforward: for each test observation, we compare the loss with and without a feature (either by perturbing the feature or by retraining the model without it).
The resulting per-observation importance scores are i.i.d. under appropriate conditions, enabling standard statistical tests.

Inference is guaranteed to be valid with a single train/test split (holdout), where test observations are truly i.i.d. and the model is fixed.
Other resampling strategies may be used but come with caveats (see below).

- **CPI / cARFi** for conditional feature importance (perturbation-based, model is fixed per resampling iteration)
- **Lei et al.** for LOCO importance (retraining-based, model is refitted without each feature)


### Conditional predictive impact (CPI)

CPI (Conditional Predictive Impact) was introduced by [Watson & Wright (2021)](https://doi.org/10.1007/s10994-021-06030-6) for statistical inference with conditional feature importance using knockoffs. Two main approaches are supported:

- **CPI with knockoffs**: The original method using model-X knockoffs for conditional sampling.
- **cARFi** (Blesch et al., 2025): Uses Adversarial Random Forests for conditional sampling, which works without Gaussian assumptions and supports mixed data.
CPI is originally implemented by [the cpi package](https://bips-hb.github.io/cpi/articles/intro.html).
It works with `mlr3` and its output on our data looks like this:

```{r cpi-setup}
library(cpi)

resampling = rsmp("cv", folds = 5)
resampling$instantiate(task)
```

```{r cpi-result}
cpi_res = cpi(
	task = task,
	learner = learner,
	resampling = resampling,
	measure = measure,
	test = "t"
)
setDT(cpi_res)
setnames(cpi_res, "Variable", "feature")
cpi_res[, method := "CPI"]

cpi_res
```

#### CPI with knockoffs

Since `xplainfi` also includes knockoffs via the `KnockoffSampler` and the `KnockoffGaussianSampler`, the latter implementing the second order Gaussian knockoffs also used by default in `{cpi}`, we can recreate its results using `CFI` with the corresponding `sampler`.

`CFI` with a knockoff sampler supports CPI inference directly via `ci_method = "cpi"`.

```{r cfi-knockoff}
knockoff_gaussian = KnockoffGaussianSampler$new(task)

cfi = CFI$new(
	task = task,
	learner = learner,
	resampling = resampling,
	measure = measure,
	sampler = knockoff_gaussian,
	n_repeats = 1 # generate 1 knockoff matrix, like cpi()
)

cfi$compute()

cfi_cpi_res = cfi$importance(ci_method = "cpi")
cfi_cpi_res

# Rename columns to match cpi package output for comparison
setnames(cfi_cpi_res, c("importance", "conf_lower"), c("CPI", "ci.lo"))
cfi_cpi_res[, method := "CFI+Knockoffs"]
```

The results should be very similar to those computed by `cpi()`, so let's compare them:

```{r cpi-cfi-plot}
#| fig.alt: "Point-and-whisker plot with features on y-axis and CPI values on x-axis. Two colored points for CPI and CFI+Knockoffs methods with horizontal error bars."
rbindlist(list(cpi_res, cfi_cpi_res), fill = TRUE) |>
	ggplot(aes(y = feature, x = CPI, color = method)) +
	geom_point(position = position_dodge(width = 0.3)) +
	geom_errorbar(
		aes(xmin = CPI, xmax = ci.lo),
		position = position_dodge(width = 0.3),
		width = 0.5
	) +
	scale_color_brewer(palette = "Dark2") +
	labs(
		title = "CPI and CFI with Knockoff sampler",
		subtitle = "RF with 5-fold CV",
		color = NULL
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "top")

```

A noteable caveat of the knockoff approach is that they are not readily available for mixed data (with categorical features).

#### cARFi: CPI with ARF

An alternative is available using ARF as conditional sampler rather than knockoffs. This approach, called cARFi, was introduced by [Blesch et al. (2025)](https://doi.org/10.1609/aaai.v39i15.33712) and works without Gaussian assumptions:

```{r cfi-arf}
arf_sampler = ConditionalARFSampler$new(
	task = task,
	finite_bounds = "local",
	min_node_size = 20,
	epsilon = 1e-15
)

cfi_arf = CFI$new(
	task = task,
	learner = learner,
	resampling = resampling,
	measure = measure,
	sampler = arf_sampler
)

cfi_arf$compute()

# CPI inference with ARF sampler (cARFi)
cfi_arf_res = cfi_arf$importance(ci_method = "cpi")
cfi_arf_res

# Rename columns to match cpi package output for comparison
setnames(cfi_arf_res, c("importance", "conf_lower"), c("CPI", "ci.lo"))
cfi_arf_res[, method := "CFI+ARF"]
```

We can now compare all three methods:

```{r cpi-cfi-arf-plot}
#| fig.alt: "Point-and-whisker plot with features on y-axis and CPI values on x-axis. Three colored points for CPI, CFI+Knockoffs, and CFI+ARF methods with horizontal error bars."
rbindlist(list(cpi_res, cfi_cpi_res, cfi_arf_res), fill = TRUE) |>
	ggplot(aes(y = feature, x = CPI, color = method)) +
	geom_point(position = position_dodge(width = 0.3)) +
	geom_errorbar(
		aes(xmin = CPI, xmax = ci.lo),
		position = position_dodge(width = 0.3),
		width = 0.5
	) +
	scale_color_brewer(palette = "Dark2") +
	labs(
		title = "CPI and CFI with Knockoffs and ARF",
		subtitle = "RF with 5-fold CV",
		color = NULL
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "top")
```

As expected, the ARF-based approach differs more from both knockoff-based approaches, but they are all roughly in agreement.

**Note on resampling strategy:** CPI inference was validated using holdout (a single train/test split), and inference is provably valid in this setting.
With cross-validation, test observations are still i.i.d., but models are fit on overlapping training data — technically, this affects inference for the same reason the Nadeau & Bengio correction exists for resampling-based CIs.
With bootstrap or subsampling, test observations may no longer be i.i.d. (due to repeated observations across test sets) *and* training sets overlap.
In practice, Watson & Wright (2021) found that empirical results did not strongly depend on the choice of risk estimator, but **inference is only guaranteed to be valid with holdout**.
Other resampling strategies should be employed with the understanding that coverage guarantees may not hold.

#### Statistical tests with CPI

CPI can also perform additional tests besides the default t-test, specifically the Wilcoxon-, Fisher-, or binomial test:

```{r cpi-tests}
#| fig.alt: "Point-and-whisker plot with features on y-axis and importance on x-axis. Four colored series for t, Wilcoxon, Fisher, and Binomial tests with horizontal error bars."
(cpi_res_wilcoxon = cfi_arf$importance(ci_method = "cpi", test = "wilcoxon"))
# Fisher test with same default for B as in cpi()
(cpi_res_fisher = cfi_arf$importance(ci_method = "cpi", test = "fisher", B = 1999))
(cpi_res_binom = cfi_arf$importance(ci_method = "cpi", test = "binomial"))

rbindlist(
	list(
		cfi_arf$importance(ci_method = "cpi")[, test := "t"],
		cpi_res_wilcoxon[, test := "Wilcoxon"],
		cpi_res_fisher[, test := "Fisher"],
		cpi_res_binom[, test := "Binomial"]
	),
	fill = TRUE
) |>
	ggplot(aes(y = feature, x = importance, color = test)) +
	geom_point(position = position_dodge(width = 0.3)) +
	geom_errorbar(
		aes(xmin = importance, xmax = conf_lower),
		position = position_dodge(width = 0.3),
		width = 0.5
	) +
	scale_color_brewer(palette = "Dark2") +
	labs(
		title = "CPI test with CFI/ARF",
		subtitle = "RF with 5-fold CV",
		color = "Test"
	) +
	theme_minimal(base_size = 14) +
	theme(legend.position = "top")
```

The choice of test depends on distributional assumptions: the t-test assumes normality, while Fisher and Wilcoxon are non-parametric alternatives.

Following Watson & Wright (2021), we can additionaly apply Benjamini-Hochberg FDR control (`p_adjust = "BH"`), which is often desirable in exploratory settings where many features are tested simultaneously:


```{r cpi-fdr-control}
cfi_arf$importance(ci_method = "cpi", p_adjust = "BH")
```

Note that multiplcity adjustment is in the general case limited to p-values, and only the general Bonferroni method is easily applicable to confidence intervals.


### Distribution-free inference for LOCO (Lei et al., 2018)

[Lei et al. (2018)](https://doi.org/10.1080/01621459.2017.1307116) proposed distribution-free inference for LOCO importance based on observation-wise loss differences.
Unlike CPI/cARFi, LOCO requires retraining the model without each feature.
The inference is conditional on the training data $D_1$ from a single train/test split: the observation-wise loss differences on the test set are i.i.d. given $D_1$, enabling nonparametric tests.
The idea is to test whether the excess test error from removing feature $j$ is significantly greater than zero:

$$
\theta_j = \mathrm{med}\left(
	|Y - \hat{f}_{n_1}^{-j}(X)| - |Y - \hat{f}_{n_1}(X)| \big| D_1
\right)
$$

The paper proposes:

- **L1 (absolute) loss** as the measure
- **Median** as the aggregation function
- A **single train/test split** (holdout), since inference is conditional on the training data
- A **Wilcoxon signed-rank test** (or sign test) for inference
- **Bonferroni correction** for multiple comparisons

This is available in `xplainfi` via `ci_method = "lei"`:

```{r loco-lei}
# mae has L1 loss on observation-level, the "mean" aggregation is ignored here
measure_mae = msr("regr.mae")

loco = LOCO$new(
	task = sim_dgp_correlated(n = 2000, r = 0.7),
	learner = lrn("regr.ranger", num.trees = 500),
	resampling = rsmp("holdout"),
	measure = measure_mae
)

loco$compute()
loco$importance(
	ci_method = "lei",
	alternative = "two.sided",
	p_adjust = "bonferroni",
	aggregator = median # default spelled out explicitly
)
```

The `ci_method = "lei"` method works on observation-wise loss differences internally, using the median as the default point estimate and the Wilcoxon signed-rank test for confidence intervals and p-values.

#### Configurable parameters

All components proposed by Lei et al. are the defaults but can be customized:

- `test`: `"wilcoxon"` (default), `"t"`, `"fisher"`, or `"binomial"` (sign test)
- `aggregator`: defaults to `stats::median`, can be changed (e.g. `mean`)
- `p_adjust`: p-value correction for multiple comparisons, accepts any method from `stats::p.adjust.methods` (e.g. `"bonferroni"`, `"holm"`, `"BH"`, `"none"`). Default is `"none"`. When `"bonferroni"`, confidence intervals are also adjusted (alpha/k). For other methods like `"holm"` or `"BH"`, only p-values are adjusted because these sequential/adaptive procedures do not yield a clean per-comparison alpha for CI construction.

```{r loco-lei-ttest}
loco$importance(
	ci_method = "lei",
	test = "t",
	aggregator = mean,
	alternative = "two.sided",
	p_adjust = "holm"
)
```

<!--
### Why `ci_method = "lei"` and not just MAE + median?

One subtlety is that the standard `$importance()` pipeline (with `ci_method = "none"` or `ci_method = "raw"`) computes the aggregation function per resampling iteration first, then takes the difference:

$$
\mathrm{med}\left(|Y - \hat{f}^{-j}(X)|\right) - \mathrm{med}\left(|Y - \hat{f}(X)|\right)
$$

This is *not* what the paper proposes. Lei et al. take the difference first, then aggregate:

$$
\mathrm{med}\left(|Y - \hat{f}^{-j}(X)| - |Y - \hat{f}(X)|\right)
$$

For the arithmetic mean these are equivalent, but for the median they are not.
The `ci_method = "lei"` implementation handles this correctly by operating on observation-wise loss differences directly.
-->
